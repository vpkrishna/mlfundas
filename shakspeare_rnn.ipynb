{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpkrishna/mlfundas/blob/main/shakspeare_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bct7RoXKqX_M"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. \\\n",
        "With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
        "\n",
        "# Initialize the tokenizer and tokenize the text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRsOQdMFqky1",
        "outputId": "9994c84b-b71e-4bb6-dd1e-a5e4a4e2d13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'of', ',', 'data', 'alex', 'data', '.', ',', 'alex', ',', 'the', 'data', ',', '.', 'the', 'of', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "threshold = 1\n",
        "# Remove rare words and print common tokens\n",
        "freq_dist = FreqDist(tokens)\n",
        "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
        "print(common_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZIH6Nk0qtYz",
        "outputId": "6922fdb2-1400-44f2-880c-73b88446b406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['in', 'the', 'city', 'of', 'dataville', ',', 'a', 'data', 'analyst', 'named', 'alex', 'explores', 'hidden', 'insights', 'within', 'vast', 'data', '.', 'with', 'determination', ',', 'alex', 'uncovers', 'patterns', ',', 'cleanses', 'the', 'data', ',', 'and', 'unlocks', 'innovation', '.', 'join', 'this', 'adventure', 'to', 'unleash', 'the', 'power', 'of', 'data-driven', 'decisions', '.']\n"
          ]
        }
      ],
      "source": [
        "# Initialize and tokenize the text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p85n0My-q9Rz"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABekAAAFvCAYAAADe/kDQAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAP+lSURBVHhe7N0HmBNFGwfw1wa2TwVEBREVFUURBSnSpUvvvffee0c60pEqvXekd5AiXUGxoCAgoGABQUXFxn35z+3mNnubzSaXdsf/9zx5YJNcspmdmZ19Z3bmthgXISIiIiIiIiIiIiKisLtd+5eIiIiIiIiIiIiIiMKMQXoiIiIiIiIiIiIioghhkJ6IiIiIiIiIiIiIKEIYpCciIiIiIiIiIiIiihAG6YmIiIiIiIiIiIiIIoRBeiIiIiIiIiIiIiKiCGGQnoiIiIiIiIiIiIgoQhikJyIiIiIiIiIiIiKKEAbpiYiIiIiIiIiIiIgihEF6IiIiIiIiIiIiIqIIYZCeyIubN2/Krn2HpWbzLvLYi/klVcbcUrZ2S3lvw3b56++/tXcREQXXT1euytsTZ0rO4tUkWdoskjl/Oen21ig5d+Gi9g4iIiIiIiIiSkpui3HR/k9Emj9v/CWjJs2SQaOnaM94ql2lrLzdv4ukTpVCe4aIKOGOHv9C2vUaIoePfqo9Eyd9urQyakAXKV+yiNx2223as0RERERERESU2FkG6fcfOSZvlK+vbUVG/66tpXfH5toWkX985WG7/IUiMWvRKmnZ9S3tGWutGtaUYX07yT13J9eeISKzk6e/kVrNu8rxL77SnvHEuj7Oxe9/lAZte6k7eLxBoH7epGGSJ0dW7RkiIiIiIiIiSuw43Q2RybcXv5fFqzZoW97NX75WDnz4sbZFRJQwqzftsA3Qw/lvL6pOxN//+FN7hoiIiIiIiIgSOwbpiUy+uXBR9hz4UNvy7rfrv8unX5zUtoiIAvfHnzfk+OfWdxuYnTp9Tq78fFXbIiIiIiIiIqLEjkF6IhOMpHfq+u9/aP+jaHXl6jUpU6uFWoDT6oHX8B6iSPrzxg3HdQ9G0d/4i4tXExERERERESUVDNITmaRL+5j2P9/uvPMO7X9ERIG75+67Hdc9qHfuuIOnbyIiIiIiIqKkglf5RCZPpksjubO/qm1597/775McWV/WtoiIAnfvPXdLpozPaFv2Mmd6Th57JLW2RURERERERESJHYP0RCZPPJ5GqpQroW15V7dqOUfBfCIiJyqULCJ5cmTVtqylT5dWGtWqJPfde4/2DBEREREREREldgzSE5ncdttt0qROFenbuaX2THy1q5SV3p1ayD13J9eeISJKmCefSCtjBnWXnNms79BBgH7UgC7sHCQiIiIiIiJKYgIK0vfv2lr+vng8pI/eHZtr30YUfgi+Iw9uXT5DKpctLikfelBNb1OiUF5ZOn2MTB3VX1KnSqG9m4goOLJleVHemztRBvdqL69mfkE9l/GZp6RD83qyY+UsqVCqqOpIJCIiIiIiIqKkgyPpiby4/fbb5Y28OWXxtFHy/Rd75crJA7Ju4RSpWLqoJE+WTHsXEVFwoQOwW5vGcnjrMtVp/dnetfJ2/y5qpD0RERERERERJT0M0hMRERERERERERERRQiD9EREREREREREREREEcIgPRERERERERERERFRhNwW46L9323/kWPyRvn62lZ8WDg2kgu7+tq/uROHSc1KpbWtWL//8af6u3Vb3pdjn56QQx8dV89jMdBXXnpe8ubMJkUL5pbsr2aW++69R70WKld/+VUOHPlY9h78UD765As5/vlX8vO1X9Rr6dOlleeeTq/2o0Ce7JIrWxZ54H/3q9dCBVngh5+uyJ4DH8rWXfvkq6/PutMHsGjhC89lkAK5s0sxVxo9l+EpufPOO7RXQ+fX367LB4eOyqoN2zz2CWmU5cWMUqJQPildrIA8nubReAspJjQPX7l6Teq37uFKj/3aM56Kv5FH5k4aLqlSPKQ94+nPG39Jl/5vy/T5y7VnPFn9/c2bN+XchYuybusu2bXvsBz/4qSc//aieg3HAItIFsyTQy1emy7tY2rO/FD499//5OPPTsiW9/e58ukx+fDjz935E/uQxVVeir+RV83X/8jDKdXzOl+/O8uLz8uiaSPV7wmFIWOnyVsjJ2lbwWNVpzjx9z//yCeffaXK1ZFjn8rXZ8/LydPfqNewGDHSEmUcdU9O179YsDgcwlkH4ffWat7VlZ+/0p7xZFUW9Ty43lUWUC994tq/367/rl7L9VoWyfpyJnmzcH7JlytbwPu2eNUGqd+mp7YV3641cyVPjqzaViyk2849B2XTzr0qzT7+7Ev1PI5l9ldfktyu96NsvJL5eUl2113qNX/52q9A86IOZfTw0eOyZtNOlcb+pK2vY9m0blUZ9VY3r/k4ofWqHV9l3+p4+itayvOPl39WxxD78dmJUx7H0FcdTURERERERNEpyQfpEZyfsWCFvDNjoTvgaQcBqrZNakuTOlWCGqxHABYX0u/MWKCCI/oFtS/oRKhUuph0btVAnn/26XjB6IT46++/Zde+IzJ1zhLZsG239qxvmTM9J83rVXelcamQdCBc+O6Sa5+WysyFK92BQzvVK5SUrm0aycuZMrrTJzEF6VEEEewbOu5dV97Yob3DXvmSRaRXh2YqIBOsPIH8sHHbHhk5aZZ8+PFn2rPeWeVNBuljXfrhJ5m7dI2qe5zUO4AgX7tmdVWgEwuHBluk6iB/gvTYx/f3HZYhY6aqDjpfkGbN6lWVtk3r+p1m/gTpf7pyVd6ZPl/enbfcUZ2EOrJ72yZSoVQRvxeZDlWQHnXaPFeenDRrsaM8aZW2t2qQPhrKM84Tn544KeOmzXNcfhNSPoiIiIiIiCi8kvR0NxgxX7pmC+k6YKTjC2u8D+/H3+HvgwGjo5t07Cu5SlSXBcvXOQ6OAd47d+lqyVu6tvQf8Y5ff+sNLvbx22o07Sxla7f0K0APGLnXtudgyVOqlixdvUmNeg0GBIkXrVwvhSo2VIFiJ8EwwD4UqtBABS8QJE5MsL/Dxk+XopUbOQ7QA96Lv8HfBuM3I9/Xadldqjft5ChAD3reRGcIOlVw/G516BQc/+58yVqokvQbPsFxvQPI7wPenii5S9ZU5SCY6RltdZAV/P4OfYZJyerNHAXoAX8zfMIM9TfomLPoc04QfB7uKClUob76Hqd1EurIuq26qzLlTx4IBXR8bN9zQEpUbSrdB452vD+hTtvEIFrKM8ocyh7Oc/6UX/0Ylq/bWg58+PEteQyJiIiIiIgSiyQbpMcUCVUbd1TBhUDg7/D3+JxA6QGe0rVaqAvrhMBFOS62EVjH1C+BQqAAAVUEXvwNzpthVCUCUZ36DXccvPIGv6/HwDHSoG2vgIJa+HsEoDr1Tfi+hMsff/zp2udRKpDjNOhihL/B3+IzEEwK1MGPPpFKDdr71UlghPRu33uo9Bo8Vq7//of27K0HU1/UadlNdfIlJA8i/6McJPRzIBrrICuYbqt190GqbgoERnbXa91T5eFgBiLRAVireRf3lCb+wv7UatFVvjx1RnsmvFDfj5k6V6o36eR19LsvetoiH91KoqU8o2w069xflb1AzhOAjte6rXqoY8hAPRERERERUXRKkkF6jBLvNWRsQMFeI/w9PieQAAtGl0+ZsyRBAR4r23bvl9otuqnAqr8QyMU0EgioJjT4Z4TAWtOO/eTi9z9qz/gH+9LtrVEyadYi7ZnAYYocBK6v/x540Doc/vjzhoydOi/goKQRPmPy7MUB3dGATiiM9g00gGeEKaUQqA9m3kosjh7/Qhq07Zngji8jvVwhSBeIaKyDrPzy628yZOxUWbluq/ZMYFBfdxkwSo0YDoat7++TPsMmBBwY1R0++qn0Hjo+4OMYKNxh8/Y7M1WZTOhvQNq26THYY62SpCxayrM6ZwehbIB+DPce/Eh7hoiIiIiIiKJJkgzSY9oTBEaCAZ+D2939GamM4Ni785dJ3yAEeKwgoIp5v/3pPMD+j5gwXY3GCwUsyNu25xC/A1HYLwTVEVwPFgRD0BkRzTCdx8SZC7WthJs4Y6HfwRfkn2B0ZhlhWpRgBJQSE6Rju15DglbnGKFcteo20O9yFY11kDeYCmThivXaVsIgL6OzCAu8JhTWiAhW2cBxRB0XSEdaIPA9qF8GjZ6iPZNwSIueg8fI2fPfas8kTdFSnjHifcl7G4PSkavDMUR7JtwdRkRERERERORbkp6TPliWrdns17Q5qzZsC1lwTIcAQt/h76gFDX3577+b6kI/VAF6HQIQGPXntEMDQQhMBxHMIIQuWKNpEwssbDh/+Rr59bfr2jP2EMR8a9TkkASibiUIdmGUdCjTMZAAb7TVQXZ+vPxzUPcTnUS79x3WtqIHAq4nTp3WtkILHXbouAu2YB+raBNN5fn7Hy/LirVbtK3gwfev37qL094QERERERFFmYCC9G+NnCTJ0mYJ+iN70apBnZYhWBCUWL1xh6MFOjEKb8L0+WEJZCDAjcCPr4vtfYePqmkvwgEBdyf7BJjXesyUudoWJdSaTTvlsy9PaVve4dhgfvKkOOK9d8fm8vfF4x6PS5/vkeJv5NHeER9ew3vMf2d81KxUWnt3HDXf95Q5Kuhl53/33yetGtaUfRsWyc+nDro/8/q5j+TYzlXSr0srSfnQg9q7rY2ePFve33dI27IXjXVQuK3fttuvu5/CAcdl3ZbQB0cR3B3/7jzVcUfORVt5/vyrr2XH3oPalrXSxQrKjlWz5ZczR9Q+/HH+mBzdsVLqVC2rvcMagv/IJ0RERERERBQ9kvxIev2CGheuuIA1XlC/v3quVK9QUnunPczFe+G7S9qWNVzkz178nqNReJkzPSfTRg+Q8x/vlL+++8Rjvz7asULtM/bdl6WrN8q5C96nZMCIaQTonUzbkC9XNlk+c6z8cOID9/7ggQCAP2k1ZfYSOXXmnLZlTU8rJ3OhI+DRtXUj+WTXex7H8Ma3H8tXBzY6CopEu+yvZpZ5k4Z7pD3yBfLH2ME9JOMzT2nv9A5B2V37jvgMAiK/IN84gQD22gWT5acv97v3C4+rXx+SzcumS/mSRbR33nown/+MBSu0LWvFCuaR/RsXybghPSVH1sxy/333aq+IJLvrLnnphWelT6cWcmDTYtu0xLFFucJIZjvRWAf5I1j19f7Dx+TMuQvaVsKhfkE9g/oG9Y6+XygHKB92nUBGB44ckytXQ7dmA8r+8rVbZOP2Pdoz3iGt61evoILNSF/9N/lb7yQV0VaeP//ya+1/1prXr6bOGflff03uuTu5eu7OO+9Q5frd0QNlSO8O6jkrCP6jE4CIiIiIiIiiR5IO0iPAsGjaKBVswIUrLmB1uKDOmzOrzJ4wVGaOG+wzyItgsq9R/giMYVSpHQRG+ndtLXvXLZCGNSvJY488LLfddpv2aux+vZwpo9rn7StnqaCAHXzn6k07vAZmN+/Y63PENH779LEDZdPSd1VgIcWDD2ivxEIAAGk1d+IwWT1vos/ADdJqwfK1trfznz57QTa59s0X7A8CHgg4ZMr4jMcxvP322+XpJ9M5CopEsxYNqsv6RVOkRsVSHmmPfIH80bpRLdm4eKqj34e0//U37yOokU+QX3wFcfU8sXLOBHmzcD558IH7tVdi3XfvPVI4Xy5ZOn20LHaVsfTp0mqv3BowrdDiVRtsR6uXLVFIpeHzzz6tPeMd8vHE4X3U33iDKSr2Hz6qbVmLxjrIKeShhVNH2tbXqIMQmPRVX3999nxQ5suHPDmyqroR9QyOE+odHcoBysfyWeNlQLc2Pjs1jn9xMmjz3FvB6OiNDhY7xW/aunyGvDvmLRVsRvrqjPUOOkZQPyV10Vier//+h/a/+JDPcHePt/yGstO8XjXbEfUfffK59j8iIiIiIiKKBkk2SI+Az+S3+0mJQnk9AlBmuJjFhSxGxvkKsNiNPMMI1vc2bLedYgCfP+qtrtK9bRMV3LGDfc76ciYVFLC70Id9h4/JtV9/07biYBT9Ktc+2UE6zZk4TOpVKy/JkyXTnrWG4FSpogVk6fQxkjPby9qz1lZv2um1UwPBvK279vkMojWuXVmN9EXAwxcnQZFohADYsD6dfAYdcZyc/L5vzn8nl3/2PjoTQbwtOz/Qtqz5mycqly0us8YPVn93q/jwk8/V9ELeIC16dWgmaR97RHvGt0dTp5IB3VrLC89l0J6Jb83m970GEqOxDnIK6YU8hIC3XX2N/IbR9E7q6y9PnU1wxwHquamj+qt0sIOOzG5tGkuP9k21Z6zh2Jz+5ry2FXyHPvrE5xQpqMPnTRomr73ykm1aQ+pUKVT9lNQD9dFYnu08nT6dPJwyhbZl7YH/3S+5s7+qbcV3+cpV+fuff7QtIiIiIiIiirQkG6RvWreK5M2ZTduyh0AFAhflSxbWnrH23aUf5MZf1vPSY2T49j0HtC1rnVs1lLpVy3uMEPUFQYEOzetJmkdTa8/E9/mXp+Tbi99rW3GOfXpCdvoI2PRs39RnR4YZbukf2rujbVAWAXjMhW8F0z3s8JFWOB4Y7esreG2EoMiQXu19diBEiywvPi9tGtf2GSzV4fchWGYXnMQ0Q5evXNO24jvhOi6Hj9mPoh/YvY3feQJTLuDvbgVYiHnvgQ9tg2t1q5aVVzPbB3atvJjxWaldpYy2FR/Ks7eppKKxDnKqUa1KanS3E8iXuKsEd57YQYfUjb/+1rYCg9HkdkFWI6Rp7cpl5I28ObVnrJ059632v+DCminbdtsff/yWQT3b+dWhhvqpd8cWia4D1KloLc92ZfTs+W/lnIM7MlCPb1sx0/KBwQl33O68HiAiIiIiIqLQSpJBegQiyhYvJHfc4fznYdRZrcreL6bh77//kZs3rUdmfvL5l7YjwxG4aVCjgl/BMV22LC+qgBSmnbB6YFSdOcCAEaQHP/zENvCAEdCVyhTzKxirQ0CtViX7INn+Ix9bfj+me8C0D94gCN26cS015YK/MBUBpvBIDMqWeEOeeSq9tuXM66+9IuXe9N6ZhPS2Gx155NintnkCgZsyxd/wO0/g/fg7XwsWJgU/Xr6iypY3CGaXKlrQr/pHh79BmfYWEMdIbG/TVERbHeTUs0+nV2XBn/3CyHXUX3YdVnadqk4Uyf+6z4C7GTo0qpYroW1Zw3kkoSP8rfzkypefnbBfOLpp3aqS+YXntC3nnHQQJlbRWp5fev5Z7X/xoawNGTNN/a1dXkJnTME8OSwfr7z0QkC/iYiIiIiIiEIjSV6h4Tb+Jx5/TNty7un0j6tglDcYKfrnjRvaVhwERY8e/0LbsoaR4XYjUe1gJOOIfp1lw+KpXh/mUaiYl9zXoqzVy78Zb/55pxBQq1GxtO0o0xMnT6sAiBmme7CbkqNw/tfVQqqBQLAYaY0gRLTLmyub30ESBMleeM5+TmRvI5r/+POGnPUxirdciUKqwyoQ+Dv8fVKHvPuFK297k+XFjI6maPIGgTV8hjefnjgZL/gcjXWQU1leel7SP+7/VEnPPvWEbcD533//lZs3b2pb/kMd9MjDqbQt53CHjF0wOxgj/K1c+uGyfPal9yA97jAqG0AHnA7pgbo5qYnG8gzI23Z3he0/ckyKV20iVRt3lOnzl6sOOk5fQ0RERERElHglySB9ujSPyr33OJtCxOihBx+Qh1M+pG059/sff3q9ZR0QGCuQO3vAwZFAYF5yzE/uDeaqteuQcAKBC0xz4g2Chj/8FD9If/K097SC4m/kCbjzAFK5juGrmV/QtqITRg9jdGog0j+eRvuff5BPz13wPkVCMPLEi88/q4KUSZmvTqYt7++TtJkLSLK0WQJ64G/xGd6c+eaCOpZG0VgHOfXIwyklefK4hUuduu++e+V/99+rbQVf+nRpAhppnOKhB9SdBeF25twF27sZUL7TpnE+p7oZ6uR8uZxNIZeYRGN5hidc9TxG4dvB8V67eae07j5IshSsIGleKiBla7eUUZNny5Fjn6kpkIiIiIiIiChxCChIj7nC/754POiPD7cvl4zPPKV9S+Duvjt5WG/j/u2363L12q/aVnwvZnxGXXCH0y+/XrcNPGTKmEEeTpVS2woMppzwdbww5YQRRvpd+8V7WoHTOaC9SXbXXepW/miGTiRfi7IG29Vrv8hPV65qW/EFI088mvpheTaDf1P4JDYXv/dersLh8s/X4pWhaKyDnEKeuTt5cm0retx/X+g6AELhx8veF4wG1ImoGxMC65EkNdFYngFtmAY1Kvp1hwqC9ugQ6DV4rOQtXUuezlZUmnbsJwc/+iRBd5UQERERERFR6CXJkfThhqkLrEbC6TI89UTYAz64fd4uSI+A3b333K1tBQ6jYO38++9/2v9iYZE+TLviDUZhBzrC3OihB/+n/Y+cSp0qZYI7DpIlu0tSpfD/bpTE5Prvf2j/iwyUKZQjo2isgyi8fOXLJ5/wf0ohMyzkHeiUSdEqGsuzDsdszKDuAS+G/vO1X2Tu0tVSoGxdqdywvVpMPhTrIRAREREREVHCMUhPSVLKFA9q/yPdP//8aztncbjvQCGi8AlG2UZHDzrzKHwwBdmiqSOlfMki2jOB2bBtt5Ss3kwWLF8Xr/OciIiIiIiIIo8RuTD477//OHrNoZsxN4OSVj9f/UX7H+nuuutO2+kuMOUCFx5MmlgHUTBg1PlPV+yn1aHgw+KzC6aMkNXzJiZoXQCMrO/QZ5iMf3c+A/VERERERERRhkH6ILg7eTK5717vC9VisU67qShCAXM8201LcOG7S7bTzjjlax7kO++8Q/tfLIzmtJtm57MTp3x+phN2U/2QNcyN/Ndff2tbgcE0S+Z1CJKaaJw2JhrrIAovX/nyzDffav8LHPJQUqtbE8s0UJiKrFTRArJ95Sz5+vBmmTSir1QuW1xNQeQPzFs/dNw0eX/fIe0ZIiIiIiIiigYM0gfB//53v6R46AFtK74vTp5WQfFwevCB+22D9CdOnpHLCRwR+eeNv+Tk6W+0rfj+d/99kv5xz3mQMZL7oQe9pxUcPf6F9r/AYDS43X7dqlI89KCkTpVC24rv1Jlv5MrP17StwPx0+Wf59mLSDtKnfcx+Tu5QLaytP6wW2I7GOojCy9f6IF99fdbr3OdOfXnqjPa/6IK7RAJdGDUay7Od22+/XY2sb1q3qiyeNkq+Pb5LPtu71q+gPQL1785bLld9LOJORERERERE4cMgfRBgBOtzGZ7UtuLDyMM9Bz4M63QTD6dMKU+lf1zbiu/Ahx8nOBh+9ty3svfgR9pWfEiTh1PFX0Q04zPe0wo+OHQ0QcGDi5d+VL+PPCGf2i0eefjop/LxZye0rcDsO3xUjn/xlbaVNKHjCR1Q3nx99rwKgoVTNNZBFF4ZnnzCNl9i5HRCOmqQp/cfSVi9igDznXfeqW3Fd9GVTy9fuaptOYdpeM64zkeBiMby7A/crYYgvx60P//JTtmxaraULlZQe4e1nXsPqoVkiYiIiIiIKDowSB8EGB2Oxd3srN64Q85/G1iA5K+//5ZOfUdIsrRZLB9PZi0SL+D+wP/ukywvPq9tWVu6ZnPAwXCMyFy39X3bkZWZMj4jjzycStuK88xT6W1H+SN48IFN8N8OgpA7XH+PgDN5wjRDTz+ZTtuylpA8gb/buH2vtpV0pU+Xxra8f/TJ53Lhu++1rfCIxjqIwivNow9L5hee07biQ52IujHQjpojH38mazfv1LYCg2nYHk/zqLYVH6Y7O3HytLblHDqhPv3ipLbln2gszwmBuiD/66/JommjpFeHZtqz8aHjIdA0IyIiIiIiouBjkD5IXnnpBXnhuQzaVnz7jxyTybMXq2CXvzB1y/Y9B7St+DCKzhz0vu222+T17K/YjhBcuW6rrFq/LaCgDUZMT5+/QtuylifHq5bfj1v1s7yYUduKD8GDd2YslIvf/6g94xymdJi9eJW2RWY5sr4ckjyB9y9Yvk7WbNqhPZN0pUr5kLya+QVtKz50XG3dtS/gYOj5by/K7v1HvD5OnTmnvdNTtNVBFF6pH04lmTN5D9LDlNlLVB3prx9+uiKTZi5K8IhyX2uSwNot78uvv13XtnxDOdu4fU/Ad/BEY3met3SNlK7ZwutjnSuNfLnn7uRSs1Jp2zyBOxCIiIiIiIgoOjBIHyTPPP2EFC2QW9uyNmPBCpk4Y5H8++9/2jO+ITgy4O1JtiPWEWBAoMEs68uZpHD+17Uta8PGT5ct7/sXgMC+9BoyVgUfvEGwMG/ObNqWp1QpHpQiPtJq177DMmj0FPn52i/aM74hrXoPHc9R9DYyuY5Lzqwva1vW/M0TeN+ufUdk/LvztWcSl9+u/yG/+xGswkjVkkUL2HZ2zFq0Sj778pS25RyCkwNGTpJiVRp7fXgLskZjHUThg6BssYL2xx+B7B6DxtjW3WZYLHbSzIWOAsO+oOw8bLMuBqCzb/3WXY7rH0y5NnHmIm3Lf9FYnu+6607Ztnu/18enJ045Sp+UKR6SRy3uZtNhHn9/zv1EREREREQUOgzSB0nyZMmkYumitqNJMQqx5+Ax0mfYeEfB508+/1IatettGxxBYAEBBgQazFI8+IBUcu2THQRrGrTpKfOWrfE5whYX9BhNW6dld5+B8AolC3tdDA+j/Iu/kdd21C/MXLhSmnceoOa+9wXvadNjcFACSUnZY488LCUK59O2rPmTJxDsXbp6kzTt1M+vwF843XnHnXK/TQAO6xdgHQR/glW+OsAQ0EZg25+7QZCWSHMEKb3Jnf1Vr3ehRGMdROGV67VXpIiPjlmMOq/XuqeaxsVXnkceQX4ZPmGG9kzCvfT8s9r/vGvbc4jMWfKebWcS9h0j0Vt1G5jguifayrOv9QU2bt8tp7+5oG15992lH+ScTdpgjQCcj4mIiIiIiCjyGKQPopzZXpYaFUtpW96NmTJHCpStK5NmLVJz6SL4rfv7n3/k0xMnpUPvYVK4YkM1as4Ovg8X+t68WSS/VC5bXNuyhkBM0479pGT1Zmq6EvOc5H/e+Ev2HT4m9dv0lFI1mvucVgBz4depWk4taOcNRv2WdO2bL9if3CVrSu8h49RcxcagDdINwfnBY6aq99wKU60kFAIyFUoWUXnVjp4nKjdop0a1mvMERtfu/OCQ1GzeReq17hG1AXpIluwuSZXCfpR3vxETZfGqDep36ZC/UD4Xrlgnm3Z4zrWPDrB61crZBtIQ2EYaYnFGX8FQlLF3ZiyQ/q79sFOqWAF5PM1j2lZ80VgHUfigE66UjwVDAVMfFa/aRJp16i9Hjn2mjrkOefX7Hy+racOKVm4kU+cs1V4JDnTe+lovBZ1J6KBt2K6XOvcY9w/nAMxd37HPcKnkqp8wFVNCRVt5xjz5dnc8oZN8yNhp8pPNIrtIw5kLV6iFb73J4GONEiIiIiIiIgqfgIL0b42cZLl4YDAfuABNbDCStWHNij4DoIDAAoIMWHDx7nSvun/3/U++Jq8VqaLmjvY1/y/mdm9Uq5Ka5sAbBB9aNqih3usLRhNXbdxRHs2Uz+NYPJghhxSqUF+NmHaiZcMa8lyGJ7Uta3pa+QrWAALGIyfNklfeqCj3ps/q3i+k2/O5S8nAUZMdjQqmWE8+kVaqV/AdyIWtu/arQJg5T6R4Npe8Wa1pougYQflA0MuOunugbS/1u4z5C+WzYbvecs1iMV2MvK1dpYy2ZQ0BbgQ6G3foo0b9orNDD/AhMI4g25L3Nkrpms2l+8DRtmU+T46sUsN13DCvtzfRWAdR+KATrmq5ElKqaAHtGe9wbOcuXS15S9dSx1w//skff0XSv1pYBckRDA821D8FcmfXtuzhnINzj3H/cA7IVqSyo/zpj2gqz07ueELnYfm6rdX3Xbl6zb0fv/x6XTbv/EBqNuss0+YuU89Zefbp9D7XMCAiIiIiIqLw4Uj6IMMULu2a1rUdkRcsPds3lWxZXtS2vMPc8AjUh0OLBtXVyFont9A//+zT0qllfW2LwgXHBseofMki2jNJn68FcwNx3733SKuGNX0GxBGow5QXmH8anR0IgiLYiMD44y8XVHcioIPMl7rVyvnsbIBorIMofBDgbd+sXtQu5IuOpDIl3ghL/vRHNJVn1NHoaPHVif3hx5+p70vzUgH3fqR+IY+Uq9NKdbDayZMzq5pWh4iIiIiIiKIDg/QhUKl0MRnUs11IgxBtm9R2HAzHSD0Ez3u0a6I9ExplSxSS3h1bqGCHE9h3BIqxb8EWrQGqaJE6VQoZ1KOtoxHXSYGTBXMDgYD40N4dHd2pkhD+dH5BtNVBFF75X39N2riOT7BhgeBg1K2YHqlu1XLaVvSIpvKMu9FwV1oo6HfAOD1XExERERERUegxSB8CmIu9Wd1qIQuSITg2sEc7vy6w8d7u7ZqGLFCPAP07w3rLo6lTac84g/0a0K2NNK5dWXsm4TACcczA7toWeROKgBRGVterXl7bih4ILIZqvzB1x+wJQ7wulJxQtauUlb6dW/lV3qOxDqLwwfFv07i2K9+01J5JOHTo9evSSlKnSqk9EzhMj4RAdbA6CVGXBauzN1rKMwL4COSHohO7ef1qkjNrFm2LiIiIiIiIogGD9CGCIEmL+tVl+piBQbvYR7Bt7OAeMrRPx4CCY/ib3p1ayPghvSTlQw9qzyYcggjTxw6UtI89oj3jH+zL2/27SOtGtbRnAoegz/C+nSTNYxxJ7wRG3M4cNyjBeRR5c9yQntK5VUM1nUU0Kvdm4ZAEvADpuHL2eClWMI/2THB0a9NYJgztpe588Fc01kEUPgiEd2vbWB2rhHbUYP70WeODG7hGYH1Yn04J/kx0MuL88fprr2jPJFy0lGeUsWB3YqPjrl3TOqp+ICIiIiIioujBIH0I3X777VKpTDHZsGiq1KlaVns2MNlfzSzrF01Rc+YmJAiKv0WgctPSd6V0sYLas4FBcGX+5BEyZmCPBAf9EUQa3q+TCvYH+lkIqMwcN1gFf8gZjNYsmCeHrFswOeA56vV8gIDwXXfdpT0bfRDwwghWjGQNBayxsGzmWBk5oGuCy4OepgjQJSTAGo11EIUPjlOnFvVVXgo0GF69QklZMCXwv7eDYDj2DZ0AgUCenD95uJQolDfo0y5FS3nGd48Z1CPB+6HvQ+eW0duRSkREREREdCtjkD4MnnwircwYO0gObVmqAmX+XKRnzvScCjxvWzFTzeMbjEAEPiPry5lk+cxxsnnZdL+D9dind4b1kf0bF6kATrBG5CFwUL96BTmybZl0bd3IcUACIykRwEBABYEV8t/TT6ZTgTgEcXB8nUC6j+jXWXavnaemGEJAONphBOvkt/vJ6IHd/Zrmx2keR0dA+2Z15dj7q9R0MP5OJYT34+/eXz03qGUr2uogCh+US5RPlFOUV6d5EgHwpdPHyIxxgyRd2se0Z4MP56INi6eqOtzpvuHcgFH46xZODmmejJbyjLsisB8ovzhH+lN+kVaY5g7lN5h1ChEREREREQUXg/RhgkAJghGYMuDrI1tk9byJ0rlVA3kjb06PYDRGu2FajlFvdZNPdr0nh7csk7rVyoVkaglcrBfOl0tWzZkg5z/eKQumvK3m7c71mudctVb7hDltH/jf/do7guuJx9PIkN4d5OShTSqdMPLfuE8IUOTLlU26t40NPHy6Z40KYIQijW4l6CRBEAfHFx0wvTo0U+msB4Ss0r1ji/qSKsVD6vXEAgEvzKmOvIzgIPKXt9+J13848YFULfemes0pzIGPwNgX+9bJwc1L1BQx1cq/qRbeNNLLFqYk2bdhkXo//i6Q6W18icY6iMIH5RTlFeUW5Rf525jvkQeKv5FH+ndtrcr/nrXzpWLpomEZda0Hw5HfrPYNcA5AWUW+xbkBeTdcdU+0lGd0tuFusxP7N6jztbnuAnQMlCySX61HgLQ8/eFW1VHAxdSJiIiIiIii220xLtr/iYgS5MrVa1K/dQ/Zumu/9ownTK0zb9JwBoyIEqGTp7+RWs27yvEvvtKe8dS0blXVuYOOMCIiIiIiIiJyjiPpiShoLn7/o5w59622FV+qlA/JvfdwRDYREREREREREZGOQXqiW8BXX5+VbEUqS7K0WSwf2YtWVaNkEwI35ezYc1C+Pnteeya+J9I+Jvfcw1G2REREREREREREOgbpiW4BWD/Abv5mTF+x7/BRbSsw57+9JKs37tC2rGXL8qIku+subYuIiIiIiIiIiIgYpCe6BTz04ANqUUM7MxeutB0Fb+evv/+WybMXy/4jx7Rn4nvhuQzyykueCy0SERERERERERHd6hikJ7oFYCHHfLmyaVvWDh/9VPoOnyA//HRFe8aZf//9TwX4ZyxYoT1jLWe2l+XJJ9JqW0RERERERERERAQM0hPdIgrmySFv5M2pbVlbuW6rNGrXW459ekLNMe/Lb9d/l4GjJknfYRPU/7353/33SfUKJeW+e7loLBERERERERERkRGD9ES3iDSPplaBcl+27d4vRSs3ksYd+sju/Ufkl1+va6/Eunnzplz64Sc1vU3B8vVk+IQZtgF6KF+ysOTKlkXbIiIiIiIiIiIiIt1tMU6GyxJRkvDTlavSqttAWbPJfoHXYEqfLq0snT5aXnvlJe0ZIkqMTp7+Rmo176oWmrbStG5VGfVWNzW9FhERERERERE5x5H0RLeQ1KlSSNsmtVXgPFx6tm8q2bK8qG0RERERERERERGREYP0RLeY/K+/JoN7tlPzxIda384tpVblMnLbbbdpzxAREREREREREZERg/REtxgEzDE3/ZSR/SXlQw9qzwZfj3ZNpFPLBpz6goiIiIiIiIiIyAaD9ES3IATqq5YrIe/Ne0eyv5pZezY4EPifOW6w9OvSWu679x7tWSIiIiIiIiIiIrLChWOJbnFYTHbc1Lkyde5S+e3679qzgalTtaz079JannwifHPeExERERERERERJWYM0hORcuG7SzJv2VqZvfg9Of/tRe1Z3zC3ffmShaVtkzryykvPy+238wYdIiIiIiIiIiIipxikJyIPf//zj3x24pQc+PBjOXDkYzn37UU59NFx7VWR9OnSykvPP6OmyXk9+yuSK1sWeeB/92uvEhERERERERERkT8YpCciIiIiIiIiIiIiihDOS0FEREREREREREREFCEM0hMRERERERERERERRQiD9EREREREREREREREEcIgPRERERERERERERFRhDBIT0REREREREREREQUIQzSExERERERERERERFFCIP0REREREREREREREQRwiA9EREREREREREREVGEMEhPRERERERERERERBQhDNITEREREREREREREUUIg/RERERERERERERERBHCID0RERERERERERERUYQwSE9EREREREREREREFCEM0hMRERERERERERERRQiD9EREREREREREREREEcIgPRERERERERERERFRhDBIT0REREREREREREQUIQzSExERERERERERERFFCIP0REREREREREREREQRwiA9EREREREREREREVGEMEhPRERERERERERERBQhDNITEREREREREREREUUIg/RERERERERERERERBHCID0RERERERERERERUYQwSE9EREREREREREREFCEM0hMRERERERERERERRQiD9EREREREREREREREEcIgPRERERERERERERFRhDBIT0REREREREREREQUIQzSExERERERERERERFFyG0xLtr/iYiIiIjI5OL3P8oXJ0/LA/ffL69kfl6SJ0umvUJJyV9//y2ffPaV/Hr9uryY8RlJ+9gj2itEpGM5IaJQYpuLbmVJciT9zZs35ctTZ2TC9AVSqUE7eTbnm5IsbRb1yJy/nNRp2U2mz18u3178XthH4d2QsdPc6YZH6+6D5M8bf2mvEkUP5EvkT2N+Rf618v2Pl6X7wNHy2Iv5VX2AugAXG0TR6Jdfr8u4afMkZ/FqKl8j31as31bmLl3NfEtJjrHd4a0Oj4Q9Bz6UAuXqSakazSVfmdrSdcBI+f2PP7VXKanAMcWxxTHGscYxx7EnojhJuZzsP3LMfQ4qU6uFXLl6TXuFiMKFbS661SWpIP2///4n2/cckKKVG0mWghWkS/+3Zf3WXXL+24vaO0ROnv5Glq3ZrAJ6GbIXl2pNOsm+w8dUYJ+IkrZff7suvYaMlbFT58rP135R9QHqgnfnLWeHHUWdr8+el1rNu0i3t0bJx599qZ5Dvt2wbbcsd53Hrv/+h3qOiELnxl9/yYp1WzzakgtXrJfPTpzStiipwDHFsdXhmOPYIw8QUSyWEyIKFba5iJJQkB6j4pt17qd63D44dFR71rc1m3ZIoQr1pUOfYfLTlavas5RYIWj17rxlMnLSLPfj0EfHtVfpVnfi5BlXmd+pbcXZseeAXPv1N22LrPz9zz+ydPUmd7nauH2P9gqFAu4OQWfStt37tWeIKFrcf9+9ctddd2pbocf6N3Jwi/3tt3MJLyI7LCdE0QFxD72tgJhIpAf0BKP9Eu42F1GkJYmz6cGPPpEK9drKguXrtGf8N3XOUqnaqIMaWUuJF6Z/WLt5p/QeMs79OHPugvYqEQXqv/9uqtsP9XJ17NMT2isUChgxgjvBdKWLFZR1C6fIyYObZPOy6ZI/d3ZeEBOFwd3Jk0uVsiUkfbq02jMi5UsWlueffVrbCj3Wv+GROdNzUrtKGW1L1DEvW6KQJLvrLu0ZImI5IYpeiHvobQXERCI9Naa/7ZdoaHMRRVqiv8Lfe/AjadKhrxz/4ivtmTj/u/8+KVEor/Tv2lqG9O6gHj3bN5UCubNr7/CEeegate+t5rMnoqQn47NPSXFXnWBWpEBueeiB/2lbRJH31ddn5dIPP6n/v5E3p0wa0Vedz55K/7gUzpdLurdtIikefEC9TkShhXbjnrXzZOOSafLB+oUyckBXue/ee7RXKanAMcWxxTHGscYx93bNQHSrYjkholBim4tudYk6SI/5ensOHhNv9HvKhx6UAd3ayGd716qRh707NpeurRupx1vd28q2FTPl1KHN0qllAxXINzp89FPpPXS8/PDTFe0ZIkoqENQcO6iHNKtXTdUTGA307pi3XNtV5bbbbtPeRRR55wxzMebKlkUeTf2wtkVEkZD2sUekaIHckjPby2pqB0qacGxxjHGsccyJKD6WEyIKJba56FaWaIP0mK934oyFKqhulCdHVtmweKoaMZ/m0dTas54QjHvyibQyrE9HWbdwsmR58XntlVjrtryvps7B7TlElLQ89sjDMnF4H/n+i71ydMdKaVCjIk/+FNXuvju53HEHp7YhIiIiIiIiSqoS7VX/gQ8/lvnL12pbsdDTNnVUf3ntlZccjYrFexDUnztxmPpbo7lLV8sXJ7/WtoiIKJTYKZp4xcTEyM2bPH63MuYBIqKki2200LtV0hhtBbQZiIjIWqIM0v/+x5+yaOV6+e3679ozokbND+rRXl54LoP2jHMvvfCsmt/XOPUN5qVftX5bvBMmRvC37j5IkqXN4n4MGTvN/drqjdulZvMu8tiL+dVrmfOXk059R6jFbf29gMX7P//yaxnw9kTJX7aO+/tyFq8W8GcGA74T39288wB5Nuebap/wb+MOfWT7ngN+L1CCE/W5Cxdl1OTZ8ma1ppIqY273Z9Zq0VWWrdmsjrk3WEtAT5s0LxWQrbv2a6/Eqt+mp/t1/YG/CZZgHKdw5iukN/I39rdwxQbu9MZnNunYV626bpfe3mA/t+3erz4Dn6X/DqQJvgvfGUijDPuCPGD8/UjboePelQvfXdLe5dziVRvc+4ZHmVot5MrVa9qrcby9D+mNRW+Q/vrvxH5h/3zlVStOy5N5f7IXrRryha7x+fgefN+DGXLI9PnLtVdE3ho5yWN/8EAeRj4wQj7WX9fz9E9XrsrYqXOlaOVG7teGT5iuXrOCudnxfmP9gHyFPIC6Q89XxjSy2hcw/iZjGurfYdwn/Tv8yWfG8oV8is/BPpet3VJmL17lNX+Y6wCkr84qrb3lW/BVJ6H++/ff/7R3e2eVVvhsLACF/Knn1xzFqqnp53TYL+yf/r16fWtOY1/pYlX34Td0e2uUX/WJXsasyuyaTTuCsqgWfoP+e2s06yy//Hpde8Uafi/qSv1vug4YKX//84/2qjXz3xjLo1mw8oDVsUS6Yx0gY3omf/wVlcZW8H6UU5QlqzLx62/2aeVNOI6rzlueNrOqg7CfVueMivXbypL3NnqtE4JR/xrhNfM5GscB9SraX8a6NFiQTvr+2dVZRt7qaDOrcwvy9O79RzzqJ70tueX9fY7yhJPziBnes3nnBx51FdK4RZcBHmXNyW9zmteM8P3G8wd+gxMoe2i3IH309NLz5owFKxwdL38Fs4zoAv39VnnIzGp/rfIZ9hv7j7rRXI6wffbctx7tAn9+nxXsA/IGvlP/TDz8bXNbpYGTNlog5QT7gzxvvt5D2qFeCvQaUk9b/Vyn12uTZy+WHy//rL3T2fEOhUDT2ChYZdXbccPnIy/i84x1GNpbaEugjPpi9dnI2zjP43yvH++2PYfIjb+sj3Ow8rUu2O2EYO2fVVoFWg/ib/EZ+uch7qFDTASxEf01POzOqxCMay7juS6Q9ksg50HAZwSjnRPM42MF7zXHNvCZ+GyrcwjdehJlkB4FzDzNTf3q5SX/669pW/4rWjCP1KhYStuKtXPvIfnu0vfalj0s8lelYXup1qSTrFy3VX6+9ot6HpXUxJkLpUDZutKhzzD3876gwVGnVXfJWriSqhQPfXRce0Xk48++dH8mKiGkR7jgJN6539vqu3HSPa/Nm4x/5y9bK6VqNJcGbXvJxe9/VM/7gvToM3S8ZCtSWXoNHis7Pzjk7nzBZ65Yu0XqtOwmuUpUl6WrN7kvdKJFqI9TsPPVtxe/VxcVWQpWUPv7waGj7vTGZ85bukYq1Gsjxao0lh17DzpqlOFE8v4Hh6VIpYZSumYL9RnGkz/SBN+F78R3Yx+c0D8X+4I8YPz9SFs0hhAcnDJ7SVCDMXauX/9D+g1/R+VHpL/+O7Ff2D/sJ9IAJ3EnkBbIG3blqUbTzh4BUF9wPAeNnqIa8Xi8M2OhR8Mn0tBYLlm9mXQfOFoFe+3guOL4Zi1USb3fWD8gXyEPoO4YNn56wL8RdQrSWv8O4z7p34F8hvf4qn/Q6ELjM2/p2irPI58C9hkBInTEID8b64lg0/OUXZ30Rvn6UrVxB/nk89j9c+r673+o9MBFJdJDz6++eEtju3RBGUI+Mdd9+A3jps1TaezkuONCuEWXt1QZsyqzVRt39LuMWcmV7RX3IIEDRz6W09/Yf953l36Qjz+NS3+0ab7XFgr2xtj2efbp9PJ69lfU/81CmQdwLKfMWSKFKjTwSE9vUCawDyinyDtWZSJPqVpqUIST840uXMc1oX52XWjiHG11ztiwbbfUa93Dr3NGIOzO0TgOqFfR/sIxQntML2uJDeojXPCiLjHWT/gXbUkEieq07O643nICafvRJ5+ruqpcnVYedRXSeNaiVaqsNWzXy3HbJxxQjtGmRtlDuwXpo6eLnjdbdRsoBcvVU8GHULaxoqGM+OPche8s8xn2G/uPuhHtAD1gg3MUzlXZi1X1aBck5Peh3kb9jbyF79Q/E1Df43twjkR5Rl3pD3/aaP74/sfLqr5HIMp8vYe0Q72kt3mdnpfw9/1HvONOW/1cp9drHXoPkywFyjtqv4WTP2kc6rKK8y4CoIUrNlR5EZ9nrMPQ3kJbAmXU37z09dlzUs11LsZxx/leP97eBDtfB7udEMpyFw31YDiuuUIl1O2cYB4fpKVVbAOfic82n0Po1uQzSD9p1iKV4dNmLijpsryhMuDcJau1VyMDlSR6KnUYRV+2RGG58847tGf8d8/dyaVy2eIeo+kxpc7JM+e0Le8Out6H4CN67uxMnbNUVXB2hQ6VDEZnlHU19nEi9gVz55eu1UL2HvxIeyZ0UImg0kaesIMKB73kvhbfxUmxruuCaeSkWT5P3Ki46rbqLgNHRUelFY7jFMx8BWgUVqjXVu2LLx9+/JlqWE2du9S2YasHbKo0aq/+xhd8N/YB+2IH6YtGqa/PRZ5s33uojJ0yJ+SB+u9/vKJO0G9PnKk9Yw2N3bY9B3vUUVaQpxEw8HU8cPJv0LanoxM/Gk19h01QQXo04vHo3G+ETJ61OCpuo0We7tL/bTVKwBfkZzSocHztGlOoO5D/uw8c5bMeMbsZc1PmL1+jjqvdd+A1vGfFui0qb1pB/hs4arJqpNvtB/Jz5/4jfOaPQBw9/oUaxe2kjCNfVW7YQY1U9/abjP74808ZPmGGevgLgSpfaWxMF1y04uLJbuSMftzRoPVWR1395Vdp12uIzFnyXuwTXiAtcG5LyILxTz+Zzj1QAKOQfNX1COwZy4GT9saho5+4802u116Rp554XP3fKJR5AHAsUcc4KWu4SG3TY7C62LB7P+rCpp36qVFFTvYjnMc1IZDfe7suBnGOtuP0nBEIlI05rnRyco7GMUJ7DO2ySHdu+Ovs+W+lY98RamSkHbyO9wUjTyCv4vOqN+3sc5Qf2jPtew9TdUOk4Vw1esps1abWAw3e4HUEH4aMmRqStnc0lBF/nHLV0ajT7PIZyhHaARjFfe3X31TbBOcquzrQ6e9DnkMgCvU26jc7enlu0Kan4/LsTxvNH198dVoFQ+f4qLPB6XkJZbhZ5/6qTWKXtshjuJYaO22u/OPjbrVw8CeNw1FW0d6q17qnz/1BGfUnL6GsdOwz3Oc1LIQiXweznRDqchcN9WA4rrlCJdTtnGAeH7T9a7fsZruf2Ef9HGIXg6GkzWuQHiO2cGsLKlhcbF7++aq6ZQyVLS6mMGIEo+rCDYEmnOyNsr6cSZ7NkF7bClym5zJItiwvaluxcIuXL+gdNo/s9wYFfNuufdpWfEjrxh36+jwZG+G9PQePCXnjFcF3TDPkBBbftbvgxskQJ0UnJ28jNMaiodIKx3EKZr7Cd/rb8MZJAsEYb4FJPIfXnAZsdNgH7ItdOiB9+/jxuejVR695KGG/fTXOdDhu785b7rXjAPm/p6sx5OvCXofPw2gWXzBqZatFPli7eafju4KMMjz5hGxbOVMufb5Hvjm6XepXr6C9ItKjXRP1vPExtHdHuTu590V49TyNztBm9arJ6nkT5eTBTXJk2zIpVbSA9q7YBhfKOe4C0KV86EFp26S2bF42Xb4+vFn2b1wkowd2l8yZnlOvowz4e/vyZydOyejJc1Q+y/5qZpk5brDal+O7V8uSd0dL8TfyaO+MazR99uUp7RlP+w4fU7cc6/TPw2cd3LxE+nVppX4DIA3Q2WOc5gPphvTT0xLpq7NK6/lT3pYUDz6gvSO2jOOCxFhnYP9nTxiifhPSefnMsaozWodOnC4DRvkMKAMas7hoBvy2d4b1UccAx2L80F6SKuVD6jUr+vR0xjTBPo0c0FUyPvOU9q7YdEFDeISrnse+IZ+0alhTtq2YqfZ/3cIpUqdqWY/OdCwgjwC3Fewvzls6pAeO64n962XXmrnqs/XPwjlrkk3A3xd09OfLlU3big26e5vyBvXCkWPxG+i79x3x2pmGCyjkMV2eHK96pAOEOg+Afixx3Ib07iA735sjZz/apv59On067V2xHYYIFixcEddZgPyPvGxVhvGZqPNXb7QPskI4j2tCGNtM1SuUVPUd9hG/2yrvm+uEhNa/+jm6S/+RKn0B6WKue1GWUTZ1aJdFsnMjEDj/41jj9+H4I4/h9yFfGuteCFaeQBlH2UEZ0pnLG9JZr7PwvZjW6qcrcdNvhBvql4kzFql2m85Yls8d26HypzlPhKrtndAyEm679h1WQU09nxnPTdh/I5ybEPRauCL295nPf8b2C+D3+brzEXkO1+B6njOXZ3y2uY73pzw7baP5A+elZp37ebR3ca7U21v4fG/nJW/ndpwPh4yd6nEeSJ8urfTv2trjnIRtPA/DXdcJxjZapDhN43CU1X2Hj6qBPEhv1JGoK43ph+8ylkHkJVy7OBklrpcVwLHFMUb+xKNBjYpyxx1xIahQ5OtgthNCXe4SWg+arx+mjOyvvSJSOF8u9VnGtgLaFWhf6PC7g33NFezrR2/C0c4J1nkK9RYGNhrzkfE8gjyEfKpDnfX+vkPaFt1yXJnb0utv1oi5K83Lto/ydVtr7w6fP/68EdOq20CP/ejS/+0Y10Wv9o7AWX12n6HjY1yVl/YO6/foj0cz5VOvuQpnzLDx02Mq1Gtj+b4GbXvFuAqv9olxvv/xckylBu3ivf+ZHCVi2vQYpD4XD1elop4zv69FlwEx13//Q/u0hBs8Zmq878Aj5XOvx9Rp2c29P/g/njO/781qTWMufv+j9mlxkIbtew2N9348SlRtor7X7nfiu9Zu3ql9WkzMqTPn3PsycNTkmJcLlPd4f/02Pd2v6w/8TaBCcZxCma9+vPxzTJVGHeK933wcse9W+5u3dC3L9HI17izfb0wHu/2t0axzzM/XftE+LY7rhBlTsX5by7/BZ+Ez7dJXfyAfWXGdaD3eV7pm85jLP1/VXo1jfp/xYdwPHBscI/N7MuUpHXP8i6+0T4tz8+bNmAnTF8R7Px5ZC1eK6Td8gvpc7H+hCvUt34fHa0WqxHz19VntU2PhmDh9r7/MedRb+pqZ6xHsC/YT6eDNhx9/5nFsS9VoHuO60NNe9XTl6rWYtj0He3wHHthX7LMZ0gH7YHxvh97DLMvOjb/+ipk4c6FH/Tbg7Yke5wTd8AnT3e8pVqVxjKsRpr0SZ/f+I+7fhc/csfeg9kp8xnTzldaoT1Cv6O9Hfpyz5D21/2b//fdfzIZtu2NeylfW/X7s73eXftDeEccqrXoPGWeZVkYoTyhXxr9DnWD1d+cufKfaEsb34uEtn2D/F65Y53FMrNoAf9644ZEvUD9e/eVX7dVY+OzFqza4Pwvnjs+//Fp71X/G9Er/auGYjz75XHvFE35zgbJ13fumP7ydMwH7pZ/brOqWUOUBq2OJtLz0w0/aO6zhHG08RjjXXPjukvaqJ+QLlEHjd+Bhle8jcVzBnA7Im1bM5w0cB+wL0twMaYj9199rVycEUv+eOHnao/wWLFdP1a1WdS/yDz7TeMz6Dhsf888//2rvCIzxnOTtXGtmLEf419u5y1hH4oH8vGvfYcvfh/MHziP6e+3yhPEYejuPoO2CNoz+Prvyhv3ZtGNvvPaKt9/mNK8ZmfMHfoMV87kVdQbaiFbwmWhT6+9FnbbnwIfaq4ELRRlx+vvNjHnIW5ky7y/Sb+feQ/HyGcrKrEUrPcqQ/vB23kSda7yesDtvmPMc2uZHjn1qmd+t8hzaKFZtF3M5Qr701UYDJ+UEzxmv95ycl4z77O06wXx+sTsn4XxrdQ3k7XgHUmf5Ekgah6qsmvMzHmiDnfnmgvYOT/hOcxsb1zBW+25VVlas3WJZtnWhyNfBbCeEqtyZ0ypYbQUwfraTPBzKay4w18/eyp6R0/NgqNo5oTg+X589F5OrRHX3+96dtyze52EfjenbuEMf9RzdeixH0uM2ENyO4QsWmDSOkgoH14ldzeVq9NCDD0iyu+7StgKHzzCOtgFX5exzMTfIme1l2bhkmkwa0Ve6tm6keglXzh4vi6eNcvfi6zBdj9Wc7Ru371a9uUaNa1eWA5sWqx5AfC4ew/t2Us/hNSOMXD90NHRzHQN6C1fNmSDzJg137w/+v2LW+Hi/8/CxT+X8t/EXXESP9Pzla7WtWPhc9EwiDXt3bO7+nZ/sek/6dm6pvSsWekrnLVurbmUDzM2r70urRjUl/eOPqed1JQrldb+uP/A3gQrncQpGvlq/9f14t+YWK5hHtq+c5XEcse9W+4te4fVbd3mMpkdvMKY90HuDdS0aVPdIB+wv8gt6m/FbjNAzvXvfYW0rzo49B9T3GWV58XmVDvjt+Ew9fQ9vXaq+M1zQ6z1t9ABZPnOcez9wbHavnafS1Agjjz/5LP6dC7hLaenqjdpWLHzu2ME9VNq91b2t+lyUAxwjq2PtDT7HKm/fd+89AY1QCDbsX/+urSRPjqxy2223ac96wihjjDzT8xbyzeiB3eT5Z59W22aos4f16RQv3zqFkS/9XPuEfTNLniyZNK9XXVq76hWdq/Hnrnt0GPH0u+HOMkx7ki6tZz0EuV7LIuVKFFL/Rz326Rcn1f8TCvUJ6hXA7xg3pKfUq1Ze7b/Z7bffrkZrTX67nztfYcTTyvXbPMq4FaRV59YNLdPKDr6vd8cWln+HfRjUs53Hou92+QT7X618SencqqH2TOw861d+9lw0zXVRpOopXZH8r8tDD/xP24qFzy5ROJ8Udr0GGPF35twF9f9APPF4GnWMwdVo9zo6/bjruOsjBCuUKiqlixVU/8c584SXO4yMU93kyZnVYxQUhCsP4JgM6NpaHnvkYe2Z+FA+cI7WRzWVdeX5UW91sywTgP1FGTSOQvMmEsc1IXp3aqFGXiHNzZCGQ3q1lzfy5lTbSK+DH37i8xg4gToJU6zod8+hHp06qr+89spLlnUvzhHd2zaRHu2bas+ILFq1UbUrEgPkoWF9OkrBPDksfx/OH8P7dXLXM8gTCfltaLvoozR9lTeVH13t0MGueg7vjRTzuRUj/Ef06yypU6VQ22a4O6hn+2aqrQOo0+YuXe1R/oIhUmUkUAO7t3HtT/x8hilX61Yt79FeALvzZtrHHlG/D21cQBp7u/425jnkY4yWxchQq/yO59501YFTRvZzfy+O/Vdfn1X/9wbv9dVG84fxes9XOcHxL1kkv0c52fr+vnhTPVqdX3A94O2chHPcO8N6B3wnQLA5SeNwllWcG5B+mLLPCr4TbWzjtRauYZyss9a+WV2pVKaYZdnWhSJfB7OdEI5yB5GqB8N9zRVM4WznBOP43Ljxt/zx5w31f9yRUCB39nifh33EeQRTecOp0+dc1zf+rW9ASYNlrbn0vU3a/3xb7mA+7qQOBWlQj/bxpspBwcPJCScpI0yxgKmDjNDo2Lh9r7YVq3zJIjKwRzvLkzKew218xkYHKoVN2/e4OxXwLyovdLr4ekyYvkAt6uMLAofmiyD8Hw1WNFyNsD/mEx5OBuu37HI3rgANKAQL8FusKitjw0O3c+/BkC6e4k0ojpM3ocpX+gkY00SZT2JW+wuYQ/7X3+KOGaac2rbLc6oiNOBw4janA74DDRo0bIyBOFi/bbdHQwr/3+E6tkbIH2MGdZeiBXLHyx9oLAzu2V41YMMBgUGcPM3rX6AxM7BHW3fAS4d5cs0naVy0oOPDCA0GBIPNFy76sTaXLW/Q2YULFrNybxaWx9NYB8jCCVN05HZdmNhBw1+/RRZaN6oVL9+YoZ5AB52v91mpXv5Nj2ljzHCsseaJ3mC69MNltYCQEW7bve++e7UtV9379z9y82b8xjOOb7N6VdWtjXggcJNQ5jq1muv3IA9YNVCNcIFYq1LcYulb3/9Arlz1Pg8loIPBLq28KV+ysNcLaEC+RfnW+conOCa4ENKPCTruL5sasXgP8oXuzxuxDWOzB+6/X/p0au4+JllezKi94j/zlDfoPDCe6wAXFfridlCsYG6fDXvUi8apbvLmzOrx28KZB4q78mz6dGm0LWsnTp5W52jAMerQvJ48mjqV2vYG+Qq3/erH1JtIHNdA4VxbtvgbtscBv9d4vkVevvFXwtdXufDdJXU7tw5p6+2iW4e0rV25jDs/4qIdUzNEMiDqFAIt+bQ1Ibwx1zNnzn2r/c8/mI5k2+4D2pZIjYqlVDvQ7jjjNZTJiqWLas+En/HcinzXskFNeeB/96ttb5An6lQt5w4i73fVQ8Hs8IpkGQkEgn2F8uXyur9IL2N7AXydNzM89YTrM2PLHGBwE84TRuY816hWJcn8QtxUOd7ky/Wayp+AIOS+w0fV/71x0kZzKpDzEl5D4LRW5dJq4Mvrr72izidGgZxf0P5o3biWO3AaSf62g0NdVhvWrOTz3IBzbpvGtd2fjWsYX9Nw4L1o49od71Dl62C1E8JV7iJZD4b7miuYwtXOCdbxufvuZHLvPXer/99w5S1vcaAXnntaFk0bpfLk0D4dJYVpADHdGuIF6ZFJ9R4pJz753Pl7g+Hu5Mnl8TSPaluxrv3yq6PR7r7gMzBy3ghBQF+j9HO7TrjmQKoOBRoBbPPo1m8ves4PfdZ1sXDYNLq6tquRYhX41aHRgeCHEQK1+jy4aOSh4u09ZJzPB+bz/NXHPI8IzuuVmhl+J4ITubO/qj0T67pp3YIffrwcb5QIggQIFniDCrVRrcqqktSh0bfv0NGwXzyG4jh5E6p85atBhv3FqAq90YJHu6Z11HHQYW5JjNTQ4UTdtG5Vj0aRGRo2aOAYYZFD4z5//+NPKm2MfOUPNFzRgDVeFIUCPh+BQWM6GCEAoC8cqUPHl/EkjToGv9kIF311qpT1+rk41iVdJ34EAXxBELhPpxbq7hN0GOCBOQTRmDLO/xgpyCcP/M/+IglrN+jnILs6x+zZp5+Ml/6+YCRDpozPaFvePZo6pXuRTnS8XL3mOZIecmR92X0BuPi9jWqkkdXcskgD/C48gtHANdepGLVnVw51yG9VXRfNeh2y/8jHctpmESXkf1+NX298/R3ybaaMcWmBfOvrNzyc8iH3McFdK+ZgONoK6IjUTZu7TDXmMcLKCOXilZdecB8Tc0ebvzCCR7+QPfTRJ/LNhe/U/3U/Xr7iXjAKaYr358ia2Z13DrjqVnOgHBdResce8kyubK+o/+vClQcAF7F2FyqA+fb144Hg6auZX1D/9wXlMWe22DsRvInUcQ0EznmP+AgeIS2N5/HYCzvvc1I7hbUz9DyD328XWDTCyN6q5UpoWwjGHFeLX0Y75EvzSEkz1DNPPhGXD374Cedn/9P6J1cZNrZTEHhHB50vKJNltLtmIsF4bkX7Em0WJ4x3CKGuRdApWCJZRgKBgDru3raTLu2jHmlrzHNWcI1pvMvI6g5uY57DeQMjMJ2UZ3PH8acnTtqmnZM2mlOBnpfQoYG7cTcsnqoeCBoaBXp+yfryiypAHmn+toNDWVbRVkG72Uleeuap9OpOBx0GytnlJawV+Ghq74MzIFT5OljthHCVu0jWg+G85gq2cLVzgnV8HnsktXtOf5TPt9+ZqeI05lgWrgcwGAe/Cf9GQ+cihV+8qM3Nm9aLlnkT7iCp1ZQ0OAkZR/kGCsF+VFZGD6dK4TO4lSrFQ5IsmfdAPgrlkz4uEE9/c94j6AnVmnSSZGmz2D6adx6gvTsWFnO5arMqd0I842qcPmhzEYSRpP+7P240KZhHdmMUqnnhxQKuSshbgFKXNs0j8ToAUAFaBcFCKZzHKRT5CicQdHb4OokZA4l4IEiuN65x0sHnGiHQpAfMvMF3ooGDho4OjR9jHrl85ZqcOnNO24rlJH88lf5xedFBsDUhUqdKKfcbRkub4bUMXm4X1f35519ywdSRgqkr7EYZAy5ajA0+OziZI0iPhX7wwMI/TgII4eBkarLPv4pbrBuB2xQPORu5jd/o9GJNd/tttztq0BnrNlwc/ncz/uJSubJlkdpVyqj/Y2RGg7a95OlsRaVm8y4yadYiNXraya3H/rr88zV3IBidek+7yoJTCAzoF3/4XXajrnzl/4Ty97OtzjdmpYoWVCP3ABchpWu2kBfylJbGHfrI7MWr1C2uwejgNzJfJGNkvBE6eTCtDSAgjVvMMz7ztDt4gKlw0Gg3Mk51g/rbHPQJVx6AlCnsR/SYzw8YEeT0AgOj2l5w0BEUieMaiDvuuMNR/WJM03///dfvdriV09/EHUd/6lFA4EY/Zt9e/EF+MrXjohHuOnOS1o88nFL7X+xAlkAuYYztWH/L2zOuNpjxQj6cjOfWVeu3SeoX8li2V82PBzPkkOnzl2t/iaCOZxstISJZRkIF7fbkNm13K8Z8acWY59Cmf/3NGpbHyupRv01P9XdwxlUv2LVDgjV9LCTkvORNQs4vaEe/mjkucBsp/raDQ1lW0aGU2kfe0yEWkuWluJHmvvKSr2tYCGW+DkY7IVzlLpL1YDivuYItXO2cYB0fxFEw3ZfeGYTZLp7PXUpylaguPQePUVMT4/oxms9vFD7xos/IiC8+7zzY9dLzz2r/Cw+cJMz7Z3VRGwjMBWse5frSCwn/fbfffpvabzvm3t1o5LSSsoPglj4CAtCj+ISXeWqN0KAxjyTACGxvt7CFSjQdp0DyFYL6CO4nBC5qzbfiIjjtJMiGAL3VcdSZ8wdOoukft59aAVSA/CnPOZrDDWXDPB2P2b///SvXDb8PcMHupFz5uohLKox5FqNwMCLGqVAGkX1B42tIrw5qai694YcRcSvXbZWOfYZL/rJ1JP2rhaVpx35qZHSw4KJV74jD6HJfo/yMrO5MS0pwCzzWjGhQo6L2TGwHyvxla1XHaY5i1ST9K4Wl3/AJquM0GMwjqDA6Xa/TMKhh174j7m2M9MLoXwQPMCUY4FjiTiUdOqI//DhuNKJ5qhuIpjxgPj84qb91OJ/d7aBDMRLHNbEx3sXobz2KC92n09t3ON/KjO0Uf8sbRvPfe4/v0cShEKz2KwII4R6gdaszt40DhbrZaiq+UEjIecmbcJxfokG4yirO/f6cG4LdXgxlvg5GOyExljt/JdZrLkiM7RzcfTB/8nB3mx8wNeboyXOkauOO8mzONyV7saoqgI8pw+jWZRlRqlI27hYQX5ws9BVsuDXJOE0AGgEr1m1JUGbGhfDKdVs9KmOM3M6Y4Ulti4jC7WZM0r8YNF5wUOKG4DzWpzixf4NalLlKuRIet8/i/ILFvIpUbiSrN26PurydFPMi7lLBYs9nPtyqbqHHIq3Gu/HQkTJ8wgwpWb2ZWnsjGLxNeYPbafW1VJBXXs/+iuqgQ/Agf+7s7s4drFuhT4eGOTf1OeytproJtsSSByJxXG9Ft8I5mPzj9K4Fij4412CAD90aQllWoynw7C1fR0s7geUuuoWznYPyiPjinrXzZf/GRdKrQzP33bc6zDJQt1V3adHlrVt2oAl5CdL3aNdUnMzTi3mrzHNMhwNuDzfPg7XkvY2y//DH2pZ/UDBxiwk+w6hw/lwSrsUWzdN5YMTxwc1L5O+Lx/16fLh9uXukMkb0oRfZ6n3mh/HvQumO2+9wByIAFZF5+g8raAxgMSUj3KZ/z92xC3CESyiOUyiZ9/fctxfVvO8JgfYeGhxGWIDNvP6AFSzuiGmKjIxzcQaaP/DduJ0w2t15x51yv+H3AVb9dxIYO/+dZ/5Pqox51t85g83Ta0UK7nrAolGLpo6Ukwc3yrljO2T2hCHukRMYzdNlwKh4d24FAiNH9CmkcHs5pm1zCmlrLI/mcp1UoFGMeqZ5/Wry3tx35Nvju+TUoc0yckBXdz2M26F7DBwTlLsczFPe6HPQG9cIwe3/mQyDDfB/PAfHP//KVd5j9wNz+urzhVpNdQPRlAfM5wd/6i3Ug1hMy6lwH9fExDjCzd96FGtuYFomcDol2K3E2E7xt7xhMNEffwZ/2jMnjOfWZvWqqfPSpc/3+P1o36yu9ikULsY8h/PEkW3LLI+Nr8f8KW/bLmIbTAk5L3mTkPMLpjYJxj6EQ7jK6pWr1+Tvv51PDWduKyQ08ByOfJ2QdkJiLHf+SszXXIm5nYN0xzXhgG5tZO+6BfLLmSOyb8MitfitnucWrlgnoybN4oj6W5Tl1RgyzuJpo2ynvcHt3PMnj9C2wgvBZ4zg1zMxYDR93+HjA1rQ6INDR2XwmKkeo+gxYq1SmWJhC1pgQRa9MQP4PeYFVpOCNI8+rBbgMNqz/4jPW/swmtC8kjxuu0NeCKfEdpzM+4uAERZZ8dVjjGDSbtdx0R8YyamfJNDwxucaIQ300aLe4Dt37Dmo9kGH6Y6M07g8nOohec5094qT/IFFefR5nqPZPfckjze906Gjx1Xjws5V14WFccqLpMw4hdqJk2csF2m1gruhzGuKRAOMZEIZrF2lrGxZNl1dKAAC9egYTujc2cYFVBH0P3vevhwaYaopPc1wPs3wZGSnjAoXtHEQ7MYF7NblM9zzlmKamdWbdqj/J4R5yhuUXeRP1JP67f+vv/aKq+6LW4gK/8dzoM9ljzoXi+TprKa6gWjKA+bzw5enzjq+XfzX69fly6/Palv+C/VxTUywhpDOn3oUELDQj5k/cxb74jRQF8lAthPGdqy/5Q0LMxvbQE6YBzZYwfy3f/xpP/2j8dx6ztVeu+uuu9S80f4+Ij3FgRNOgrf+dgpGkjHPYd0mzG9tdWx8PRAo9DUtY7CYz0uffnFS/T8hEnR++e33gGIEkRCusvr1mfM+rz90KC/HP487hphi1Ko94o9I5Gt/2gmJsdz5KzFfc0VjOydQuG7IkTWzjB3cQ1bMGu++CxvXiZ9/GbduAN06vNYYCNB/tH2FvNW9rbptG5Ua5gVHBho9sLvsfG+Omu8rUnCx2qROFW0rFoKPdVp2VxWtk9tW8J4t7++Tll3filfR1K9eQV7MmPD56J1CYczyYtyCLLBwxXrb0V8I3GFRQuNCJe16DfGrJzHcHn3kYTUVgNGiVRs95uA1Q4B20aoN7hW8AcGEvLmyOer5dDJK2anEdpxw1wkWJzTCgjkYve3Nr79dl0FjpkixKo3dj3nL1njc1oiFZI3BfzR8sViR3cI4+M4Fy9dpW7EwMsE4kt648rnOV/7A/i5YvtZxQz2SUIfqo2V1u/YdloUr13vtiEA9tWn7HnW3jxNIh0Gjp6h57fB4Z8ZC1ZhKLDCyRZ8qBB1ESB8nvvjqa1WfRwLmEyxbu6VamKpCvTaq08gK6q1q5UtqW2hUnk5wvjXXqe9t2O7oeKNe3OjKV3rACAuXYlHDpAK/DccDDyzgZVz7wijtY49IxVJFtS0stPZNUMqLccobLFCGO32QT3QYaW8cBID/G295xXvRWap3AttNdRNteQDtRH0Qxc69Bz1+tx1MBYT324n0cU0sEFjAnRfgTz2KkXGbduzVtmIXN8a6CYFCPtAXSkUHlT6Nk51AAtnhlPrhVB7tFKflDe/ZtvuAtuUd2gn4Dt3F733f/XjlZyy6bx8wMZ5bsVbGsU+d3cmFY4bBTPqgDXS2RBukmXEai8tXrvrsAE9op2A4GfMc2gybXWXU1+AVwHsQINePXTiD1Obz0tot76v2ui8Y3d2sU393PT9v6RrtlViBnl8OuK4jkO8Tg3CVVQQq9x78yFHMBAv2Gs8NWFMHnSYJEap8Hax2QmIsd/5KjNdcumhp5ziBwQe4Ntfz5ZCx0yzPUYhr4VqgaIHX1TbaTXYxG0q6bLv1sFBoz/ZN1dQcf5w/JtfPfaRuxWjbpLb2jsjB4ktN61Z1F04dTjhla7eSPkPHu0esmeFkhKBqz8FjpVbzLvEC9OhVrVO1bNhG0UOqFA9K8UL5tK1YCEy27z3UclFcnCxGT5qt5tE3KpwvV4JPmqGE41amxBvuBhZgRGmrbgPVSdW8ojWCviPemSGjJ8/WnolVOP/rqoHgBE6UwbpVKLEdJ/TelyqaX9uKhc4OzHOG4I+5YYa5z/oMGx8vmI5Roca7FtCJV+yNPNpWrKlzlqrVyc3zp+E78F34TpRPozLFCnqMxMD/i7iOrRHyR6e+I2T7ngPx8oe3/Y1mmIvOXG8NHz9dps1bGi+foqGH34bf7wQal32HTVANAaQbHp37jZDJsxYHtbMqlDDCpUDu7NqWyKRZi3w2cFFPzFiwQv3eSEj50ANqlNa23ftVPaZPbxIO5joVoy7QoePromvf4aMyff4KbUtUvYb6LanAXKSffXlKHZP1W3fJyTPntFfCw5iPUedu3bVPTd8FqAPMnb2A5/Aa4L1YZBbz04O3qW4g2vIApkvEORrQDhs3bZ788NMVte0NXp80c5E6Z9qJ9HFNLDDlUrGCcedoJ/UozjdYMwPpChiUUKJQ3gTdBv5wyhTuxQaRF7AWh12QA/lgfpSfz9EWKlYwt7blvLxtd+VZvNcXLCybzrBA44Ztu+Xi9z9qW/EhPZev2+IxkMWKsU5COUObzVe5xGe/O3+ZFK7YQA3Y6D10XLzF76MBrtfSp4tbRBTBI7vABo7Vlp0f+OwUjBbmPOdr8IoO76nWpJN7wE04g4Xm8xLasqhb7MqJflzmLHlP1fGo683T7wZyfvn+x8syc+FKn+eXaBHOsupr4BagjT1x5kL3NRzaI4Xy5lL/T4hQ5etgtRMSY7nzV2K85tJFSzvHCdSHD/zvfpUn8fjg4Eeqc53Im+i898YhjM4Z1qeTe04xHU5oIyfNkiezFpE3qzVVvVXYxqP/iHdUhflcrjdlzJQ58U7YOPEM6dU+7HcJoHKoXKaYmuffCAGf3CVrSsN2vd2/AYFQPPf2xJnau2KVKf6G5MkZd4t9tEIQom7VctpWLHSUYARqqRrN3ccLvzNXiery1shJHscJDb561cpZzu+Gi5sntdsrdWhYNGjby51+SEt9IT5/JcbjVKZ4ISlfsoi2FQsNiOJVm6iVxLFoDva3bc/Ban/RGDTC3xY1nAQBwXSsR6HfjqXD3+Iz8Fn4THw2vgPfZW7YYMqqgqZ0hCIFcqs0MkLDEHmjcsP27v3tMSg2fc37G+3QIKpeoZS2FQv5u2Of4er3oI7C70M5KF61sTTu0EctbuTE12fPqWCg2drNO+W7S77n9reDi+B774lbAwKNC3OnAjpRfAUqfEFDpnaVMu68hcBD535ve72IQNqgrOECLFLMI8awL1YNRTRsl6/drG3FjtLBCMCEypUti1Qr/6b6P/JSh97D1N0vVp2TOD4oi+j40RvYqM9Qr4W6kRpOxruIkCYIAFtd5OK59zZu17bQ4fJgUI4J8nGB3HFr56CjGReNgLyCPGP22KOp3R14CM7jjisdOi/tbi2PpjyAczPO0XpwZt2W96VNj8GWHdmAMjzg7Ynqfb5E+rhGir/1L95fvUJJ9wg5u855wGehoxgdxrpalUrJKy+9oG0FBtM4Gad+mjRrscxfvsYyUI/jOXjMFEf5INLQdkEbBrDffYZNkKWrN1n+LqQ3RvlhYBDe6wuOXaH8udzlB6MD0fFu1Q7AMV+2ZpPHcfPGfG5FOqMdhQCmFfwWHCvjZ2MQEwIj0Sivq12NO44A518MWLAKIuF4oAMUx8zJ8YgWxjyH34X6G/W4VXkGDFDqNWSsOw0icY1ovt5r23OICsBbnZeQl1et3yb9RkzUnhE1ytl41wpYnV/s8jF+P74X12mJRTjLKs4N+GyrsgIoI8PGvetxrYVrGG+DBvwVinwdzHZCYix3etmAa7/+Jr/95lnPYd/1QW/huOYK1fVjtLRznDLOQrBj70FZsGKdZZsBA72274nrQE4d4al4KDISdZAesIDsjHGD3AXUbOcHh1SQt/eQceoxzFUw9xz4UHvVEwrPrPFD3I28cMNtV306tYgX+ESFiMUj9N8wevKceKP/USG3bFjDY37vaIWe6Z4dmrnngTMyHi+r3wk92jeVkkUKaFue8NnmThvASHY9/ZCW/930fbuaN4ntOKVOlUIG9WjrDv7o0HBB8Lbf8Alqf6fNXRavkYbfiDtn8BlmaHwP7tnOozEA+Ax8Fj4Tn43vMF8IYV/6d2ll2dGCtOnQvF689AWMKNP3F51s5v1NDBAEQzDNKv9j9CzqKPw+lAPctuoPpLPVNAEIDt/4K/5FkT/QcDWWLYxEQEMHDVJM54FjjovihH4PoMGkz90OGHVQsFw9dVcA6ggcdzRiMJVP0cqN3BcPximYwgmN3Iqli7q/Hw3FKo06yPh356u0QcAAF5/Vm3T0uNBBENdcfgKB4C3m19TLOOqiph37SeUG7VSdhH345vx36iIVnT6420wfEYVyhvoM9VpSgrqlkuuY6HCRW7F+W5m/bK06HkgT/B/P6UFBHAsEx9DwD4ZXM2dyHxPcXqvXg7hdH3nGDGUMU/mZoX1j7ASyEm15AB27xuAMRhqjExL1t1UZxgUf0t/XuTEajmskBFL/Pv/s09KldUN3HYPAAjrM9YAV8oOeXiWrN1MdxXoexfmpdePaaqrLhECaI0ihn8/x+c07D5CqjTuoOlE/Ztj/EtWaqn8TA+RDtI3034X8XK91D6lQr7VleavkKodoDyJ/O6nzcado8UJ5ta3Yjl+UE5QXlBtMPYEyVbtlNzUIxdzG8gZ1UssGNbSt2IXp8pWpowYHoFxi/nvsO44NjhGOlf7Z6MhDmY7WcvTM00+4rg3i7hxF2heu1EgNeEBbCscDI+yR/6s0ap/o2o/mPIf6G/U48h3yAsoSfiPuOkVbCQNb0BYB5LlIXCPimqxFg+ru85Je/lHfoN7RywnyGwbhYGpQ/bjgb/C3+AwzjKRHYFGn52P9WCMf419sIw8gfZycX6JJOMqqXh+Zy4qefhgwhPO2cbAZBm7VqFgqaIM6QpGvg9lOSIzlDh0o+jRz2JeeQ8aq6yicg7DPmL3AeK0Y6muuUF4/RkM7xyl0OBoHIeJav2G7Xuq8hP1EmcMamdUad3TXgxigY1w3gG4difeKxQCLra2e946aoiZQaAgsnzXOMsAbTuh0GDWgi/tk4AQqpneG9Y43TUg0w50KI/p19rhNyYke7Zqola/tKtTSxQq6G4ShktiOEzqeRr3VzWtnlhWUhZnjBqnfagUNtCplS8ggi0C9HewD9sWuMwzfObR3B485Ru3UqlxG9JEOiQHy/7A+HVXHoBN4X6eWDbQt73Ac9IaZEQJ4dyePHxT0l3GkGho5aNC//mYNyVGsmqtBNFiN2AgGlG+UczSMdQg6ooGIu6Mw136eUrVUA1KfQgR1eO+OzdX/IwGdVm0M+4vGcNcBI1XaZClYQWo06yxbd+3XXhWpWam0VDDMhZlQOC4Thvb2qPvwfbi7B/uQ8fWS6m4l3HKuN1BRf6Ee81bGE7tybxZW+UKHiwwEzHA8kCb4P57T4YLYKkgeqDSPpZacWT3XBPEVcH/R1Rg319OYn9LJiLhoygMIrPTu1EKwYLIOZRh3Qnkrw+iAN681ZCXSxzVS/K1/9XP0qLe6us/R+Lt35y1T+QD5QU8vXBzq0C5D+yxYd5RiHZZOLet7tBPQ4Y46UT9m2H8cM7ync6sG8UbPRiPU+eZ2oF15Q7mcNKKvPJ0+nfZu7xAY6tq6kcc1CcoJygvKTaY8ZdRdihiAAk7bQAhAtWlSS7XbdAgGYHAAyuXTrxVT+45jg2Okw74P79spqjtz0fHZulFNKZgnh/ZM7G/DgAdMAYLjgbVjkP9xPBCgqVe9vPbOxAF5bvqYge48h9+BOziQF1CW8BsRJERbCfUtoEyhDgjG9CSBQJ317uiBHu1d1Deod/RyYs5vvq4T0Kbt3bGFR543HmvkY/yLbT3Y5fT8Ei3CUVZRBpA3cK1llX4IIhoHm+HcgGsXq4FbCRGKfB3MdkJiK3cZnnxC8hh+C84TpWu2kMz5y6l9PnLMc0rOcFxzher6MVraOU6gXYw0NrbRkY9wXsJ+oswNHDXZnYdQLnFdGakBaBRZSSJID1h8csbYQbJp6bset9b6gh7h91fPlXGDewb9pBMIVDYI3GxYNCXeFCVWsr+aWa0CjYBPtK4c7g2CifOnjFAXInrF6g0uVOZPHiH9urS2veUfsNr3kF4dQtrhkhiPkz+dWejoWDl7vLrYwW/1Bid2NGzw2/AbfcF3Yx+wL3bwnVXLlZDF7472+bnd2jRWJ1qnAf1ogfy5wJX/caueHaQZ3vfKS747WPCZViP00Vh9PE3cAr2BypQxgxpxG460Rjkf6roYGD+kl+33oe5A5x3KvK96JJRQFto1raNW5ve1Hw1qVFQXoAjEBBOCYUtcZcZ5GR+n6jG7Mp6YIQ8hX6COsIPj1atDM3WHl9UI90Dhs4xT3sCrL7/gnqPbCoLxxgVkwbwmiJ1oygNoU00c3kf6d21tWyZQvlHOUX7uusv3lDSRPq6REkj9i3oJ9Y2TczTSC+0xtMusOnsDhbzVrG41Gdq7o+2+4zW0xetWLS+33xb97Vn8LrT/lk4f7bPDHe9bNHVkvPm17aAszxw32OdxQ7AE03Q6zRcoCzjOqCectJNRl+C95kXvoxGCaFNG9ld1mx0cj7GDujvqMIkmyHOF8uVU1x6+fiPo1x6oA1AXRArWsVriKifYD19wbJbPHOvzOgHBtXdHv6Xaf8E8v0STUJdVXI9WK1/S0bUW6pk5E4cF9dygC0W+DmY7IbGVO/x2/G6nA8Eg1Ndcobx+jIZ2jlPoqMB53dcgVZT3aaMHSKmi1jNHUNKXZIL0gJMNRilvXzlLju9erYIgxttsAZkeU01gNMuZD7fKshljVM9ptAW4cfsOGv7Hdq5SJw9cpOuVIX5PlXIlZMGUt2XbipnqxIETSGKEynqw6+Li6I6V6uSABVWtfuehLUtVMNPJyQ5pgeAy0gbBW31BFHwuPh8Bg2A1zBPbcUJnFk4OKB/m/X018wvSpnFt2bVmrquBPE79Nif0xsuOVbNlw+KpamSSsTGJgBO+C9+J78Y+OKF/LtIOaYgRM/rJHZ+PfUW+GNijrVqMJTFCWsyeMFSlecOacXP8679vz7r5qvPRKs0QAEthauyggYlGUN/OLdVn4TF6YHdp1ahmUG5RxzFB58nWFTOkdaNaHvuLKYqwmHcwRuzr8Htwq+ix91epsmysH5Bfka/2bVgob3Vv634+krC/GI3y4bblat+wjzq9fCHPTh3VP2SdwnqHtV4nGQO+5jIernkYIwn5AnUE0h2/3apuwvHCeQEXKcFmnPIGMHe83fcgGG8caOBr5L2VaMoD+K3YB5zjB3Rr4y4TOC4ozzjvH9m2TJVzbxfIViJ9XCMh0PrX7hxtPA44RmiPheoiGrfT41gjH+h5Uv/+cUN6yvE9a6RutXIRCWoECmmL8okBQmsXTBZzOwVr96CsLZ42yn28/IF0Qhto3qThKjikfzbKEY45yjg6NlKmeEg97xSueSqVKabKz+p5E7222/D53tog0QqBFwSE1y2cotrgxnKS0OMRLdA+x0Cag5uXxKvj8TtxPFHWUeaj5RoRi3ki6PTZ3rXxrvewz/p5CccGc4o7gb9H+09vcxnTAdd+gZ5fokmoyyo+3+5ay1jPhHowY7DzdbDbCYmp3GF/MEMErp30NiiOK47vW93bWN6dGcprLqRFKK8f8fmRbuc4hXy0wpWPEIM0ljf8i/M86sn9GxepDstoqLspMm6L8XeVBiIiCjvMb47pU3TF38gjc10X7qn8vDgnIiKi8MPUEbWad1XzGqMDbtG0kR6BIyKiUFq8aoPUb9NT/R+BUQxodHqnHhERhUeSGklPRBTN5i1do+YF1B9YXBSLDvmChV8/PXFS24qV+uFUajEeIiIiIiIiIiJK3BikJyIKk7RpHlGr5uuPtZt3uv49IL5uaMLq/NsMi44C5qi//757tS0iIiIiIiIiIkqsGKQnIgqTjBmelNzZX9W2YmEam137jngN1GOkfd/h4+XSDz9pz4ha6R0r5XOuOiIiIiIiIiKixI9BeiKiMMFCPVjIzOj8txelZrPOMmbKXBWIv3nzpnr+6i+/ytylq9WUOBhJb4QFsTNnek7bIiIiIiIiIiKixIxBeiKiMMHI92rl35SyJQppz8T6+dov0nPwGHkyaxG5O92rkixtFnk0Uz5p2rGfWmjOCKv0t21Smws9ERERERERERElEQzSExGF0aOpU8mQXu1VsN1f6dOllaG9O8oLz2XQniEiIiIiIiIiosSOQXoiojBDkH3muMFSulhB7Rnfsr+aWeZPHi75X39Ne4aIiIiIiIiIiJKC22K8rVZIREQh9dfff8vqjTtkxDsz5LMTp7RnPWHu+eb1qkudqmXlvnvv0Z4lIiIiIiIiIqKkgkF6IqIIw2KxP/x0RU5/c0FOnflG/vrrH8n47FPyzJNPyONpHpU777xDeycRERERERERESU1DNITEREREREREREREUUI56QnIiIiIiIiIiIiIooQBumJiIiIiIiIiIiIiCKEQXoiIiIiIiIiIiIioghhkJ6IiIiIiIiIiIiIKEIYpCciIiIiIiIiIiIiihAG6YmIiIiIiIiIiIiIIoRBeiIiIiIiIiIiIiKiCGGQnoiIiIiIiIiIiIgoQhikJyIiIiIiIiIiIiKKEAbpiYiIiIiIiIiIiIgihEF6IiIiIiIiIiIiIqIIYZCeiIiIiIiIiIiIiChCGKQnIiIiIiIiIiIiIooQBumJiIiIiIiIiIiIiCKEQXoiIiIiIiIiIiIioghhkJ6IiIiIiIiIiIiIKEIYpCciIiIiIiIiIiIiihAG6YmIiIiIiIiIiIiIIoRBeiIiIiIiIiIiIiKiCGGQnoiIiIiIiIiIiIgoQhikJyIiIiIiIiIiIiKKEAbpiYiIiIiIiIiIiIgihEF6IiIiIiIiIiIiIqIIYZCeiIiIiIiIiIiIiChCGKQnIiIiIiIiIiIiIooQBumJiIiIiIiIiIiIiCKEQXoiIiIiIiIiIiIioghhkJ6IiIiIiIiIiIiIKEIYpCciIiIiIiIiIiIiihAG6YmIiIiIiIiIiIiIIoRBeiIiIiIiIiIiIiKiCGGQnoiIiIiIiIiIiIgoQhikJ6Ik599//5ObN29qWxRMSNvrv/+hbRERERERERERUUIxSE9EScrP136Rdr2GyNylq1VAmYIH6Tl/+RopV6eVnDz9jfYsERERERERERElBIP0RJRk/P7HnzLg7YkyY8EK6dJ/pAooM1AfHHqAHun6waGj0rHPcPn67HntVSIiIiIiIiIiChSD9ESUJPx54y/pP+IdmTpnqdr+7frvKqC8dvNOtU0Js3H7bpWeSFfYtnu/dB84Wn746YraJiIiIiIiIiKiwDBIT0SJXkxMjKzZtENmL35PeyZW7SplpEThfNoWJUSxN/JKh+b1tK1Y67a8LzMXruTdCkRERGQLdzviTrydHxySK1evac8S+eevv/+Ww0c/le17DsjF73/UniUiIkoaEm2QfvGqDZIsbRbHj2dzvim1WnSV1Ru3qxG3FDlopC95b6O8Wa2p+/jkLF5NmnTsG9D0GZgbO3vRqh7HG/nDCYwKbtiut8ffFqvSWC798JP2DntHj38hT2Yt4v7bVBlzy/4jx7RXEz9zOStTq0VUXljhOPQZNsE9yhtaNKguw/p0kvvuvUd7xjqv2D1wPJFPx06d6zhPBMPln69J2dotPfZl6Lh3VWeEL//9d1P6utLC+Lcv5i0jn544qb3D3vlvL0rBcvU8/h754J67k0unlg1UuhqNnjxbNu3Yo20RESUczjM43+h1UCjOq6jT+w2fIJnzl1PfobcTN2zbrepRInLG2LbCv1Zr1uCuu3qte0jhig1Uu6p83dby5akz2qtEzvIRriG7Dhgp+crUllI1mksBV3t1z4EPtVcpsQnHuZ6IKLG5ZUbSI/C0Yu0WqdakkxSp1FAOfPixo4AXBRca6QjGo6GOkTS6jz/7UuYtXSM/XvZ/6ow0j6aWl1/MqG3FQtD273/+0ba8u3rtFznzzQVtKxb+9uz5b7Utex998rlH8DZblhfl6fTptC0Kh6u//CqjJs9WZVxXtkQh6d2xhUeAPhAI+iOfYlqXrIUqyZTZS9QInlBL8eAD8mrmTNpWrONffCW//hbXCeHN9d//kFNnz2lbsdD59dmJU9qWvZNnzqn6Uffs0+klc6bn1P+RnkhXpK8OaTR68hw5dyEu/YmIotnBjz6RsrVbyfAJM9yBIL2duHH7HkftByJybs+BI+ruOx1GQq/hdITkJ7RlF65Yr21p9fa6LXLjLw7AIyKipOGWnO7mw48/k4r12spy18UYA/Xhg+DmmClzZOW6rdozwfG/+++TVzO/oG3FQtD/ys++R3ybA5KAoOORY59pW97hIt48yiNTxgyS4qEHtC0Kh8079nrkqfTp0kr7ZnXl0dSptGeC4+drv0j73kOl1+CxaiRPKN1xx+2S67Us2las459/Jee/8x0Ix3vwXjOnHVeff/m19r9YGZ95StKlfUzbEpWuQ3q1l5zZXtaeETXyZcnqjRx9GmE4vktXb5KRk2apB4KNROTpx8s/y9vvzFQdn0QUOXcnT679jyhwyZMlk9tv5wy+RESUNNyyZzQE3HoNGacCVxQeCP5hmptQyJE1swrW63Bcz397Sduyhg4ajOSxgiC/ceoUK+gEwPuMXs6UkRcdYYS7GHAHhlHLBjUkb85s2lbwvTNjocxatCrkHXwIjmd58XltK3Y0vJNbwzHKyGraKCcdV8jzVnn6gfvv17ZiPf/s09KqYU1tKxZGNn319VltiyIBnSS47bu369yGx7FPT2ivEJFu/+Gjsn7rLm1LpE7VsrJrzVw5sX+9rJ43UdVvRBRcBXLn8LgLDx39JQrl1baInMGdnVhvSoeBOchXye66S3uGiIgocUsyQfoXnssgfTu3lCG9O1g+MJcyTuRGuEVuypwlIR8VS7HMU8MAGls92zdVx6hzqwZy5x13aq/4B1PMYKoZHYKNvkbJYeoQb+/59IuTPucgx5Q4xk4eTLvz2isvaVsUDpt37pUdew9qWyJv5M0pNSuVUiPRnUKw2Vxf6I/+XVtLvlzxA/7T5y+XEydDO5eq1TROxz8/aTtaHSOpvXU84nlf0zghzyPvG2FEvzk9b7vtNilZtIBULltce0ZUB8K6re9zND0RRS10rn5qmPoLAfpxg3tKnhxZ5Zmn0kspV73WrmkdtQYHEQUP7sKbN2m47HxvjmxeNl3WzJ+krt2I/IFpF0cO6CofrF8oG5dMkz1r50mB3Nm1V4mIiBK/JBOkT//4Y9KqUU3p2rqR5WN4306yc9UsdQFmtG3Xfo7+DANcGH//42VtKxYCqusXTpG3urdVxwiLfBqn0PBHqpQPWU55Y7dIsLdpQQDB+/Pf2Y/Ex5Q4xtH2WV7MGK8jiEIHc9Fv3L5X24qFAH3axx7RtpypVuHNePWF/ujdsblsWDxNdQAaISC9bfd+bSs0rKdxOqF+tzdWd3fokFfNU9mYYfomY8cVRvJjRL8VzJtfq1JpjztYNm7bI99d+l7bIiKKLjf++tujLZI7+6vywP887xQiotBAgBUDHwrnyyWpUjykPUvkH0xvg+vFogVy+93mJyIiina31HQ3CKB2btVQjVDVYeQog/ShZ74whucyPCkpHnpQ20oY3OZoHEkPmPbjJ5uFaBFotZoWRIepcLxNafLHnzfkxMnT2lYsLPSJwCWFB6by2GkaRV+iUD5tK3gworJ5/epSpvgb2jOxENC26wQKBvM0Tse/OKnuAPIGUzzZTeFl13GFvG6eHgUj+Y31pVm+11+Twvlf17ZEre9g9/1ERNHk/vvu1f5HREREREQUWbdUkB5wa+WLGZ/RtmKdOWc/BQQlDpg659mn02tbsQFDLAxrBVNyYOoQo5JF8psCol+pKXGsXL7yc7zpTqymBaHQwPHbe+BDjzsZcJeMXUA5IVKnSiFZX86kbcU6d+E7+ePP0E6VZZ7GCZ2KmDbKG+RZY5pgxJrxdnK7jivk9c+/8hxpj5H8xjJhhk6pUkXza1ux9hz4SC0STeQUp0hKWtDhd/PmrXdMb9XfTZSYsdwGB9LQ28AmSppulbLDOoKIwu2Wiyjefvtt8QKpTkZSYd56LHpasX5beezF/JIsbRbJWbyaDHh7opw9962qwDFCtXX3Qeo1/TFk7DTtE+xduXpNJs9eLEUrN3L/Lf4/avJsuaBNu4L3lKnVIqDPDwROSJgeA78xf9k67u/MnL+cNOnYV7bvOWAbjDPu74MZcqh5vI2wjef1z8UjIb8n/eNpJctLcQttgrfpPTBlCKYO0SEQWaVsCcn8wnPaM5j/G1PeWI9aRvAfnQA6u2lBAHkD06Mg3ZB++u9FuiJ9MarfSeN28aoNHumVvWhVNaIbxwoLRjbu0Eeezfmm+3W83wref/CjT6RT3xHu/cHfNe88QD0faGME+QH5wvg7U2XMLWVrt1TlJ1jrP+D4ffjxZ9pW7PztWCwWc6WHAj739ttN9YYrzzhZQ+HX367LsjWbpVaLru5jgzR5s1pTj/JtxWoaJxxvzD1vhjxmnuoGc8a/+Hxcp6Rdx5V5+ieUCYzk9wXpblzgFp0IP5jumgkX5K+N2/fEy39O0tobdIyMnTpXfQY+C5+Jcjt03Lty7sJFd7k1lk2cB6zuWDCfI/Tyic/AZ+EzjXUtzgE4L/x4+Wf1PivID6gH8H5zPfvWyEnuz9If5n3bf+SY+zXU16i3fTF+p14HWUF9rn+2Xrf/dOWqSk/juW74hOnqNStW5QfnYJyLZyxY4Wh/jZBH8Hk1m3dxn8uRV1AXopPLST1sBx0OxnTvOmCkZXk1Qh4rWb2Z+2/Gvztfe8U789+g3vXG6vxjLBfGfGzH6rhbnXtyFKvm9S61f//9T+U5nGv09+Nf/P3u/UfU68Fm1xap36anet74MJeRYPxuvN+qPYV2JPJeoOddb+mJ8rJ55wfu32FuP+JvzLzVT75YlXNf9P3Gb0ca6H/vb5vI6rvx2chLxmOjp8mW9/cF1ImMv8FnJmR/jXWtXb1pZMx7eFgdt1BCnkA+sqovceedv3nWn7yCzza3U7EP2Jc1m3b4PI7RXG69nYfM11aBHH/kQ1yXGvdZr+/N7Qknx8OqbYP9n714lWrb6+2itj2HqLumrQQ7Hzlpb1kdf9DbdMY2CNIJ7a9A2oje8qm5jWKuX33l/0DgO26Fc32wymUwficRUSg4CtJ/c/471bBFwwEnt8Ts99//kN+u/6FtxXrmqSe0/8WHk9nODw5JwXL1pF7rHrJh2275+dov6jUExHBSz16sqgwbP911cryhnvcHTlJLV29Sn9+h9zB1YtDh/70Gj1Unh2lzlyXohOavby9+r07yWQtXUr/x0EfHtVdiT2rzlq6RUjWaS6EKDeT9Dw47OumH2gP/u88jWAg4RsaRxTpMGYKpQ3QIzmd/9SXJlDFu1DFOyBh5bIbfiqlwjLxNC4L3In2KVGoopWu2UOmG9NMhXZG+WQpWUA0CpLu/rrvyNAJBaHDOX7bWdjoU0I9tgbJ1ZeLMhe79wd+h0Y3nW3R5K970RL6goV2ialOVL4y/E+mP+gPlp1iVxh55KVCXfvhRTn9zQdsSyZnN1fjLEHcXRbAhyHbNNBf8E2kfk3vu8b64oF6285SqJXVadpMVa7e4jw3SBPUKyverhSpJ7yHj3PWKkdU0TsjTmHveDCPkjfkV+fH1116Rl55/VnsmlrdpnPC3xkYovhcj+X3B9xgXuP3sy1Py7aUftK1Y+L2DRk9RDV483pmx0PKiKlBoVK9av01ylaguFeq1iZf/9LRGXYoAqJPvxkXylNlLJKvr+HQfOFp9hl6XIA/j4iBbkcpa3R/4b8H34MIZn4XPNJYPnANwXshSoLzMXbraZ1Ai2uHCCYFlpKfxXGfFrvygrOBc3KrbQHXuRAegk7RB2qIOwuetXLfVXeaQV1AX4nyGAHtCOhMxCCB/7uzuO1BQ3r730W7Cgs6Hj8WdU9CZ9suv17UtaydOnXH/TcE8OeKVc7A7/xjLBfJen6HjLesgOzj3IM86PfcgcIQgzRvl66tzjf5+/Iu/x7Gp2rhD1E9B6O/vtmtPoT5H3sN5F+9BEMUpu/REeSlXp5VUadhePvncep2SSMH+4Dhjv/HbjZ3LSBukUd7StVWeRKeeP/DbkSbIS8Zjo6cJgop1WnZ3P+8LyhDKI+oGfKav/fW3DEUr/G50uKO+Rj6yqi9xvg2kregEjjs+29xOxT5gX6o27ig1mnb2K3AWDeVWz0/ezkP6tVWl+u0CKreo1/uPeEddlxr3Wa/v9fYEfn+g15Rfnz0n1Vzpj+Ar2vZ6u8hKpPOREX4vfrfepjO2QZBOyBtoI/qTNthnb/lUb6PkKlFDtVH+8dFZnxC30rk+VOdT8Pd3EhGFim2Qfte+I6oizfh6SdWwRcPhyaxFVO+wsZGaWOAktm33AY8R0Fh4xjh62gjvR5CgaqMOaoSdNzgBolJH760/Jz40AhAwwklcP5lawWe27TlYRkyYoeZCDzUEUirUaysLlq/TnvEOo5mrNGovU+YsCbjBFywY7WxeePbTL05adiyh0Wh8HlPlPJU+nWQyTYWE+bXNoyAxLYg5P1hNC4L0QLogfYyjvr1BeiPdkf5OYbqV4a58gaCnExjtVaNZZ5/Hds6S99SID3Ow1Rt8LvKor1E+SAd0RiR0NBgaXsaLMyza+9AD/9O2gg8NSVyMGOXNmVUF0a0gyDdw1CSp26q7bdkG1B8jJ82Sui27W15wmqdxQp5EUM8s/t0dGeXJJx6PN02P1TROyOPmueSRp1Ol9L2wG/K9cbQ/fo/xDhYEsPsOm6CC9Gjw4tG53wiZPGtxUKY5QXB2zNS50rRTP59pjboUHVqoc+2CPzh+uKBp33uobZ2u1/3dB45S//cX6ogJ0xdIxz7Dbf8e+9C0Yz8ZMmZqggLIkXTQlTe79H/b9lyqwzEdPWW2o/KD19EB6CttUOfXbtnNti7GMcBFHzpNEnI+Q2dxnhyvqv+jTBo7hM3QzkBby3j87e7iAvN0X+iMe+ThVOr/Ouz/HFc97uT8g8+xq4Os6OcePJzAeQ2BNYyAtYPABs4R5unkooW/vxt1oZNzLuA9lRu2d9QGcJqeGFWJc3k0pKceSKrcsIM6znb0PNmgTU/HeRLnxY6udrivNMHreN8PP3lfswjQ+YtrgIqudlkoylC0wnFCGlVv2tlnW22O1lYM5gAu3CnZrtcQ9dl2kIcQbPV1HCEayq2ernVb9fCZn1BuG7fvKx8cOqo94xvSoVnn/uo3Gs8nZmhPoI4dO22u34HjU652Jtor2D9fIp2PjG7G3JT5y9dIhz7DbNt0eA3vWbFui9p/Oyjnjdv38ZlP0ebFb5s6d2lIBlrcSuf6UJ1Pwd/fSUQUSl6D9OhBLF61sbolyQwVa+6SNWTTjr3aM9EPt8zjFmeMWDWqXqGUPPlEWm3L096DH0mfYRNsGztGi1auV6MEnMDJH42AoeOmOf589A7702ALBIKtTgMpOuw/gnBOGjWhljHDk5I7e2xwBPA7zEEeBA3NnUwIMmKBUCejln/46bJ8fSaucWM1LYh+fJEuTo8vYH+R/jgOTqCRtXrjdm3LHhrwvYeOj3cXgDfrtrwfr7xYQWAMt1E6/VwcD3Qq4EIsUOa501947mnVSRNsaPyiDmzbY4jHMcE0MgXz5tS2POFvEOTzt6GHix6rC07zNE7IT0eOeTbEkd/M6a8vZIwFmhHo11kFABG0N+c5lAVvnRBmL73gOYoXo4v0ugCjrrbu8uzggLWbd8p3l/y/c8QIaY0gNwLqejlL+dCD0rZJbdm8bLp8fXiz7F23QIb07uAxHdXCFetUUNdqBLx+/IwdX+bP3L9xkYwe2N2drlPnLA3o1uV35y+T4eNjp3rJ/mpmmTlusBzZtkxOHtwkq+dNlGb1qnl0/iFP4buMnRsZnnxCtq2cKZc+3yPfHN0u9atX0F4R6dGuiXre+Bjau6PcnTyZ9o7wQScX8ih+D34Xfh9+J34v1pPQ4bdNnLFI1Z06HDscw53vzZFzx3ao9H9nWB+VZjqkjbfgOuoodJjqo6KwD60a1pRtK2aqfVg+c6wUfyOPeg1wTN7fd0jb8l+qFA9K7hxZtS1RdYi3KW+u/fpbvAWbUa8f/ND7heWPl694vF4wbw6Pafz080+X/iPd5cIq3c1p6K0OsmI89+Az8Fk4Ligf44f28ujgw3vN7Qrz36BsoYyhrCGfYMTnl1/bd9A4hXpw/pS3LcvIlJH9/Soj/v7u5l36e9TNOHcgvx3fvdqynDtpA0Q6PQOFDit0phrLoTFPIk2QNkgjnT95EqNH0W7RyzfSAJ+LeqNfl1YqLXR43yRXm9qqvgCUIUxHguCaHtTD59apWtZnGerpOh/5ewdANMFx6jJglPs4AerH2ROGeJyfkBZIE6QlOr9/uuJ9WjZ/oHwZr6Xw3UveHS0n9q+XXWvmqmOrlxdfx1EXDeUWAyHM6Vq6WEGv6YrPbNFlgEc59wbnuCFjp3qkW/p0aaV/19Yq/5/9aJv6F9t4HnCew3Qs/ti177B7BLoxTfBoUKOix3ko0vnICHeKjp48R50Pcfz1thb2G3nLeP7He9BZj7tCvcH1S9/hEzw6K9AeHNqno2V64zMxgh11VDDdSuf6UJVLnT+/k4go5FwVfDyff/l1zF1pXvb5SJu5QMyVq9e0vwqvRSvXW+6TP49azbvG/Hj5Z+0TPblOXDEV67e1/LsK9drEDBs/PebtiTNjWnUbGPNopnyW78Nj8Jip2id6+ub8dzEFy9WL9/6Uz70eU6dlN/XZCfn8QCAtqjTqEO87sE+VG7b3+ZufyVEi5sOPP9M+LSbG1WCImTZ3qfqbIWOnxfu92Mbz+m/FY8O23dpfB+aPP2+o/TN+z8BRk2Nu3rypvSMm5uL3P8YUrdzI/Tp+377DR9Vr5y58F1OgbF2P1w5++Il6TedqSLpfxwOfhc80wuchPYzvwwPPtekxSP1WpCfykvk9eNRo1jnGdWGofVocu3xv/Gw82vYcHLNq/Tb1d64LmJjhE6Zb/l3WwpVi+g2foP4G+alQhfqW78OjdM3mMZd/vqo+U+dq/KjPML4PefjMNxdUuuOYLFyxziM9kK479h7UPsE/f/39d4yr0eXxffrx8+Wrr8/GvFakisff+vt4KV9Z2+9bunqT+n1Wf9dj0Gif6dztrVExN/76S/u0GJWGKCfG9zTv3D/GdVGmvSMm5tffrsc0aNvL4z3IK3Dtl99iqjft5PHasjWb1Gu6jz75PCb9q4Xdr2fKUzrG1bjVXvUN78Xf6H+PMojjDkgr/XnjA8cBxyMhzOUM9RfynRXUb64LXvd7cYzWbt6pvRoHdZjxM0vVaK7yuBWc/1DO9PfqD+PvN7Kqn/BAHXX99z+0d8XBscf+GOtO7JuxnjUyf76T84Px+FiVbyvGcmR3HPH9+mfr78X3GetjM3P645h5O0/j9yLt9PciD+858KH2apyvz56LyVWiuvt9785bFvPff/9pr8ZC+huPZeMOfSyPiVP4nXo9gHMKzi1WzGXPyfej7tQ/2+r8c+LkaY96DvkH6WqV7vgOHCdjndV32PiYf/75V3tHHKv6s/eQcar+8QZ1Geo0/f34njFT5liWD0D706pthPQMBnMZ0etJO8H43cjTaN+Y850O+SBv6Vru93urQ6zSE8fPW14xf67+sErPQNIGjOXcW52D9gzaNfr7sE9Hjn1qmSfx3KYdez3qAbRf0I4xM9cxOM/u2nfY8nNRj6M+19/7coHyKr9ZOXXmnEe6+SpDxnoIjwnTF8R7r7Gutas3jcx5L1jlwBvzccIxWLxqg2V9gN+HtEaa6+/Hw+63+corf9644VEPo11z9ZdftVdj4XuxT3qd5e04RlO5tUrXFWu3WH6ut3TFw9vxR1vGWIcj3S798JP2qieci6yu9byVXfN1h92+60Kdj4z75C3NrY5/h97DLI8/jvvEmQs90nDA2xMt6xzsL8q38XPt8tV3l37wSAv94S29/XGrnOtDVS4D+Z1EROFgOZJ+wnTfi5YBRopMmrlI20pc0OM/bkhPSZ0qhfaMp/2Hj8r6rbu0rVi4hR2jOFbNmaBGKHZt3UgmjegrH+96T2pXKau9yzdXust7G7fHu/2vWME8qtd2/uQR6rMD/fxArd/6frzb0/LkyCpbl89QPdXG3/zp3rXSuHZl7V2xMFpi4Yr17tv5sCAverTxNx1b1BfzaFts43n9t+JhHFEZCIyGNy/gilHXxuk9cEu0cWqPbIa5t1M/nMpj1DFGJhhHBGCUp3H+OzBPC4IRLbMWxc3Bp2vRoLoc2LRY9c7jtyI9kZdwzM3T9GA0zO59h7Ut37q1aSzHdq50fzYeE4b2loqli6rXT39zXpav2aL+r8NIg+F9O6mRxm91b6v+pnfH5q7jPVONMjGOOrODRZHMc6G3a1pXnn4ynRrdjmNSs1JpadO4lvYOfTS4s5H3ZjgGrsaWthVbLh9OaV2Og00fgYNyYQW36s52HXt9RAsgnTGa5tCWpTKsTyd3Om9fOUuN+tBHNekwd+Xxz+Omx0AaYuoaI9weetkw2gjfi6mddJgeR8/HVms14PONo7FdDVqP24wxch8j+J1KniyZ3HvPPdqWiOuiRFyNavV//H7jdD26++69J0Ejus3lrGyJQjJxeB+V76ygrh/QrY16H+AYrdm8U32ODnUX6jD9M1EuRw/sJs8/+7TaNkMZwTE114X+QL3QqWUDlR5mOPavvfKSTB3V311HmOvZxCK2HLRSZcfbXS/m9McIuxH9Ons9T6Nu6dm+mapLAXkYc/cbjyncuPG3u85AuSiQO3u8haCR/nWrlnevLXLq9Dm58nPgo2EzPvO0oylvcMeeXvbqVisn+XJlU//HaDGr+VRRbo1T3eD9xqlu8Dqm6NDPW8g3yD/IR1bpjt/dvW0T6dG+qfaMyKJVGx3Nh4xRbJ1bN3SPWrOCugZ1mg7f07ZJHXXsrGCha5Q5c70YTfz93XjflJH9VPvGnO90aIdgVP8Lz8Wui4O/Rb4xs0pPHD+r+gPwubgzINLpifaMPsoXvxG/FedTqzyJ594snE+lmZ7GqBe+8jF/Md47rE9HtUaD1eeiHh/er5M7jTG60iqfowzhbit9xKaTMoQ6HHW5buO23SGfYzsUjMcJ6Tm4ZzupXqGk3HnnHeo5I6QF0hppblcW/IER8cb6u0j+1+NNZYjvLeHKH4VdrwGO45lzcWsUeRPJcvuBq57fqk2ZqKdrpTLFLD/X33TFqO55y9a6zwlo46Bt/9gjD6ttM9QF7wzrHfD1Vvtmdb3uuy7S+cgKjn8/VxvE6jvQhm1er7q0blRTeyZ2ek6rO37Pf3vJ444FlPueHZp53fe0jz0iw13tRG/XDYG6lc71oSqXZk5+JxFROFjWbv5MY5OYprwxwu10WHwEAXMzNBDXm+bLxElkzKDulgEGNITQINKDP75cufqL7NhzQNuKZRcMwuej8eKkQYXbtTDfnJPHxu17tL+KbeRt3O55LO1O+AiaIHiCIIoRFhc+fdZ3Y9kXXNxgGgur/TY/0EgxTiWAqWeMJ1jz9B6YKsQYRDUG2dGYMM6vDZjyRp8WA+n08WeeUxOgMWCcFuSLr07Ltl2e8zWiEYdgnjnYhHTFhaqxMaFDHjQHm6zgGHRr29i2UbHv8FGPzgZAAwrBdPPFPRrR+ExML+FEqhQPeXRsWMHvRKAe00voj/JvFtZe9c+///7rEaQPp19/u646ebwFSA999Ins2HtQ24rlLYiCxiXKtDEQAQjYbdy+2yOIjo4nY6AdjU3MQa/DFELG42sMsiPtUZaNkIf1iw+r6Z/wXQjuO4Xy89gjcYFCHKObN2P3H/tuVTeWcx3/x9M8pm35z1jOEFht07i2PJrac15uM7yOgDo6RPFAPvrRcLsvgqLGxcRaN6oVr1ya4bi2cl3Y+XqfFfxN07pV4+UNM7wP+6ILVj0bTghYG6eAsWJMfxzTlg1quvLh/Wrbm9j6qpy7fOw/fCxewObuu9GJdLf6/w1Xfvc29QymzVo0bZSqn3DbegqHHZVWzFPeoHPXWKYB5yFj2atUupg634K3AKJxqhvUG1ik1jjFwIXvLnncgo+pIbx1MumQhrUrl5E3tCm80EmC6Yms2kdG5VzlGlPJeIO/x77onRBlir+hppqxCtQYYaqupnWraFvRx9/fjd+MjiFfXsz4rNSuUkb9H3nj/b2HPPJMoOmZN2c2qWtqq4UTzjFYD0rXqFYlr2tBGeXL9ZrUqFhK/R/lAe0YOwja5nv9NW3LGs5HRQvk1rbEVVfEX98lkDKEOrxhzUrqnI5zC87v+nFKLMzHCWlfvmSReO1/s6Ku36sfp4RCXjaeD/+8Yd3We+D++6VPp+bu9qR5IIOVSJVbpOvmnR+4rzuCna4nTp6WnVq7E+fNDs3r+WwL4bqydeNafgcjcZ4tUSiv7b5HQz6yUr38m7bHH3mvbInC7o76Sz9clp+vek53Chhgpw+yQ3qg7emrDYdpdZvXr6ZtBcetcq4PVbm04ut3EhGFS7wgPS5e/WlY4iSRGGFefaw2v3xt/HnUv//xJ4+RwYALHFzoeIMGkdMGD06K5lF1CBzZBXnQoGpQI24eVW9wAd97yDhHD+M8uFiJ/fBRzxHiuOCwO+EjeIIgit6gAVxImRegDASCoZi7z2q/zQ8EdYwnXoyKR+Bch44L/XiagyJgDrIjSGL8Tfjbn1zpCpd++FFOfxMXAML7zGmExpuxDDkJxOGCFReuRkhHdCT5gkaFXSALI5rNcx6jgYaGmrcGFBrTGCmDhrUvjz2S2uPuBfx2LLiJkWj4vx6sRRnBaBn9EUhQE1BH6ccDEBzWO1lCTV+kEnOgmztQsF/7DnveHeMrnQGNTTQ6jRCE+9mwuBXy2cumi1CM8kPdhbxvvrvDHGSPv1bDSfdo5Wu//Bpv3QZc8Pq6oHIKI5T6dGohfTu3VJ2deGA+dwS2jcFFfxnLGYIz2V+NDW76giDKhsVT1WPR1JEeI++NnR3Io/qFjC/PPv2k5PcRHLKCv8HfOoF9wT4B6tnPv/I+X2o0Qnn31fFjTP/cOV71qFfsPPF4Gsn1Whb1f9T3SB8j1FF6RyJef/udmeqcZz734/yNBaGRzvjX3wCGEcrPG3lzuD8Di5fh/Gx04bvv1V0sgPL5smsfjecurEGDgIfRCddvO6zdhZQz68uSyVSPYh5dfQQwfkehfLkclWWM9qtaroS2hfrluJov3xurc5+ZeZH1kkXyyyMPp9S2vEO9UDjf6x7n4WgRyO8unD+Xqgd9we9GnaDnmU9PnJRfr19X/4eEpGepogUjlp44X+ttMOwDznlO8iQGTeh3lgDSQ79Dy4qTBeRxHIzrUGGNIfNnBlqGsEg77o7Tzy/GspwYGI8T4E5Mb6NgjfCeYgXjOj4S4u7kyT0Wu582d5kK0GGEvRHy9CsvvaCODx6+RuNGstya0xXl1mm64r2+GAceoS1kHmjkTdaXX3Tf7eXUsxnSu9ry1iP0ddGQj8xw/s+U8Rlty7tHU6eUp554XP0fA3KuXvMcSY92vvEat1C+nPJU+tj3+4I2hbEdnlC3yrk+VOXSzMnvJCIKl3gREgQrnZxMdQm5iA0mBAAQBMLoX2+Pnu2beoz4RRCs15Bx6nZzo8tXrqkV7I3yui4UUOHbeTlTRnXR7MvF73/0COJin5zcBocGfzBP8EaYDsW4T5ieAqNvfZ3wEURBMMUIDRtz8COcELA1N1LRqELjCkESjDrRGacF0aGxbxyVYxy1jIYngjw6vM94cYCLPaSlEYL+eqPPG6QzLlyNDRV814+X7RdQctLwRJAHt2ca5cqWxWdDGxe7TkYnPfjA/aqTyVgX4DbRhu16y5NZi0jG10upFfxXrttqeeuovzBC7c4779S20PHyh/z++x/alv8w+sSqvtAfA3u0U4t7GWFRUfMilQjam+sNJ+mMxiYanUY/uPKpcQQP0tacp9FoReMVjU40Po3MZdc8jRPKuh4cNE//hAC/0+CoU9h/1M9YgAkPLBrlz3nGDGXZ2IGFEdDBOBcZFyTOlDGDpHjI2Yga/BanF8ZG6dOlcZwO2Bfsk+7kac+8Fu0eevABnwsRG9N/1fptkvqFPJIsbRafjwcz5FALw+vMaYMO0nrVyrvratx99XzuUpKrRHXpOXiMmuYNnVZ6h2KwGKe8weKxxt8HGCmvdyjgPPHoIw+rf/W7AvD6D4YpM9AhZ5zq5vXsr3hMdQPGTmR/8jDge/Vy9O3FH+Qnm/NP6lQp1ZR2di7//LN8c/479X98rv67nEiX9tGg10PB4O/vhkoN2lnmW6vHG+Xru4+v+Rgk1vTEiFR9EUace15/s4blb7d61G/TU/0dnHHlbbu7C9E2cBKkMgaPUKbMzdWElKHEzHiccL3xtMPgIzzjaktbTWsXCHQo6XffoZ1TumYLeSFPadWOnL14laoX0QbwR2TL7TX55kLs56IdhsX8nUKHkl26mq85/GkLYcQwRjL7A3fOJktmfx6PlnxkdPttzuqG+1x55H/3x+YTHM//bnp2Dv35519ywdD2REeRr3aNDnfmZXjqCW0r4W6Vc32oyqWZk99JRBQullFnuxHjZnlyBneOtUClf/wxNTJTn4/b6oF5t7csm+ExvzsuzKfMWeIRPMRJWa/UASeb9I+n0ba8Q+Xu5ARs/GxI+2hqedjLnLtGxsZDsJlHqTyZLq0afegLfvNzppGgmKrmxl+Rmy8ZDSbzCCYEQhDQPOs60XsEJC3m3rZquH7+5dfxRlAA3me8NQ4Xe+bb6TI8mc7RiR8BenNDxddIeicNTwSeMP2IEeYC9NXphM912ljG7eOzxg+2bGihjM1ftlZqNu8imXKXlrcnzoxXBvyBkVaPp3lU24oNjickv1Wr8KZlfaE/MN/18pnj1BoWxosfjPAyTkdhlc4YleErncF8IYYOGsz1b2SexunrM+fVKEDkEeNIeHTkYeS8kVUQGX+DPG2e/gkj9u1GtVjBaHxciIYLyphxyiMn9bMTxnoQnSvIa04F0rj3Z7+xL746fBI783koUCiL5o5ijKiaP3m4ml5Mh7uqRk+eI1Ubd5Rnc74p2YtVVQH8YM33b57yZve+I+7zAzpPMVJeh/KNDjvUba++HFtWMUpOD3YA2inoAAXUBeapbuC6ocPS3zyMi3x9fZZgwG/Vjyk+158gAgJByX0Eg6KV8XcnxM0Yz3ycWNPT3KYOFH7/zZuhHwCSkDKUmBmP08MpH1Idq06h7jKuS5MQuOsSa19h7TCd3o5s3nmA5ChWTdK/Ulj6DZ+g1kYLllCVWwTS9UFQjz6cSlKmcH7np690NV9z+NOmwLnj7gQMlvAmWvJRKPz7379y3VCXGe/K8QWDi/Rp94LhVjnXh6pcEhFFM8vokXnaDTvGRlRigDnBO7Ws7zHVBhbzMU8HEk7huvC4lWC0ijHoiREHmKpGHz2sQweMeQoGNFz1qRN0COZ872pkxxsp7XqfkyBstDE26oMBAX1MjYNFcOdNGq7mrDQGlHW4e6XP0PHSrHN/+cEwF3hCYH0JY2M1FDBdTZ0qZV2/MW4efVw0YnHpYKelN2j4GjufMMLs/HeXVAeU8e4O5H2MnDczT+OEPP3dxR88RmEBgvlWx84OGtDGkW1PPvF4VF9oJQVWgWiKZTWiFtvowNqzdr6qp3p1aBavnkfnWN1W3aVFl7eCEvzBd3qb8gYj5PVOPrRHMCIPMOoft8XrMAJfL1snT5+V/UdiFz+zmuommHhBG3lOR3/eKtDWuv12psetAFN8Ths9QM58uFXeGdZH3c2Ihdp1aEsOnzBDTVuKejWasNxSYnKrnOtZLokoMbGMLlYpV8JRoL5Hu6Ye80UmFph6BAErHXr8PzXMEX/H7Xd4BKn0YJgvCBTidlxfzAGwc99eVPPg++JktCqmzfn74nFHj94dm2t/FRuENHK6T3/8+ad8e+kHbSsWGtd3J/c9X5wdjMj+cPtyy/02PzDixjxlBEbHY5S8DkFMzNv95amz2jOxvE3pg+833q6H4/rJ5195jFjG6+aR4/goc9AeC5M5CSJf/vmqfGdKy3RpA19YU2eeHga++vqsz+AyGm3G4K8TmBsfCz+tWzhFLn2+R47vXq0usArn85zOZeW6rTJ1zpKAAtw41phTUYcRSsb520MFv8083RTmh9aPbaDpDOiwMAfacVuxkdU0TsjTxz/3nOoG77GaQsU8jRPyMkbpnjgZN3836iaM6PUXpmUyzkGKi2mntwAHAmXMOCLJSf3shLEetJqr2I6vqams+PM32Bfsk87p1A6JiTH9m9WrJueO7VD1iL+P9s3qap8SH74Do+kHdGsje9ctkF/OHJF9Gxapaa/0czPW0xg1aVZQRtQbp7zBnVioMwBlT59PFu2RJx6Pq+uxf3onMzrTrrjO+6iPd+074h6haDXVDRjv6PA3D2P+XUx/BcG4oEU51Y+p1fy+dv7++x9X+vs3pUW0MP5uTK2yedl0y3zq67Ft5UzJ8GTc3ZmJNT2NbWp0NB/Ztszy9/p6zJ/ydlgW9UtIGUrMjMcJ1xq45nAKdSWuB4IJ9Q/awFhw872578i3x3fJqUObZeSAru62N67PegwcoxYdT6hQlVuMcNYHSJinMvTFV7qarzn8aQuh89efY+xUtOWjYLrzjjvlfu23gT/5DnfaGu8ATahb5VwfqnJJRBTNvA4BnjpqgHRq2UDbig/zNA/s0VbbSvyMQdSHUz0Ub87AfYeO+gy2YV5ofUE3OwgwGke0IjiHi3VfPdnoSDBPtxIszzyVPqB9QtB7596D2lYsjPiNdPAIo+PNc+JhRKKxE8VqWhAd0sK4UCfmpcdiw8ZAqtW0IGiMIy2NMHpfn4/SG6Tzjj0H4wVqnSy84wsCtpj72ujQ0eMeAT8rmLZov2khVDPs9y+/XldTs+gPbON5BGkxQhQXWBuXTJOZ4wZrfxVr595D8t0l3wvjWsF0TEaYjigScNsrbn8FjIA11xtO0hkXJRu37dG2YlndEm01jRMCeEcMdQ7yo7ED0sg8jRM6N1as2+rRMRQ7f6j/t8CaF+pEfjNeOAYb0sLYgfXhx58HZb2Dl55/VvufqM4LpxcamLrE2IHnFP7GvDCoN9gXY4dKxmes666EcHpBHaoLaWP6n3PVmXe5jjM6q/x9+DP1EOpHdEyNHdxDVswa7563fsl7G4NSrxinvEGAHYtC484T42LPCOLrQQ3AQIJcr72i/q/PZY+F3fQ7/vBeq6lu4BnDlHv+5GFAwEvvBMA8sakTeP55OGVK96J6+Fx8vlOYPzaQMhUNjL8bHXG4a8Iqn/p6oM7WgxMQ7vR0EvBDu/iGjzoszaMPq8XxAXcjYoo6q9/r64H0QOdkqBnLENrc3/9gfw53yjwAyAm0pTDSNRyMxwm/G9NDOnXa1XY1tl9DAWUB04ugE3br8hnueeuxiPzqTTvU/xMidOX2Ife6VBjMYL4j146vdDVfc+CaTK/DfcEUoOa2WzBEez5KiHvuSS5PGNqe/qyPgM72U2eCd067Vc71oSqXRETRzLa1O7xvJ/l0zxoZ3Ku91KlaVupVL69GMOD2Q8zTnFhd+O77eNOeGCtuzMWOAKnR/OXrZN/huPljzTASdtLMRY4aR+YRrYDFkDDq1hsETOcsWa1tBd/TT6aTnNk8b/33tU+40Jq1aKV7rkVAUNYcSIwEdBJglLwR5hpGsF3nbVoQwIWUedTyrEWrtP/F8jYtCO5mMAbv0QjGooZIL2+QzgtcecwI6RiMkfRoxGd92XOO/V37DsvCleu9zvOHC8ON2/fIDlMHjBmmacKI0zQvFXA/OvQZFu/OAVxYI3hsnGYKI0mxwFQgEAw3pr0/wc5A4fgh6G6EkfN60ACBY+M0FeArnWH/4Y9l5fqt2lYsjJI13tqtQ541TuOEBS93fnBI28LdHZ4LGRshoGee3mPxqg0eF0TI0xix7w+kOzoLjFCfGKFeHDR6ipr3Gw8svJvQ42UsZ+go1Ofq9uW9DdvVQnR4NGjbSy3krTPeQYPgKI6fE1989bVseX+ftuUcFi3/+qyzC3bsC/YJUI5eet7zHBUolCM9T6EuRyebL6G6kDamP6Z1Ofaps05p7DPmd0f64GG84wwdCsh7+jEfMnaa5UU1zhkoH0ULvK62kRZ25z+n8LnGKW8QaMdIs48/iw24W3Wsmc8/6DBHB/Nhrf6xm+oGwRH93OdPHsbF76Yde7Ut3GWWRS0enhDmznJ8vpO7RxD43bh9t0fbIjEx/+5tu/fLr7/5Llc476JjSM/HCAAZB4ckJD13fnDQZ3riHGY871y+ctVnAAoLl3/po5wYFy7HuWCza7+dzDGM9yDIp6dHKAKKVoxlCGUP7SAcG1/wXr2eweKG5k4+4zzQCGI56QQ5cfK0x11qoWReYB7nSifnabSN1m/brW0lDNqdehpi0WBv6zJhsFPFUkW1reC0AUNVbs3pinLrZF/xnm27D2hb3hnXK0JbyNwe8+bAkWPu6dOCKRryUaiYB8u8/8Fhj0VN7eD6E3VEsNwq5/pQlUsiomjmc0gKFj7s1qaxzBo/RGaMHaRGMAQjcBgpmBbjnRkL4jX2jRfJGBFbplhBbSsW5p/u1HeEGrFhbqwjgN5j0BhZt+V97Rl7GFlXpEBubSsWTtyd+71tGRTAd7ftOUQ1XkMFPcyliubXtmJhnzA3Lzo0zL8Z8/X2HDxGZi5cqT0TCwuIPvN0dNxOhlHy5ulJjLxNC6IzL9RphOe9TQuCRVmLvZFH24o1dc5SlV7meY6RrkhfpLN55AHyIPJiMGAxaGMjB4aPny4Tps+P13mAoNa8ZWuk95Bx2jPeIfBrHqW/dvNOVRYwZ7YO/8fvNJY7LA52772BLaKE7zQ2lNH4wuiKUEGwYNmaTepiw+jlTBnlgfvv17awRsErUiR/bJBPh3Qe8c6MeOmMNEGZbttzsEcDFUG7UkULWo6SNU/jZGZeyNjMPI2TGdIUFyH+QLoj/XX4jKe1US+Ai7O+wyaoQCnqMjw69xshk2ctTlCD2VjOEPhBJynqYjt4fe7S1aqRjwcCUsZOCYzSK5A7u7YlMmnWIp+BIRzXGQtWqN/lLycdeID3YV90waxnH06Zwr0QM/Lh6o3bbQNo6JBGp3UoGNMfxxT1pq+1K7Cv785fJoUrNpBiVRpL76HjPBZ2w0J0mKpKP+YfHPxIjWgLp0wZn5HCWr2AQDv247g2xV7uHK/Gu/sKsNCt3gmFcwOCNXo94W2qG3ji8TRSrGDc+cdJHkYaolxgjQ1ARx/WFUnoHXH4e+yL/jvw+fgeu/wFGBARqjwWDubfvWbTTlXXm9tRZmj/Ne/SX+VjPBAAMs7BHmh6okNo+vwV2pZ35vM5Oh7tOqrwe7bs/CDe3ZRmaGcVKxjX5l20aqNqS/uC91Rr0smdHr7ycbCYy9CU2Ut8dtihDsfAFr2eueOOOyRtmrhp+QCLOz6bIa6sL1q53jbYhPbiwpUbtK3QMx8n3E2EgQB2+Ravob1nbhsFCtNlYgAH0hD5+6Qfo84TKlTlNpB0he2uNMB7fTGeX3COGDdtns/zJtpCuH5zMrDMX9GQj0IJ15V6cBzn5okzF/psw2FanGlzl2lbwXGrnOtDVS6JiKJZ6O8bDZPz332vgj4jJ83y+ug/4h0pWrlRvMAygmrG2+whT85sUqb4G9pWLJyM3yhfX42QwYJF+MzW3QfJq29UVHPYOoUTDkaAYCSoERqleUrVUgvX6fuMz89ZvLpq4IRameKF1OKfRrhIKl61iVRt3NH9mxFUzF2ypgqgGOFkX7tKGRUUiQbm0RxGONl7mxZEZ16o0yg2EGk9LQgC61jTwTyiGemFdEP6IR2RnkhXpK/5grVy2eJSMG9ObSvhEASqWr6EthULjXN0LuUvW0eVDewTRpliIa6mHfs5nucdHQDGEfL4XIxQrtywvTvP1G7ZTRp36KO9IxamrEGAMBApUzyoAuQ6dCjpI1P9tWz1ZrWP3h6xx6mDNO88wOOCBh01hfLn8gimI181dB17Y+cO/uatkZMkV4nqqqMGn4l0LlWjuVSo1ybebZ6Y0z/LS5532ujMI0rMfC1kjP0zTuNkhNHU3sqLHaS7cXQQylXax2KDvoCR4lt3xR9ljs6cQKc7AnM5Q6O9RZcBcvZc7DybZrgo7Tl4rHof4O+qlCvuUV/h/6jD9M+06zwFlBGrzkp/oF4YM2WO15Fm+G7sg57G2Ddv9SyOvXGufgSj0elmZF5wFsFe49oyk1zn0fnL11heXCEvDx4zxXGHtL/M6Y/vQR3lrfMF+4h9RUeYDlMg4OLVyHjXBUbFLnCdr61+H+7G2L4nLuCYOgjTjQFGqel32SCIMsJVp7gD7q+9osq1GepsBPBh1fpt0m/4BPV/1C3onPJWzvF89Qol3fWEXWc7IH9Mm7fUIw1rVSrlXsg2oVCXoU7Teeu01GHkdK8hYwPq9Iomxt+NctNn2AR1HI2d10b4vcjrejnH8StVtEC84IlVemLgibf644uvTqv6w2l6Gs/nCPigg9Xqb5GXsEYCfpfxvOgN2jNo1wA+z9ugF505H6A9jnZ5OKAM1a5S1iMQ16RjX1U/WO0vfv+wce+6zwMoo1XLlYjXYY5yjvKuw92Tb78z0zL98NxQVzvBSdsfwfyWXd+SVBlzq85K41Ra/jIeJ+xDh97D1KAN83kEkBboyHGaB5ww3tWLz0Tnu1XAGc+9tzEuoBusdXBCVW7N6Wr3uUhXBBQ7usqIk3RFPqtXrZy73enrvIl9DvXAr0jno1DCYILqFeLqYLThUP697Tvu1uzUL7a+C6Zb6VwfqnJJRBSt7hjgov0/UcHtn6s3xjVesejmngMfqhE93h64Hd58+xUaNUN6d4gXsEUACBf1u/YfkV9+/U17NhbmE3z/g0PqM3GSsbuN7w1XQ8U4MlP34AP3q9EG2/ccUIum6PB/3J6l73Ognx8I/GZMl/HR8c/lu0txU0BgnxAo0n8zGgDmNEE6jn6rm+rttjoJIiCCQB1+jw5pXrxQXrnLtNhmsOBz0TiyaogiYFK/RkWPgJZZ8uTJ1DQWVhc8FUoWkTIl3nA1kqznt8PdJhgpbj6+SDekH9IR6Yl0Nb4OuDAcM7C7x7yHOnO+x8gsNIR9TVGC0QOYIgZTVJiDwigTKBvYJ9wSaHcLNuZArFi6qEe6pXBdHGHedXyGkbGc4JZts0a1K7nyS96ARjbc6Ur3P2/ckOVrt2jPYK7Iu9UdKnYXapgzf6WrYWe86DviuvDGPnp74DdYzSHaulEtqV+9vNoXI9x9hOmz8LdG+O4DRz5WzyOdrebpRIAR04s96OXWU5QtNLQXrlivPRMHjVAstmmXFxAExYiebbv2a8/EeSNfTqlTpZxfiz6j8T1h+gLBoso6rEGQI2tscANOf3NexkyZq23FwUU1OgX9nV7HKM2jj8g///7rTmscJ4xORAAU85nfcfvtgjlaMaKnXa+hahS1rlvbxlKlTAlX/vMMdiJobfzMM+cuyNL3Nnl8Jr4H+ahT3+Hu+gXnC32aJ291m7ke1P8G5y7sW+y+xKg5nnEb+vh350vHPsPdi40CFmyvUKqoZblBfYTjq+8T/u6ff/5VI8l/vHxFjUDCSHkEu/R9w+eg/sZIbdRPqI8wCgq3zKMsIc999/0P6m+7DBgZL0BvVwdhOh/kdXB6rsLn/ffff7LDVXcC1mJBWl9xneNx2Yl9wnQxWC+jz7DxKm/pdSi+o3+XVur8aoROvW8ufOdOdxxb1IX33XuvKr+fuc65CCB0df0+PbCBzvumdau6Ax8JgTTEsdm0Y4863no+wWd3bdNI0ps6FSB5srvUehHovDcq5PqNjWtXsT134VggDfTzz4WL38uyNZvVv4Bjj8XhUQ906f+2uptDT0PUQW91b2vZcWCsP52ee5C+6NA+8vGnql2B70GewBossR0lMa5//3W1O76QiTMWqkUgcWywHgvKg75fDWpWjNf5EghzGcSACWOHr5Vg/G6ULUxrhumOkL8RRMG/2EZ9joCcccouLGyMO2bM7Smr9NzuKrt4WKUnju/JM984Ts8HXPkGU4xg7QRAXbfaVfYRGMHfIyhy6OinarQuOo5+MrWpvZVztHfTPpZadn5wWKUF0hJpilHTqFMB5eJDV/sIgVk9HwDKyYh+nVX71CyQOsbYjvJWV2NqGrSJ9TKEtEYZ+ur0WXUe0MsQ2iDtew/1CBij87hlw5rx2iI4lqjz0QbUy+I+Vz2G9XnwXlXmXXUtymX73sMsA/Tm44b2ADr9J89erPYT7bfPvzrlSoccqt7zl/k44foDdf5HWgcFHvr5afiE6TJ03LsqDxjzV0LOCfh+lAv9+KC9ir8xpw/ORfr0Hsgf3do18ZijG6Kp3OJ3IY30a0qrzzWmK+48NKcreCu3uObAOR7r84B+3tTLLQK6x13PzV++Vu0z7uhCuqEDWQ+ieis/TsqLWajzkZN9CuT447oCbaTT2vplVumNY4s1BozXUyjHaHf95qrDMA2oMb1RlrF2lNP09gd+z61wrg9VuQzkdxIRhYP3IZe3AJwwe3VoLiWLFNCe8YTbzQf3bKfe50StymVUBe8EThRVypZQ3+/k8/EeTDVkHPEYChhBNeqtbu6eeSewb4Nc6YTfYz4BRhoab/oISiNf04IALgrsRtKbL8CM9OOLdHGafwDpjvQ3jkwPFnQaDOvTMd4dHN6gs6BD83ralncISLduXNu9iJcTeG+DGhVVQzZQOIb6SDdAI9TYCRRKKOdYWNtqNDPSo5XrAt3fdTvQwYVABI6THW/TOFktZGzF2zROyHtWjXU7mKPfeHsyjkehvLm0rVj4LuM8+joEQPzpELCCtG7XtI4MdeVr/TfFTmm2UN6s1lTNf487RXAng3G0Do4NjpFxLRKdfvzaNqmtPRP/M3HHE6bswYUitGhQXXp3bK7+7w/8jf496OTC3SY5ilWTjK+XVHdZvDtvmcfoLOw3vsuu3BhHwuJvEcB5/c0a6nNxFw8WIDVDfdapZX2PfIGFsms06yxZClZw/y0uevCezq0aBHTXhRP4bW2a1FJ1pw7Hbtj46Sr9n36tmNof7Bv2UYe8h3V0MFexGQIGSGdjfYE1SsrWbqnSGqNPB46a7L57CB1IbVzvd1KenMI0F+Z1X7BgbMZnnta2POEcgv01l9XcrvobU+bZ0c8/o97q6v575AXkJ+Qr/GakIfKbsXPVaR3kL5R/c7sCeQl5CmUJZQrHFmUMxyB9urQyfkgvVz0XN+o4McLvnjbqLY98hzzbsF1vlf7Iy7gN31w/9e3cUrUnvbWnQpmeOKe1blRTCubJoT0TW/6wjygnyDsoN3rdhHM51qtyAuet6WMGuu+Uwd+jHOKOQtQz+GzcYabvNyD/Ih+bzyuhhrSvVv5NmTSir3uefuwvRr8by5DxPAAYgd+3cyt1frOCUbitXOlrLNc4diiLel2L/+M5fC+mG7Wra9Gx8fmXnnPWY/So0/U8rOA4jRrQxX2cYKurfaXnW/x2pAHSAmmC/I10QhAtGMq9WVid53Te0kfXskEN951KwRCqcotryqG9O7jzExg/15yuaKs7TVfkt94dW6i2qc5YbrHP+Ne4zz3aN5Umdaqo/4dCpPNRKFldT6Ee6DV4rGV645ijHvPnOsmJW+lcH6pySUQUjW7ZID3mZsbFQscW9SwDNYAKHbeSLZ81zuOEY4YTI4InOOEZG1++4HsReEejxNiIMcO+Tn67n3Rp3ch29Fyw4Hbc1fPeUYsF+5L91cyyYtZ41Uj2lo6RhHS1Gn3la1oQHS6O0DAwwraTABXSA+mC9EE6+YL0Rrobb4cONuSlBVNG+Dy2aMDNHDc43oKz3qChh3yMwLsveM/UUQMS3DjESAvspw4jnTH3pd2dJwmll/V3R79lu/+4YOrXpbXMnzxCpbkdfGZXV9me7zou5rxmxds0Tt4WMjbDBZBV5xMavv40YjEaCKPWcUGgw/Ewj4DB77e6MMGF+ONpEr6+CYJKnVrUV/W5r7RG/Tx97EDp3amF1yAK4DUE/nHxYFen6/lhSK8OjtLeDH8z2PW3WJDd7nuc7jdkyphB+rje58+5CMe9Wd1qMrR3R5/7MW5wT6lbtbzcflvomg84pigTS94d7fOYAuozvNdbpyqg4wJ1mrHOsILvmzZ6gLo1OpiMU97ocF6wX0PiaRXI1yG/YBFaJ+UU5x/UtU7OP/hcf+qgQOC8tnzm2HhT6plhX+dPHh7U6d4i6aUXnlXHwMm5EXkP54ye7ZupjiU7TtMTQSS8D3dKOYV205SR/aW0aW0mM3z32EHdHQfVkG8LufZjw6IpPj8b9PYl0i4S7UuM7sQ1wHuudpmTMtS/a2uZOLyPpE5lP40fpsKZNX6wbd2G71vsqtPQAWJX196dPLk8ZtGZePJ04HO54zjh2C6dPtojCGkFxxH1KuZFDxac43BORQeFHaR5rw7NpKfrYTVgIiFCUW6Rrjj2OK6+8hPOaWir+5OuaJOibYo2CdLGG5zH0bbBAAfcFRIqkc5HoYbj7uR6Si/L1cqXVHVKsN1K5/pQnU+JiKLNLRWkR8MEDYF5k4bL/o2LpFKZYj5PmGhkFM6XS3avnaf+Dn+vBzEQFEMDcd+GhdotY5632DuBkyvmWcP+jBvS033rG06kWMwFwXl8Ny4UrC5SMK93KODWSSwUfGznKvUbEdTW4cSHC4eNS6bJ+6vnqIsupFM0QgAEI66N0OFid3FkZLVQJ7bxvBNIF6TPjlWzZcPiqSrdjN+NdEX6Ht+9WjVQw7Eos35s96ybL20a13bvD/I1RuEsmzFGVswer6Zt8QcW/Jo6qn+8zwX9dyI/4T2+LmKdQCcLyrDxrgME6T84FDelSbBg/3G75NEdK1VZt7sA0qG8otyibC+Y8rZUcV2c6Z1x+HvUKwgEf/z+KjXlll6v+ILGJuoeI3yet4WMzXArp/nvMeIJI/T9gSlDjKPo8dswb7C58wsXzwgaYzQL3oPH6IHd1WhCJx1lTqAeR144tGWprJ430aOcGevS43vWSP3qFRxd0OM9LRvWkGOu44MOWBwv/bib634n+cEbHE901h7ZtkzlB+Ptz/g/zgv+7LceCNi6YoaakknPc0gP3BmDKVys7mBAfsVURdgP5HW9ztfzqr4fdauVszwXBZvdMQVjnYL6zEndiToNdRvqONR1epnT2wYIzqO84uIy2Oc0fB7ucjCOzs+bK5ttGcCIeYyc19mNvLdid/7RjyvyHOo1TLPltA4KFOabXjxtlOxaM1ca1oxbTwL/on5EPbltxUxVHwU5+SMK50bkrc/2rlXpbaxLjL8dec9bW8+Kr/Rcu2CybFr6ruPOdiMEcJZMHy3rFk7xOHch/2A6F3wnvlt/3h8ohytd5fDg5iVe25fIr8i3kW5f4ruRH9HWRd5E28Z4/kQdrZch/BZfnaiAz0Qdg8/DuUWv8/V6CNcaeA2/3Rd8H84fyFNGGVx5IyGwj7gTFfkH+chYX+rHCNcAyCP+thWdQPkY2KOtqv+9tSc/3LZcdYw4SfNAhKLc6nUyjq+3a0p/zmlm2D+0SZA25rKl51Wc49G2CXbHhpVI56NQM15PoV7U8ynyB9pKxnosFAF6nZ6vboVzfajOp0RE0eS2GEzkRYkCbt+q26qHHPjwY+0ZkbkTh0nNSqW1LaJbC+Y5HDhqklrcVYd5Hue8M9Ry2gsKDqz1gMWB9TlhoWOL+uqiOhwXfokV7vLQ5wUF1t9EFA6Ye7d+6x5quglA8MTX6FaKPMx5Xat5V7V4Le6gQ+DLWzDz0xMnpXqTTuout/fmvmN7ZxElDvoxxdzdGGC0aNpIjw4LSjxw1ynWRlq4Yp3aZvuPiIi8CV23LlnSg1ula7ZwP0ZNni3//We9QrnR199cUItsGaV+OKX2P6JbD0ZI1KtewWOOQgSO335nZkinvbmVIQCAhb6MAXpcPDauXZkBeiIiIpMvT51Riyjiceij42rBVyeuXvtVLZQN/7vvPq/TNmDAwuadH6hgbrE38iTKUclJEa7t0MGiH3usmxS7iKYzp13HU19AGYsZ6yOGKTKwmLt+LDHH+y+/Xtde8e3Kz9fk1JnYhWaB1+9EROQNg/RhhgYWbkvbtnu/+7F+yy75VluJ3RsExpav2ewx/zMCk88+FZo55YgSi2eeekIt9Gk0f/laeW/DNrl503fnFzmHi8vJsxfL1DlLtWdiYdHR5/ycLoeIiOhWcOzTE2pRQzxadx8k5y7ELWxoBwFevd2fwdXWSWExTcWvv12XCdPny/Dx09V0D1gLKVRTwJB/MJ3Z+x8cdh/7dr2G+Lze0+G6b71hcfSXM2WUlD4WDafQ+uGnK1KvdQ91LLEw7PsfHNRe8e39fYfUos7A63ciIrLDIH2YIUhvng96/5FjMmryLI8AvJEeGJu5cKX2TKycWbNImsfiLxhFdCtBpxfmd23RoLr2TOxtpW17DpFlazYLZ/QKDqTjinVbVCDACOkeijm8iYiIkgKMbNfXo0Dgfd/ho+r/dnDn7fK1W7Qt68Xhcf3w8PN5pMegMWoBXyxkiTnAKXpgWin92CNIu9zVjnIymt687k+B3K/xbsUIw1zrObPFzfM/f/k6Fbj3BZ1yc5es0bZ4/U5ERPYYpI+ACiWLxJsLdNrcZap3fu/Bj9zTdKAR99mJU9Kscz/pPWScek6HBl/F0kXZYCNywaixHu2aqvnodQjUY8Sa8SKHArduy/sqPY2diWVLFJLeHVtw1B4REZEXCNJjGhodBt18/uXX2lZ85mnlMKVcySL51f+N7k6eXMq9WVgtjL5l+XR5/bVXtFcoWrz4/DMexx4DHZat2eT1Tk8MiMB0Kj0Hj3W3tzAQIm+u19T/KXJSPPiAlCoaVw7RLh48ZorXQXaAzrYeg8eoDjWIXdy0OK/fiYjIKwbpIyB9ujRq9Kl5RMyGbbulSKWG8mCGHJIsbRa5N31WyVaksixYHrvIjFGbJrXVquhEFAsLxY4d1MNjfvpq5d+UN/LFBe4pcEUK5JbWjeKmFUI6D+nVXh5NnUp7hoiIiMzQkV2vWnkVoAOMqK7etJNMmrVILv3wkztgi0E6GKxTp2U3j2nl6teoYDmlHBaHXTFrnDSpU0VSpXhIe5aiCY59o1qV3Mdev9OzU98RamHYv//5Rz2PgVlnvrkgQ8ZOU4vFYtFgwN+1dV3zpU6VQm1TZJUpXkh1mugwyK5ms85qTQh9jnp0tGCx7iXvbZQqjTrIynVb1fPQvH41NZKeiIjIm9tcJxLOBREBaIyNnTY33gh5JxDgH9anE0evElnABW7Lrm9Jjqwvy9v9u/DCJogQQBg1aZYsXb1JpozsL/lf58gufyD9uvR/W6bPX662504cJjUrlVb/JyIKFQSM6rfuIVt37Vfbu9bMjXdHJ4UW2v3vzl8mfYdNsB15a8Y2f+KHS21MXdSu5xD5+dov2rO+YTDXpBF9pXqFkpxSMIqgA6VZp/7u0fFOsSwTEZETHEkfIXfeeYe0a1pHxg/pJSktFoKygsZaj3ZNZEivDjzBE3mBwPHymWMZoA+Be+5OLl1aN5Ity6YzQE9EROQQ2v3N6lZTQVd9VLUdtPl7dWjGoF4SgAB71XIlZPG7oyX7q5m1Z+1hiqPls8YxQB+FMj7zlFr/oU7Vstoz9lCWR/Tr7Hp0YVkmIiKfOJI+CmBBmcFjpsqqDdssR9fg5F6+ZGHp0LyeWt2fjTUiIiIiosQHU9zMX75W5i1d457WRIeBO1jvpW3T2mzzJ0FYbwBzmc9Z/J7s/OCQ9mwcTCVYt2p5qVmplDzwv/u1ZykaYZqqw8c+lZkLVqpjar5LAp1xFUsVkVYNa8pT6R9nWSYiIkcYpI8imArh5Omz8t2lH+XLU2fkmaeekMfTPCrPZXhKHnyADTUiIiIioqQAl2C//va7/Pvfv2r79ttvl//dd58adU9JH677/vjzT21L1GKi9993r7ZFiQmms/rt99/d60ugLD/4v/vVv0RERP5gkJ6IiIiIiIiIiIiIKELYvUtEREREREREREREFCEM0hMRERERERERERERRQiD9EREREREREREREREEcIgPRERERERERERERFRhDBIT0REREREREREREQUIQzSExERERERERERERFFCIP0REREREREREREREQRwiA9EREREREREREREVGEMEhPRERERERERERERBQhDNITEREREREREREREUUIg/RERERERERERERERBHCID0RERERERERERERUYQwSE9EREREREREREREFCEM0hMRERERERERERERRQiD9EREREREREREREREEcIgPRERERERERERERFRhDBIT0RERET/b+88wKQonjZeivHTvwEjqCiKmBGUIEiQJJKRnKPknHPmyDkHyZKTBImC5CiiKKKYc0LMEeS+ffum52bnZndm93Zv9+D9Pc88t7M7N9PTXV3dXV1dTQghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhBBCSIygkZ4QQgghhBBCCCGEEEIIiRE00hNCCCGEEEIIIYQQQgghMYJGekIIIYQQQgghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhBBCSIygkZ4QQgghhBBCCCGEEEIIiRE00hNCCCGEEEIIIYQQQgghMYJGekIIIYQQQgghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmLEJYk+jM+EEEJIqvnn33/lrXfel19//10ezn6fZL79VuMXQkh6h/WbEEIIIYQQQiJPujLS//jTz9KgdQ/ZunO/8U3qyfHwA7J4xijJft896nzJ6lekQZue6nNqaFqvmowe2E2uvupK45vwOPXRp1K7eVc5/u77xjciO9fOlwJ5chlnySSMmyEDR00xztzJ92QOeSBbVqlctqTkz5NTbrz+OuMXb0SyPAK9UyT56+9/pEv/kTJr4Qp1Pn/yMKlVuaz6HO+k57STi4s//vxLeg4ZK9PnLVPnWe7MLHMmDJHC+XOrc0JI2mFvp1Pb1rJ+E0IIIYQQQkh0YLibi5hDR4/LgmVrpVL9NpItTynp0HuYfPbF18avhBASOu+c/EAWrdxgnIl8/uXXsnL9Fvn7n3+Mbwgh6RXWb0IIIYQQQgiJDjTSE8Vvv/8hU+cukXylasj8ZS+r5eyEEBIJrrziCrn0UjY3hFyIsH7HF7//8afMXLBcRk2Zow44ZBBCCCGEEELiH46qiB9nfv5FmnbsJz0GjVWGe0IICYVHH7pf6lQtZ5wlhcMoX6qoXHH55cY3hJD0Cut3/AMni3Wbd0jvhPHq+PizL4xfCCGEEEIIIfFMuopJD++gxas2yC+//W5848/Zs+dk62v7ZP+RY8Y3omKvPlv0abn88suMb/yBB1j1is/J7bferM7tMekfvP9eqVahlFwVYmz5LHdkkufLlkj1wDU1Mend3v2Lr76RHXsOqWc40aJhDRnWp5Nc839XG9/44xSTvnaVcvLIg9mMM+88X6aEZMuaxTiLDoxJT0jawI0lCYkPIh2THrB+xzf2Mmd/gRBCCCGEkPRBujLSu2E3ZIJQN3C1G+mffaaAzJ8yXG668Qbjm7QlNUZ6L+9+7tx/cvDoWzJm6lx5Zdsu49tkEnp3kI7NG8hll2UwvknGyUgfz4NBGukJIYRcTETDSE/iGxrpCSGEEEIISZ8w3M1FDozvBfM9IYtnjJYB3drI/669xvglickvLpI9B48aZ4QQQgghhBBCCCGEEEIiCY30RAFv+25tmkiP9k2Nb5L45rsfZOGKtfJrgBBDhBBCCCEk7cFi2PPnzxtnhBBCCCGEkPQMjfTEBF71rRrVUrHorazdtENef+uEcXZhgvj8o6fOlRJVGssVmXOoA5+nzl0i358+Y1wVGpjgGDd9vjxXvanclD2/umeh8nVl6PiZ8tkXX6vBNUCIJf3M1t0Hq9A2kQChjHbtPyIvdOwrjxaqoO6fLe9zUrtFV9m8Y6/5HCyNL1e7hZkG654OVpzSCePAsbdPSqe+I8xn3P5wIXm+QVtZumaj/PHnX8Z/ewfxjpFu3DPvs9XNZyLvBoycLO998LGZd15BOk+896H6f9xH3xP3x3MQ8ikUQweej3TgfjqNKOPydVrK3CWrw3rvQHgtHysoG5SR/h+UnRMIkaWvwWeg5aZJhz5KXvCblpstr+1T5eNGODKNayCXtZp3UTKE/4VMtegyQL0z0gUQAix3iWrqd/x12lMjmnlmBxOYy9duVvmj80vXgRdfWqnS4oVolYUd6DOkC+nT+azvifcIVXbxfrgfZF/rOdQJ1A3oOa9E6j7hEEldHUg+9TOsbYx+BtqfUIFsQK6bdx7gJxuQFciMri/RwEs+BKqD9nxw05u498sbX/XTC5CLbgNHe2oLAtVr/B/KFvlvbRNS2+7jedt27fdrd/GOkC30MazyFAwnOUIbtfvA6376IE/J6vLhJ5+r/NXvkOmRwn5hCBHCUf+mD61jnED6PvnsS7/2Ur+DPW+c9JYdJ3mJRt8B6UY+IZ+tdRn3R3m8uvtAyDoz0n0HQgghhBBCgkEjPfEDm8Qilj02zNX89vsfsunV3fLv2bPGN9EHzxw8ZpoaiOKY9OIiR0NAaoEhY8b85Wqg22vIODUA1uBzh97DJEfhijJ/2cueB3e4btrcpZKraGXpPmiM7Nh7SL0POHT0uBrsPVG8igybMCsq7wTe//ATqdakg5Ss2kQWLFtrGoo+//JrWblui1So20pK12imBsnhcuann6VDn2GSr1QNmTx7kfmMMz//ovY3qN+6h5St1cLzMzDAPvD6m1K0UkOVbtzzzXfeM35NyjsYVJ4uW0f6DJ2gnuOFL7/+Vg3QcxWrrP4f99Hg/nhO4fL11DVejIEwHmDvB6QD99NpRBnDcAqjGdJvfU56AfIBYxjSv3D5OnWuv4fcwJhWt2V38/tIgHI/+tYJJY+Qy1Xrt5plC5mas3i1PFOxgTRq10uVZbwA3bHs5U1SoExtX550U/mj80XXgVbdBkmRCvWV0SlU41CkywK6ZsLMhUqfIV1In85nfU+8B563fc9BV8MTym3tpu3q/XA/yL7Wc6gTqBvQDdCvwQzGkbpPOKSFrkaaUX76GdY2Rj8D7Q+u8fp+MKBCNlAvYNy2ygbugzKE/kc7EC8EyodgehNtB/RC9Rc6+ekFyMX4GQuUDg6nbFDuMDajbJH/1meG2+5Djl/be1iKV26k2j1ru4t3hGyhj4FnhtJ+aX7/40+VVkwiWPVBpEFa+4+YJLlLVvNrL/U76LwJRV7tRLrvAL79/rSSIRjkkc/Wuoz7ozzK1GwuNZt2lrdOJPcrghHpvgMhhBBCCCFuuBrp333/IzUIgidOo3a9Zey0eb7BQeheXyT98HD2bFLxuWLGWRIYkPx4xptHaGrBgLvvsInKSI+BKI7O/UbI1DlL5L//IuexhAHmuBnzpW3PIUEHzPitacd+kjB2uqt3F37HALF976FB74nBIwbc3QeNNgeSkQJ7CFRp1N5xI2Ar8Lyr1qSj7Nx72PjGO3i33kMnyPR5y4xvnMEzkL/weAwGDIIweD5fv628/uY7xrfOIL9GTZkj9Vp2Vx6EwYAHXM1mneWlFeuNbwKDa5Bv8IwLBAw2g0ZPVQP2YOWGd+jcf4Tre8cTn3z+pXTsO0IZTIOB33Hddz/8aHwTPtpAW6NpZ9PTNhCQj/a9hylP3FgDORgzba7Ua9XdNDAFAr/D6ORFf2giXRaQ1W4DR0nXAaOC6iUA2a3u0wvT5y8LaoTbsHWnNG7fJ+j741m9EsbJyvVbVFk7Ean7hEpa6OrziedVuDgYJIM9A7/hGi/vB/0Eve0mG9D/8Lg+eSo+dBAm29zywao3YSzHewbTC7psYCz1ajDGdRNnvSQd+wwPWp5Ip9d2H/ect3SNVG3cPqLtl+bPv/6S4RNfVEc0gR5p1rm/eo5b3kC20H86G6LzBv43kn0HgHEKjO/zfGXgBupFlUYd1OqMYHUt0n0HQgghhBBCvBDUSI8Bfc6izyuvGixXXbRyvfQYPFay5S0VdKksSd9kyHCpFHk6j3GWBAwoaWUc+/CTz2Trzn3GWTLrNu+Qr76JnCctDCLDJ8xSn7F57uzxQ+TItuVy6uAmeXnBZGnZqKZkvOF69TvAwBUDy0ATBRiowzsPXv8a/H/bF+rI5uWz5MPDm2X/xsUyZlB3efSh+9XvuF8k6xIGtD2HjPUzeDm9W91q5dUmwZgAgZHqwOuhDS7h1bh41Qb1uUal0uqeJ/dvUO83akBXyX7fPeo3cPiNt2Xk5NkB9zXAQBlhNrAMXhtwkDakEfdFmpH2ScP6SO6cj6rfAUIK9BwyTn748SfjG39gAGnepb96vqZK+WdlxexxcnzXy2ZeNKtf3dww+fi770uX/iMDGgb2HT6mQnJokB7kLe53cPNS6dellSkzbu8db8DTcP2W11ReIOwVZBZ5tGPNPL/3ArhuSgiGsUBg5USXAaP9vEKffaaAzJ2Y4CiveC7apR9+DC8URSRA/Z/84mI1kaiBvCf07qDy6rNj21U9sMsr9Af0g5c8i2RZ6ElPeKJrkC6kD+lEenFfpF/XWxjo8D+BjMZoCzBpqg15SA/StWf9S0oPoE7od8c1mNR6570P1LmVSN0nVNJKV79z8gMZM3WeSrvWFZBr6IulM8coWdd4eT/oNOgn6CmNtSyRbqQf74H3gQ6C5/N7HwafSEoL0F5Y8wF5gLxwai9gxB3hqy/QC7oObFs5W9WB9YummfpAgw3uoUu8MHPhcrPdt5aJU3sA3Np91A/Uky79R5lyjP/HfdzaL6wogGHcDZQ7jMrAXt4ThvaSmzLeoL7/5sRudaDuFCuYT10Ppo3qb/6mj/bN6hm/JoGJiIRx01XbrslyZ2bp37W10g+fHN2m/uIc3wPko7U99EIk+w4AbXWzzv38JnPsfR60+Wj7NZArtDuBZCYafQdCCCGEEEI84RtgOPJCh76Jl2d6LOjRa8g44+r44M+//k5s1W2QXxpxju+94hs8+P1/OAfuESne//CTxCeLV/W7/77Dbxi/+jNk7HS/60J9dyu+gUZirmKV/e63+8Drxq9JnD7zU2LZWs39rgn1wP/jPlbwfk7XIh+QH+HiJB84Bo2eGjCfkA9laia/4315SiW+/uY7xq/+4Hv8rq/F/+H/nfjxp58T2/YcYl6rj0BlZk+7k4z9/c8/id0GjjavyXj/U4mTZy9S39s5f/584uYdexMfKVjevF4fgeTLXjdue6hg4pLVryT+999/xhXJfPPdD4kN2/Yyr0Vatu85aPzqzwcff5b4dNna5rVFKtRXeYk02vn9jz9VeelrcUyc9VKKa+15gXJ5Zdsux7SCo2+d8EtDoHIYPnGWeU3Jqk0SfYN945dkdu0/YspBsPf2ir2eBSofK17kBdh1BuRh577DjnlvrwuPFa6YeOK9D41f/bHKSqC8PPPzL4k1m3U2r4M8zVu6JqC8btq+x69+4QikE6KZZ/Z63qLLgMTvT58xfvUH97TKa5acxVLoUU20ymLd5h1KDvW1SA/qkRPQS3gffW2g/D34+lvmPfEXZWNP57ffn06s3LCdea8BIycnnjvnX/8idZ9QiaaudmqzO/Qelvjrb78bVyQDWYeOtpZPoPdz0u9jp81zTAOAPECX6uv14aUuuOGlfjv1D9r0GOyYD5998VVixXqt/a7FgXxEeu0yAT2+aOV6v3zr0n9k4j///mtckYy9XusjUD3AsyAf1ryDrOA7J06e+sivvN3aL9Rza7r7DpuQePbsOeOKZJzkqHfCeMf8s2PP+0C6zIpdT6D9RjvuBMqrauMO5rX6wLs5YZUXHJHqO6Bs2/ca6nffQG0InoU+gLXeo/1BO2QlWn0HQgghhBBCvODoSQ+vVsTidANLdhFag1x4XHLJJXLpJf7ikVbxoOGZlC1rFuMsGcTLv+rKK4yzyIBNcju1bChXX3Wl8Y0/D2TLKmMGdZO8TzymzuGBtWjlhhRxanGO77VHMK7H/+H/nYCX47A+naRJnSrGN6nn+IlTKva1pkf7ptK8fg258oqUeYbyLVX0aeWRl+m2W4xvQ6N3pxbKE+7SS1OqkdtvvVkSerWXZ57Oq87hYXjw9bdSeOXCOxErdLTHGvJt+uj+8uTjj6g02oEMoLysmxtv3LZLxaO1Ys0LyNO0Uf2kTInCjmkFT+R4WHkb6r0Y8L92Lzuk9Y8//jTORAo99aTcmfl24yyZfE/mkAqliqrPeO+33z2lPsc7yKdhfTpKkQJ5HPMesjy8Xyczj9774GPPsX2d2LXvsOm1iWePT+gp9atXDCqvQ3q2Mz0XY4G9nsOjd0S/znLLTTeqczvQKz3bN5Me7V5Q5/AcR9vqFj4jUmXx0y+/yoLl60wPX6QD6UE9cgJ6Ce+D9wLwDsUqOrsX8R9//WXeM2+uxxzr62233CT1jPuAk740Iq62lUjdJxTSWlfDA7df11aOcgtZh45u3biW8U1SyBeUmx0n/d72hboB266HH7hPvZf2eo410L+9O7ZwzAekcbCvblv3wsF1/X35ViBPrhQyAT1evWJp6dyqkfFNaCH5dLvvVA/wLMgh2iG3dh/1AmG49MoGL+1X97YvqLLTLF690ZMehRx1bt0oKvrPrifK+9qv4X07qXbcCZTXpGG9VZmGQyT6DgBt9MIV69Rn5EuwNgTPKl28kF8bsvW1fSni3kej70AIIYQQQohXHHudcxevNj65M89DDMiLHSydxYSGl2Pjq7uN/7p4wXJnDBLtVHiumNyRKaVRNFwwqMImuYEMVhpc17pxbeNM5NXdB+SjT74wzpLApmF6IzyA660GByfw3FaNa7le5wUMYLF8HkZAUO7ZZ6RBjUpy2WUZ1HkgYGhuUKOiceYdGCPK+57hZIjQwPhvHcR/9c138vc//kaOL776RqVbg7AGgYxlGuRbo1qV1b1LFimgBs/6vYE9L5APhfPnVp+Dgb0Y6lQtpz7DMPDankN+xkmEgbrmmv8zzkT+/fesnD+f0nAAA0Gz+tVUeAYcMC6nB4oVekoK+uQhGKibJQrnN85EPv7sS+NTaCAEy7ZdB4wzkZrPl5GKpYsHlSf8Br3wfNkSxjdpj7WeQ75bNqwl1/3vWnUeCNTButUqSI6HH1Dn+w8f8+Wbv/6wE6myOHnqI9mx56D6jDrbuHYVV52A90H90pN3ew+9Id+f9g/Jcc3VV5uGrnP//Sfnzp1Tn+0UfCq3WQ86Nm8gV13lbzyL1H1CIa11dY2Kz8mN119nnKUE5VG+VDEzv7/57rTaWNNKuPo956MP+dq4qsZZbKlYulhAoy+wy3OBPDklf55cxllK8O7PFSto5hval9NnnEOfWUEZRqrdD6f9QrrrVClnGqExAYCNc52M0FYw8RtMjlKDVU8gPzs0r68mx4KBsmzdpLZZf70Sqb4DJkw2bNlpTixU99UztA9ubUgpn8zUrlJW9R2eevJx9e6aaPUdCCGEEEII8YqjkR5xl71y4Ag3SnIDBo7eCeM9HXavnliBwQo2vbPi5DUcDWDk7NOphfTt3FJ5bOFAbGAYSWAojRQwUGfLerdxFhwMqOHVCuC1euJ9/7jBiAGvvelwnR6Au4HnIx2p5dff/vCLUwyPsVtvzmicBUYZOnzXhjrQfvTB++VWl0E8BsTWFRFJA+1/jLMkEH9Ze9Ej34oWzBd0kK3J9dhDKibsK0umqwPebBp7XhQrlM/Rs84OZAtlofPi7ZOn5Nff/WPh5sn1mPn7kjUbZdnLG5XB2Q4MPHgfHOEY9mJBjoezyw3X/c84cwb5ePddyZ653/1wOkWZeuEHn05EvG4NDO+BPIKtwLBWrmQR4yztsdbz/HlyKsOiF+66I5NaYQEwaQsdEoxIlcWRY++YRqz8uXNK5ky3qs9uPJT9PpUG8Mbxd8W+WXyWOzMpz3ewa/8RmTBzod9EmebmjDeY9QDvb6+HkbpPKKSlrkY8e+SlG7fdklHuuesO9RmbBv/0s78nfbj6HTqtWMGnTEN2LHEzXqNMH8qerCvR7rsZ0iEXOt9Qr7SsByOS7X647Vfm22+VahVKGWeIvX5cfv71N+MsJSg/t/xLDVY9gQnCnI8+qD67keuxh9VkSihEqu/w3fen5ehbJ4yzpJUGbvICMNGBFYS672CdiIlm34EQQgghhBAvpLB4/nv2bEhGl98u8I4oDGwwFmNTPa/HvXffZfx3+uWLr7/1M6JhkOhlAFS7SjnHPAl0wDveaRCEwQ7yHRuj4cBGeF6MeKEAA5vXe954w3V+BoRTH31mfErixPsfGp9g4LpXXe8FPN/rgDgYp8+ckU8//0p9Rt5pr10vZM1yp5+R2wsZMmTwZIzIeGPyBpfwkj1/3n/i56NPkz0TQ8m3YFjzAlRu2E6uyJzD0/FMxQamseLLr7+TH077b1Ca74kcpsccPCAbtu0lWZ8oIbWad5EpcxbLoaPHXUOZxCtYkeClTK3GQXgLujiAOgJvYb1BJmQva5YkQ5sX7suaxc+Ak5ZY6/nqDdvklgcLOMqR/bj+3jwya+EK4z9T6g87kSgLtOMfffq5cSbKAH7t3U86ps9+ZH60sPLuBagPds9/eNE2qFnJNEqNnTZP7s5VXEpUaSyDx0xT/4sQVG7ewZG6Tyikpa5GyDgv5YgVOv+7NmmVDvL7v/P+mwCnRr/fmfk2z5NJseZay0olL1jzzSuYGIpUu5+a9gtlqOXeqa2xcstNGUPOG6/Y9cSD92c10+UGDN5YrREKkeo7nD7zs3z6RVKdCLUNCUQ0+w6EEEIIIYR4IYWR/orLLw/J6woDnguZLHfcrjy4u7Zu7PnQHpPpFRh7du07YpwlgUG+F7lAaA+nPAl0NKtfPWqDTze8eCJqrrrySrntlsBL9c+dSzaq4Dpc75VIvD/KTKcBRvdQjAVXXHG5XOk7YoE1tnSo+RYIa16kBqwksRsHMVGV0KuDiu2tDRlnfv5FxVbv2Ge4FCpfV7LkLCZNO/ZTYTWIMzBCaoMGvGFvCCGMAyb1/u9q9wnDaBAJuQIwOEXS8OwEbo+6EAns94GRDeElZo0d5GcARhgZGNfL12mp6kHxyo1k+56DKQxsmkjdJxRiravDIb3q93gkyx3e+6xu7X5q2i+UIcoy1tj1RCj5Aw/yqyLsPOEVTC7oVTehtiGBiGbfgRBCCCGEEC84xg4JZTOo54oVMj6RC4V3T30oazfvMM6SgAfhTb6BEEkiLYxsJLYE8oKFcX5Qj3Zycv8rsmDKcKlaoZQKzaCB8Rmbgxav0lhe3vgq5YSkwKunfLzgFGYM71C5XEk5sm2FihXfpkmdFJ7miGdfukYzGTBycsAVJpG6DyHRJlrtPo26FxZeV9AQQgghhBBix9FI375ZPeNTcLCxVNumdY0zEogCeXLJv18f93T07tjc+K/YAAMIwjJYYybDKFm6RGG1yuJC4vOv/OMsBwNeW4j5rLEb2awb+IUap/v7CCyLhhFNp8EppnEw/vjjT/nt92SPwLTE6pkabnxzO9a8wGqJzctnyTcndod8bFs1O2joKtwbG54unj5KTh3cKJ8d2y5zJyZI7pyPqt8RDqfLgNEqpjfxJ8OlGcyVCAhb8PMv3uUVGwb++VdsDLXWeo5VQChzJ9lxO7y2sakB6slqXB/QrY1jWrwclcoE3qwXoUMQj3vs4O5yeOty+eG9/crYXqNSaeMKkeETX5Q5i1cHNURG6j5uxFpXh0Nq9Ds2uP7Hd5AkQilDt3Y/Ne0XyhBlCWJp1LXriVD6RQiPGYrujiRYtaBXd4bahgQirfoOhBBCCCGEBMLRSI847DA2BQMG2xfHD3Hd3I6kH7DMd+rcJTJ93jLjmyQqli4muR9/xDi7cMBmiE6bfjqBAfXJU8kTF9nv89947pEHshmfRF3n1YiC52Mjw9Ryc8aMco8RkxWe3NbNz9z48pvvzPjgac199yQPZGHM/va7ZINIuFjzAgYZbDB30403hHwg3q7VoBcMGG9gMKhTtbxs8Q3smzeorr6HoX7pmo3KmBEpsImeG4jh++dffxtn8Uem225WGwgClPsnljjAbnz0yedqk8hQiFSeWev5Z198JZf72kEn2XE70iJsCoxY993jv/kiwgQ5pcft8BrDG1x/3bXK2D5/8jCZN2moORmzct0W+SIEA2Ck7mMn1ro6HFKl37/+LmbpjkeQF5Fq963tVyiyBFCGOuQX9g24JYTwe5HErife++ATM11uYKNVt02wo4V102C0IW+/e0p9Tg2x6DsQQgghhBBixdFID2Bs2r56rhQrmM/4Jhls9nlg8xIVf5xcGGDQOnLybBk+YZbxTRIwPNarVlGu+9+1xjdpAwaJg8dMk2x5n1PHpBcXeR5Ye2XPwaPy4SfBN3DU7Nx3WHbtT4rTj0msRx5IMjBqEE9Zb+aH63C9F959/0Nzg8bUcN3//DcT3LR9jyePQXglr920w/OgPNLAUJv3icfU58NvvK3iTnvxksW1ZWu1UAc2dzvxXvJmkPa82LZrv/z6m/sG13gu7oPyw/HWiff8YvW++c57KkY2nlmpfht5+6SzUQDGxOoVk71/T576KFX5iwnRW26+yTgT+frbpDi8wfjxzM/ywcfxa5jD+zz6UHIdWvPKq57qN67ZtuuAcRaYaOWZtZ7vP/KmHHvb2yqJX379XYVs0bIFz8+0IE+uR03jdij67rsffjTTuu/wMT/5xbu07j7YrH/wbHcCE1cln3la8uZKqt8HXn9TTQhqInWfUIm1rg6HcPU79NfGV3eZsbtJZNt9a/sViiyh7FCGmrxP5Iipw4tVT+zwtcFo67xw4MgxpQdjwW233ixPWpxH1m15zVM7/+NPP0uzTv1NvbNg2Vrjl+j1HQghhBBCCPFKQCM9KPTUk2q558evb5VNy2bK1hWz5Yu3XpOVc8bLYw9lN64i6Rl4z8N4VLt5FxXv125MbPNCHSUHaQkMcX2HTVRGengi4+jcb4RMnbMkogMfeIAhtI9bjGNcN2XOYuNMpETh/HJfVv+lzHfflVkK589tnIm63s3DDM998aWV6v1SC5bKlyxSwFz+vWHrThUX3W0TtP2H31Se3rHirjsyqXRrps1dKu9/+Ilx5gzybe6S1WoAjSNDhgySOdOtxq8p8wKTEBtf3e1q/Mdzm3fpLyWrNlHHa3sPy6WXJocgyHjDdcpzEM/E/V5/8x3jl+gC7+c7M91mnIm8sm2XfP3t98ZZSlDmK9ZvURMZ8Qo8s0sWyW+ciZLBtZu2u5bRq7689yKv0cozaz2HrsSqIxi0g4F7z1y4XIo931DJVe+h4+X3NJoUeyj7fVKs0FPqs1d9h98Txk0368H4GQvk3H/JeuTqq6/05e9VZv3bf+SY6z2diNR9QiXWujocwtXv+w6/IQtXrDfOCIhku29vv7zIEsoMZYcyBNjPBA4vsQp3A6x6AhM6qPNueu3b70/L7EWrYjbBjw3Ey5V6xpxceMkn58jTYG0IftuyY6/MW7pG6RysIMS7a6LVdyCEEEIIIcQrQY30mjsz3y7FfR34Z57Oo+LQk/QBPHvGTZ8vo6bMcTza9UqQnEWfV8YjGLHstGhYQ1o1qpXmy3bh5bZ1Z0qPxXWbd8hX33xrnEUGGNnGTpvnOGDHwOzoWyekRZeBpvEOA+o6VcupAaIVnON7vYEoru/cb2RAg/OZn3+RnkPGqkFupMjxSHYVI12DVREzFixT3vJ21GD1tX3StueQmHpZIgYsVu1ob0SEAHihY19lAHcaGMMgMGz8TDPfMECvVqGUWl5uxZoX+J8+wybK6g3b1MZ/TsD41mPwWLOc4U2HDbSthhO75x7S4GSQgSytWLfZOEvyGk/Nfg7Io6KF8pnGCHhrYgILMmQH77d87aYUK2LikSJP55Uq5Z9Vn3UZLXt5k6PhEbIAD8WeQ8Z5MgpFK8/s9Xz9lteU3MBg5QTeZeGKtX73Ll+qqDLupQWoF/WrVzDzAfoO9SdQHmKCFPrQGvKsctkSfvULsvx03lzGWdIqCOSDvb7ifPvuA3L4WFKdypY1i5kOEKn7hEo86OpwcNLvIya9GNDYjBAgvRLGxWxiIZ7R7X6g1TuQBciEW7sPPYM9E7T3Na5HfwH9BrscA7TFaJOt+qB25TLy+CP+GyWnlssyXCbXWuoIDO729FgdHux6wk2vQaba9kxQBuxYkj93TqlXrYJxJipN85aucezzQM+jD9BvxGTjG5Hny5TwW9EFotF3IIQQQgghxCuejPQkfQLPRHjH904Y73hgoBooVm3rxrUloVcHueb/rja+STswMHKKOQ1jxN//pBx8hUvOR5MGxjDewftp4fJ1apnyp59/pQafGPA9W+0FlY+alg1r+v7vIePMHwy0dSxyAE+tIhXqq1UAO/YeUoM5GJ8RuqdElcamIUx7baUWGBAa1XreNHgjHzv2GS6lazRL8W5NOvRRqydQ/tggLTVGr9SCuL5tmtQx04DBbqnqTVUakVakGWlHvhWpWF+FZdLAcFK2ZBHjLBnkRdN61cy8QN7X8r1vlUbtZdHK9ep+iNENw2/CuBlSrHJjP4NDy0Y15f57/eMP457Ply1hlhfSWbVxB5kwc6G6Hwz2GMzXeKGjn5GzcP4nU52/uR57SJ61hBeDwRAyhDyBTH306efKE71Oy27SsG0vT4bsWAPDUNsX6pjGUpRR/dY9pFL91mYZWeUVYY1Ckddo5RnqP/SABmktWK6u9B8xSdVzyBXSDlmo1qSDNO88wLz3M0/nVUYlGPfSihJFCvgZslB/UI9GT52rVlFhkg75AR1RtlZzpQ81dauVd6xfBfI+IeWefUZ9xrshbA10DeoTygz5gHN8r9+9QN5cKTZTjNR9QiXWujoc7DoN+TFw1BTVds2Yv1ylF+lG+vEeZWo2Vzoq1vo93tBlCDmHvNvbxjY9hkiBMrWVTGiCtfsPZMsqXVo3MvMY/QX0G7QRW7dfeA7aYsizlmVM2LX2tX2RdoTAnhf3Z01uvybPXiwTZ70k777/kRw6elw5acBb3go86dGearReQ/sIPQG9hr+6vYTuxDtDvmIFVmTBmcRaJ6Bv7X0e6GK0/egDoI4A/A/+177fRjT6DoQQQgghhHiFRnriR8YbrpdZ4wbJ8H6dYjawx3PhLWkHEwZXXenvyZYaXqhbVQ3SAAwcMATmKVldsj9VWsUcn7lguTmYBj3avaCuD2Rgw0AbKw9geNTACxOGnueqN1Wx9TH4hwHlnZNJG7Xifr07NlefIwHi5g7r00nFXdZgYG1/NywNx7vBQDohoZfkz/24cXXaA4+z6hWfkykj+ir5A0gb0oi0Is1IuzXfADzw+3ZuFXAiCTI0Y/RAc7ANsGKkUbve6n5ZnyypDFwwdOmBO+jbuaXUrlLO0RMOnnsIAaWB0bjrgFHqfjmKVJKazTrL1p3Jxp1alctKpTIljLPwgUG7a+vGfuWKvECeQKYeKlBOqjXpKKvWb1W/If3aSz2eQX6OHtDFNNQD5J8uI7u8oiwhJ1mz3GlcHZho5Rnqf5sXasvgnu2Mb5IMOcMmzFL1HHKFtEMWrCuUkPbhfTtJ5tuTQzOlBTBCIa1WozTyodeQcWoV1d25iqv8gI6ArtDAG3RA19aO+5HAMNeheX2z3FA22HQc9QllhnzAudafKAPoRntdjdR9QiUedHU4QKeNHtjNL2422i6siEJ6kW6kH++B90G+xlq/xxsoQ13uTm1jqO0+2omq5Uv5yqWr2WfC/+M+1vbLXr8QVmVEv85RWZ2KtFpXEkE/oZ3CyslC5esqJ42///FfRYA61btjCz8diP9D+wg9Ab2Gv9b2skf7pqofFUvQ55k5ZpAUyJO8KsdernZdjPqDeoT/dSIafQdCCCGEEEK8QCM9UWAwB6PFoS3LpEGNSsqbKFbAEAMPMzvYsPiOTLcbZ6kH7wyDdq8OzczBrBN64qJ3pxauxiH8PrRPR2UY0QZnJ/A8DP6xWiHYs8MBewismjvB0QPWSu6cj8rCqcNV2JFYg80hETZgzYJJKl3BQH7179paJg/vI7fcdKPxrTOPPJhNVs6ZIA1rPm98ExjI3cKpI6Rn+2YpvOs0MO61a1pXxg3p4VpueCYMAdZQIanhiRwPy+zxQ1zzBwalhF7tg8pfvABjRsXSxWXZrDF+RhYncN3i6aP8Ygi7Ea08g37EBMDSmWP8JgECAY90XIv0xALIKmTRTS8BXIt3mzMxwW/yxA70zIIpw/wMxk4g71EGgd49UvcJlXjQ1eHw1JOPy4rZ41R9CAbyK170ezyBMhziK8tRA7oGLXf85rXdR7sAfY+2xk3X6Pq1cNoIR2eESFH4qdy+vk3zkGQWEwYzxwxU8h7s/5A3qDdoCy9PRSi3SPHwA/fJUl8b4qWdR71B/UE9Ckak+w6EEEIIIYR4gUb6i5h8T+aQ+jUqyssLJsuHR7bI+ISealO9WAMDWB/fwBheSTAS4RgzqLu0alwr4mEiMPiGwXffK4uUsR55osHmgsiT47vXhjRxgeuw5PnYa6uVp1yxgskebQixg+fgeQO7tw1pAB0KWIK/YvZ42bZytipjbUjE4Bqeci9NG6l+gydzvDh9wWCL9Lz28jyVNoTA0SGJAMoDRrU3tq9SeehmONHcfuvNMmPMAHlnzzr1/9bygGxVrVBK5cf+jYvVRIFb6AGULya0Xt+2QqXDmkZ8Rrox2TV9dH/XSYRQgXwibxZMGa4mYbSRCc+FR/KxHatl/JCekvHGG9T36QGUO2L9Y3PydS9NVfKp3wty27h2Zdm5dr4smTE6qNE4ENHKM0wsVS5XUpU1dKi1ngE8F/KB+784brDa2yWWaL10Yt96lReQe52fqA+oF7p+DfEwYYFyw8TKjjVzVf3C5pf2erX8xbGyffVcP71qJ1L3CYd40NXhkPXuO1V9QL1oVKuyWY46v+JRv8cTMKS2b1ZPjmxbrmTeupFwuO0+5LhowbxKTl9ZMt1PH4RTv1IL2rGOLerLmvmT1Kozq94b0K2NauucQFoh77p9s9Y53QYj31BvvOZNWhCsnUc5oF3W7Qjqjxci3XcghBBCCCHEjUsSnXa3IoQQQuIQhBiq3byr2mQY3teLZ4zyM44TQogGm8N26T9SZi1coc7nTx6mwpARQgghhBBCSLxBT3pCCCGEEEIIIYQQQgghJEbQSE8IIYQQQgghhBBCCCGExAga6QkhhBBCCCGEEEIIIYSQGEEjPSGEEEIIIYQQQgghhBASI2ikJ4QQQgghhBBCCCGEEEJiBI30hBBCCCGEEEIIIYQQQkiMoJGeEEIIIYQQQgghhBBCCIkRlyT6MD4TQgghhBBCCCGEEEIIISQNoSc9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhBBCSIygkZ4QQgghhBBCCCGEEEIIiRE00hNCCCGEEEIIIYQQQgghMYJGekIIIYQQQgghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhBBCSIygkZ4QQgghhBBCCCGEEEIIiRE00hNCCCGEEEIIIYQQQgghMYJGekIIIYQQQgghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhBBCSIygkZ4QQgghhBBCCCGEEEIIiRE00hNCCCGEEEIIIYQQQgghMYJGekIIIYQQQgghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhBBCSIygkZ4QQgghhBBCCCGEEEIIiRE00hNCCCGEEEIIIYQQQgghMYJGekIIIYQQQgghhBBCCCEkRtBITwghhBBCCCGEEEIIIYTECBrpCSGEEEIIIYQQQgghhJAYQSM9IYQQQgghhBBCCCGEEBIjaKQnhBBCCCGEEEIIIYQQQmIEjfSEEEIIIYQQQgghhJA049+zZ41PhFxYnDv3n5w/f9448w6N9IQQQgghhBBCCCGEkDThjePvSsW6rWXPwaPGN4RcGPzw40/SrHM/GTd9gTLWhwKN9IQQQgghhBBCCCGEkKjz3gcfS7teCbJ9z0Fp2XUgDfXkguHMz79Iv+ET5aUV62Xo+BkyYebCkAz1NNITQgghhBBCCCGEEEKiync//Ci9h06Qw2+8rc7//udf+ePPvyQxMVGdE5Ke+fvvf+TnX39Tn3/7/Q9lqF+5fotn+c4wwIfxmRBCCCEkLvn62+/l0BvH5fSPP8vNN90ol2XIYPxCCCEktVDHEuIM6wYJl3/+/VfeeOtdOfnBx3LVlVfK/669xvjl4gXG+P4jJ8niVRvUeZY7M8ucCUOkeOGn5JJLLlHfEZKeQT0v9NST8uEnn8upjz6Vf/89K0ePvysF8z0hmW+/1bgqMOnWk37J6lfkisw5InLsP3LMuGv8gbRZ05q7RDVV0Bci0XxXr/fGxg6vbNslxZ5vKDdlzy+1mneR9z/8xPiVEEIubKxta+vug+Wvv/8xfoktuw+8LoUr1JcyNZtLwXJ1pOuAUaqTTwghJPVQxxLiDOsGCRfICeQFcgP5gRxBni5m4Em8dM1GmT5vmTqHMXNIz3bKoBkKP/3yq6xav1Ve6NhXCpWva45dbn+4kJSr3UISxs2Qd05+EFKIEYx5MPbR98KYiFy8QFZhLxw9da48V72psg1CLrLlfU5qt+gqy9dull9/+924OiW33XKTJPRqL3mfeEydf/7l1zJ84osqVr0bDHdDiAXERKvfuofsPfSGWpoC5d+88wD57IuvjSsIIYSkJX//849aIojOjWbRyg2q800IISR1UMcS4gzrBkkNkBPIiwZyBHmCXF2svPPeByo+t6ZH+6ZStXwpzx70sMl06D1MsuUppZwpFyxbK4eOHjd+TYoFvnXnfhk4aoo8UbyKPFutibLvwBGTEK/AkN6p7wjJX7qW9BoyTnbsPaRsg0DV43VbpG7LblKgTG1ZvWFbQPl68P57ZWjvjmq1CFi7abuapHILe0MjPSEGaDDXb3nNrIAaeOEfPpas/Am5GPn9jz9l5oLlMmrKHHVYO0SEpDXXXvN/cvnllxlnaQNkXss/6gLqBIl/sNRUl9vEWS/Jt9+fNn4hJDJciO1jLHQsIdHm37NnZdnLm8y6uvHV3cYv3mHdIKnhyiuukEsvvThNcAj9g400sWEsKF+qqNSvXlEuu8w9fBT+d9rcpZKvVA2ZOndJCntNIOB4WbpGM2nRZSD7f2FysdkAMG5o2KanTJmz2FXO4Glfs1lnGTNtXsBVG0/nfUJaNqxpnInMWrhCTVYFg0Z6QgghrqBztG7zDumdMF4dH3/2hfELIdEFMTzhZaO9EEDF0sXkgWxZjbO0ATKv5R91AXWCxD/fn/7RLDd4XAVbmkpIOKT39jFedCwh0ea//86rcCO6rh57+6TxizOsGyQ1PPrQ/VKnajnjLCn2OgzTV1x+ufHNxQU2iYUXMch02y3SpkkdFRLEDRhKewwaK+17D1We8hrkZ7c2TeTlBZPlyLbl8snRbbJt5WwZPbCbiv1tZd7SNVKzaecLNmx0NLmYbADwoO85ZJxs27Xf+CapHo8Z1F32b1wsHx7eLJuXz5K2L9SRjDdcb1whMnzCrIAbw2bIcKnUq15Byj37jDrHJNWKtZuDhmK6YIz0WErQt3NLSejdIeTj1pvdlQO58EFHrGSRAik2dCmQJ5fkzZXDOCOEEJLWFM6fW3avWyAbl86QvRsWyagBXeWa/7va+JUQQkhqoI4lxBnWDRIukBPIC+QG8gM5gjxdjMDQu+aVV+Wb735Q51XKPSsF8uZUn4OBuP69h45XXs2a7PfdI/MmDZW3d6+VIb3aS5kSheXxRx6UOzLdJkUK5JF2TevKq6vmyIFNS6RsySLGfyVFR4AB9rsffjS+ISQZvV8CQtJoWjeuLbvWLlBG+dw5H1UTQ8UK5lNG+12++gzbIcBE0ugpc+WDjz9T53ZuvTmjNKr1vGlnfHnTjqATRheMkT7LHbdLq8a1pGvrxiEf2bJmMe5CLnagyGePH6w2eEAlqlutvLzoO7/7rmQPCkIIIWkPdsMvUTi/0s9YLkwIISRyUMcS4gzrBgkXyAvkBvIDObpY+eiTL+TV3QfUZ3jR16pc1rUuwdMYoW30JrOgTtXyygBfu0o5ufqqK41vU4KQQk8+/oi8NG2kcuTVILTxsPEz1SaxhFhBOKSN23YZZyJN6lRRk0B2B14NVlSNGdRNcjz8gDo//u77smn7HvXZCUwgYSUWgDf91p37AsamZ7gbQixAoVcqU0LNeP946oDMmZDASRxCCCGEEEIIIYSQEIAhEgZJHYu+5DMF5OEH7lOfg4E9AWfMX26ciTRvUF0mDu0lt996s/GNO1jN0K1tExncs53xjcjCFevkVUs4E0LAifc/VJsMA0wkNa5dxXXVFAz1cOrVvPnOewHj2F/3v2ulQqmixpnI9t0H5MefksM3WaGRnhBCiB/oTHEX/PQFy4yQCwPWZZIWUM4IIYSkBT//+pvsO3zMOBMpV7KIq/ETYW7mLF4tn3/5tTp/5um80rN9s4BezcGAxz4MrhVLF1fnMKKu3bxDPYNceITbvzn61gnjk0ixQk/Jg/e77z1yySWXqBUbmo8//UJ+suybYCffk49Lcd+9wf4jb8qpjz5Rn+3QSO/Ajz/9LOVqt5ArMucwj4RxM9RvqMyIVVS+Tku5KXt+9Vuh8nVl6PiZ8sVX36hrrGCZzq79R6RJhz6SLe9z6vpHC1WQTn1HyMGjb6Wqg6zT8nyDtnL7w4XUvfM+W12l5bMvvlYCGipY+oONEl7o2FelE/fEez5XvamMnjrX8R3dQDowczpg5GSVPtwT6a3VvItseW1fqjbfi/S9ncp+yepXjF+TCXYdNqWbu2S1n4wgbUgj0hpqueBZWOpVokpj81n4bC2PYDIbC5DvkHvIuS4XHKgr4eYDrodc470hjzpvIaeQV8itl6Vr9rxCfDoQqNy6DRwdML2ffv6VFCxXx7zXyxtfNX5xB5tXDRw1xfzfak06yk+//Gr86g/0xIn3PlR5hzzU/4P0haJLEPssd4lq6n/xF+f4P2yiZdVReUpWVzubI2/0szI9Uli27kz2OmjQpqf5mz4CyZxTPUUeI6+R55HoJOE+Oh3YZf2XX4NvDolnQm70/3QdMEr+PXvW+NUZ+/9gd/ZARFNecW8sqUPZaz195R2PKzkIxPenzzjqkXHT55vxIUMlLcpVE6je2oEe1te07j5Y5TFkHJvDWfML7QTaTrShgdKJ/8U99P0g8xrUBdQJ/RsOXacC4da+httuO5HedJMTKJeNr+4OmF/B+iPQRTotz1RsYHybtBRV38t6BJIn4PaO+N9gmz9pUluX8f8vvrTSsW8B2YklX3/7vfJ2s/ZH0Z7UbtFVlq/dHJYugF6CfrLqT93fttYVpzpvJ5D+0GWLdt6uw6Abgm0yjHvo+4XTPnpJtx3rM/E+eC874cpZoDyyE0kdG4xA8m7XK9Y8cdPBoRIoDeHWObTDuF9q6olTPw7l+8lnX/q1xXoshNi6TmMhpz6vrl/hjPUgB2hn8C7W8W6LLgNUGWkd6ZR+J6w6XNcdpzG1zr9gYz7rM6+/N49f383a1unDXh+91g07qWnDwiFeZQPlhjxD3dVp0PdEusIZEwItD9b81fKwecdeswy9ll8kZc5KatqocMamwYDehP500tXQTUgHQBqRVp0ee/sVKpDBw28cV5/z584pT+R4WH0OxiHf9YhhD2CYR0zw1IQLuuWmG6VOlbLGmcg2X5v9/ofOBlI7KGeUt5N+C6efa62Xul+pdQPGamgrNE5yaSeQ/MSyj2MnWu0f3jGQHSMUIPOff5ms3xBJw+uEEOLNY3NZgPf4+5/AegF7oT6V+3H1GZNFR469oz7boZE+BA4dPS4lqzaR+q17qIqqlzLge1QyCMTC5evMjgjiGjXt1E/9D77XM4EQqMmzF0nh8vWk3/BJAZdEBOPA62+aaXll2y5zp2sssUBaniheRTWmXjvGEHA0aE+XrS1la7WQBcvWqnQCpG/H3kPSa8g49Y4TZi70fF+kq8/QCb771lHpQfr096vWb1VKAjtte1WSVqJ573CAwoVCK/Z8I2neeYCfjCBtSCPSOmzCLMcG2g7kaNnLm6RIhfrSofcwpYA0+KzLAwNjLXNuII2QHTQCUGIwvrx1IinfIoG+f9FKDZV8Qs51uQDUFZ0PKDstt27osoZc470hjzpvIaeQV8ht8cqN5LW9h0PutKCsCpSp7Vhu42csCFhumTPdqjobmg2+uui1bnz1zbeyY88h40ykTIlCcuP11xlnyXz59beqA5qrWGWVd8hDDdKndQmuCXXQ+Psffyp9AWOtVUdFEuQHBkH2eoo8Rl4jzyEr1vcKh3xPPK42EAcHjrwpH30avHH+6pvv5M23k2Xz8Btvy7cuxmrkL64DaLx1I2snmvKKuj5t3lJVx1D2Wk8HAh3L+ctelhyFKzrqke6DxkiuopVl2tylngYbmrQq19Ryxtdx7dBnmOQrVcMvv1BGaDvRhqIsYGCKFihflDPKO1j7CnkJRS8GIz3rJvRHVm/YpsqsUv02rv0RL+1puHh5R0wCVGvSIeS21GtdhvzAmIK+QKtugxz7FsirUPoCkQLp6Dd8ojzm0y9tew7x64+iPVm5bovUbdlN6QKvug56CPoIegn6yao/kf+6j+u1LxWIpLRPUmWLdt6uw6Ab0C+AkS3UAXg8EWqbESqR1rG6/geSd61XYCRA/YwGka5zkFPoKrTDuF+weoKl9qHI219//63qQu6S1fzaYtwfYyH08zEW0gYLvBvex6nPq+uXfTwbDNwP/X60b9Vf6KTexTrehScsdCSMwqnpY+J/cQ/kkbW/qvMPY766Lbun6hmRIl7asFjLBkC7iPYRMoC6q9MAcE+kS48Jf/jxJ+MXdzC2x30hD9b81fJQoW4rKV2jWar6drGWuUBlgTwMNjYNBOxSLboMVPrTSVdDN+UrVVMZb8+6OC2FCspLOwTB6/g2l3A1cBbZ4xuj6HeGV3PBp55Un1MDQhr/+/VxdXx2bLvabNYNpB31BOXtpN+Qn8hXr/KLd+o/YpJZL3W/UusGjNXQVoRSz5zA/eKhjxPN9i+adox7777T+OQOnPz+9lgPM2S4VJ7O94RxllSftZxbCWqkP3vunAwZO10VLjwurr4rp+QvXUsV9MXGQV8HpHP/EfL6m86zHQACh47qyvVblCLEDOSileuNX50ZOXm22rHa6+AZbN99UOq16hE0LShsGFC69B9pVoRAYEA0dvp8qdOiq7xz8gPjW2dwL3icYtDqtjM2fm/asZ+MmjLHUfg0qKyY/XJ7tpVo3jtcYB+ApnsAADm0SURBVASp37qn8lYKBNIKZYLGMZjixW9QaFBmuhF1AuWBgfGIiS/Kn3/9bXwbGHSkITtoBAAGIZDTUGcbnYBSxaTC8/XbBpVNgHxA2dXzdW7cng2F27hdb9eyBnhu1cbt1YDUa8O21ddQIQ+C5XOgcrvi8suldInC5kzr/sPH5OPPvlCf3Xjj+LuqPAA2FCr6dD712Qpmv+EV/tKK4HoE4Joqjdo7emA68edff8lwn9zgiBbQLYNGT1WdkGBlh3KDfoVXSLhk9TWmhYwOHDqCew4eVZ8DgSVt1rqKsjgVYEd2zaE33jLTiOVq99x1h/psJdryig5h32ETXe8N0K4k+Npw6Mpg7QB+a997qEyc9ZKndKRluaYGvFdv38DPuuGUE5hchR6NRjqRn/OWrlHlHEm96EZ61U26PwIHh2A6Gej+CNrJUAb3XkE+eH1H9DWqNOqgPEm9GKKB17q8YetOady+j2tfoFfCONX/9Pr81ALv+Wad+6s2JFK6DjoLxivoo2A6C89Dm9x90GjXZzvx+x9/qb43+uDBQJ7Xa9Vdps9f5llHxxuhtBmhEmkdizxGXnup/+hvwngVDUecSNY55Hu3gaOUrgom0wD1pHqTjp7lDf24hHEzVV0IVr7QT5jwwtgU+Va7eRfXd9Pj2WDvht9WrNviqd+P8QY8Jk+e+sj4xjuffP6ldOw7Qt0jGPgd17mNT6NJvLRh8SAbmJhFu4hnBAPpQ9+nYZuenvo+6N+jT+F2X+gdTETs9KUjVGItc6kZmzqBfG3i02nzfP3RYGAMg+dCB0GWIwEM7u++n1zvH8p+r+uGsVi9adUpzz5TwNFZJLXAYBoMTPLAjuQma/N8+QqbW6BVpxrIiZd+E+oZnjtuxvywJkzipY+Dd4xm+xdJOwY2IZ4yoq85iYONjb2CctW6C2GcrroyuHxnv/du05EqUHicgJL59run5MniVdVAHIMxKFtUMhg2sGQC3gXhLpFPj2DGSXtQBgPCCGUJBYfdo72waOUG2WFsUuAGjEoYvHidKZq9aJUy4AZStKgEMMxgUBRMWdjBDHzCuOkBJxeUccj3u9c8QN626THEOAtONO+dGqyzq25MfnFRwNl91DV0foaOn+G5TCBzew+9YZw5g/q76dU9KdKI/HFrfNxAmrFUCXKvlTCMQ9hI4+UFk+XUwU1yZNtymTSsj+TO+aj6HSD0Q0+f7AXqnELpofODWV4N/h/32b9xsXx4eLOsXzRNmtWvbhqjkGcYkHo1VsDQiDzJft89MmpAV3Xfk/s3qHTXqFTauCqJ4RNmyWv7kj1MQa7HHlIz/AAKGpNobs+FDGNSR1OySAG5645MxlkSuFfzLv399E6V8s/Kitnj5Piul1WeIo3Wd4d+QCfBi7ER99chMOx5OmFoL7kp4w3q+29O7FYH8qRYwWRj3bRR/c3f9NG+WT3j1yQQfxBL2zS43+zxQ1T6D25eKv26tJKMN1yvfsN7ojMRbOldMNC4FrTMTMPIGCjkDfSh0/KyXfuOqHriBMrMGk+xQJ6cZr5r0kJeF6/aoP4H8prQu4PsWDNPPjm6Tf3NmiV51h/vAcOJtfOCZ+LZ1jqJ8tD5BvmeuTB5g6ZApGW5pga0U8gvgLqM94YcoyxQ15GHGqd0opM1tHdHU74h8xrUBdzLKv/bVs2We+++y7giWZd36T/K1OVOZeCkFzFohjylhvSmm5z6I5AjLHHevHyWqj971r+k5N5adnCGwGSU1ZsMukiXC+qcBstRUf7WcsNhzX+AdLbrleD3jhgkzp2YoMoM74j3xXtr0I50GTDadYJQ46Uuo589dc4Sv/xA/UI+QP5Q73TacQ3as3fei75TAmSzbc8EVcc0yNsxg7q76rrVr2xT53ZQ/ljmPcnXP9LYyx/3xjP0smLouHCW4UNetGEZ5bp05hgluzjs5Roo3ch3LT/htI9phdc2IxxSq2PtbNq+229CAbKD+2rdggNlVbZkEfU7dCUM9ZEkknUOOgnvY934EP+r+wTw5EQ5WHWaljcvfQJrP07LMdKItFrbYYAxU4/BY9V4Ac+wv5c1XwGuGT1lrnwQxHkBuq5Xwng/4wvuYa1P1vtCBjBOgAE0FOApjfRDHlo1qqX0AXQw8s7pPafYjJZol9E+ox5++sar0qBGJeMXkR7tXkhRV9HuuxlZnIhkG5ZaYi0b6INjokKPOe19Hydd66Xvg7a555CxfsZr9GFRL3XbjGdg/Iln4vmwmxx43ZsDkya1MpdaUjs2tQLDcd/hE1X+atCGDu3T0a896N+1tWS5M7MqX6xsQB5EAhhTv/zmO+NM5JEHsxmfAvPjmZ/kq2+/V59RBjkefkB9TmvgmAy9pesE8glyAHlAvbbKAcIawakjkN7W9itrvwn5jXx3KgeAsrWOt7wSD32ctGz/rPeFntV2jLQA4+19FhvcvffcJTda5MIJ/I7rwKdffCWnz6QMXYgMSME///6b+GihComXZ3os6FG8ciPjP9IeX6fQMU2hHK26DUr886+/jTsmc/rMT4llazV3/J/78pRK9DUgib6OpvqLc6fr9JHx/qcS67bspq4fNmFWYqX6bRyva9i2V6Kv42qkIJl9h99wvB6H9d448D63PVTQ8bqtO/cZd/Rn1/4jju/wdNnaif2GTzTvjefgPvbrfB2LxPPnzxt3SyZQ+VjzL1h+4HiyeNXE9z/8xLhjMtG8t1PZ43l2vMrIkLHTE4tWauB4na8BTPQ16MYdk/n0868Si1Son+J6r+WtDzzbCmQd/+Pl2lDxddSUzOj7If2vv/mOo2z8/sefiYNGT/V7vq9Tm+Las2fPJfoG3+Y1eH+kE/9vB/+L51nz7bHCFROPv/u+cUUyTmVXs1nnxK+++c64Ipn//vsvceW6LX51pEmHPinSMGfxKvP356o3Tfz62++NX5w5+taJxCw5i6nr8V7b9xw0fkni73/+Sew2cLR5Tzz/lW27VHqcwP2s+R9It0HmIfv6Ohy9E8Y76h479nxzqhd2hk+cZV5fsmqTRF+H0/glGasOcsqLULC+H/IX+eLEZ198lVi4fD0zbfoIVnYn3vtQyRSue6hA2RSylZbyivbim+9+MK5wBve3ym2Zms0T33znpPGrP5C3UVPmqDRbnxNIjtK6XIE9H9A2OmFvH6AjfR1nx7qDPERe6mvd0mm9N9KCNAXj5KmP/Oqbm16EvFjLoO+wCUquUkN60U0AZWqV2aqNOyR+/OkXxq/+fH/6TGKLLgPMa5HWdZt3GL/6Y+1HBWr7raAsrPeGDM1buka9ux28N97/kYLlzetRJ5zak3Dq8sHX3zJlAn83bd+TQn6+/f50YuWG7cx7Dhg52bFvESlw79FT5prPQ7qC6bojx95O0T9AP8eOk85674OPjV/9+fGnnxPb9hxiXquPQPLllPdIN+qH13INlG5gv7+X9tGqT4LVCytWWQ6kg8KRM2D/v7TWsfa+L+oq0uCkL/EsPNPeB/ZSv92IZJ2DTtL3woG+r1M9AZBpq94J9C5O/Ti0yU5ybNe/+ghUt3APa/uOY/yMBcav/pz5+RfVd9bXoe6iv+wkB2jHFixbG1J5QadYr0Vd3LnvsKM84F3wTvpa9KfQZ3MC9Qz1TV+L57jhtW7g+2i0YV6JV9nAc9AOOJUdvkMds+YbnuNUn5AGa/8DeTZ59iLH98N9N+/Y66fD9RGo/KIlc150vZPeTu3YFCDtGGNb7xts3IfnWctOH17qSSCs7+ZVR6OM9LO9/k8ksOsHHJDfd05+YFzhj70eBetn29uDYO0yxqnQH/pafQQqByf5wbNi2cdJq/bPqx0jWtjTFEgvWoF+gx1Q/8+2XfuNX5Jx9KSH57XvgcZZYBDPFp7aFwuYddq9boEM79tJurZurP4e2LREmtSpYlzhD2bBFk0fJQumDFfXY7Z+1dwJsmTGaHOGTIOYbVg67BUsQd+yfJZ5bxxYovH2nnUp0oNZKSxhwwyeFZwvWL7WnOUGmK2cMWaAbF89VwZ2b2veG8/B8/BcK4j/hCVyVrAxBJY/2unUsqHKL51/yI/V8yYqbyurR0EwonnvSFCmRGHZsXqOmY7eHZvLpmUzZdyQHipvrfgGpCmW/vjqpKzZ+KpaomcF3oyYHVw4dYRZJijvN3eukTpVyxtXBQfhD6wzvlauveb/jE+hgxlEeIFoj0PIyPTR/VXMOex4bQfLgFBeLRrWML4R2bhtVwo5gpcE4uJperRvKt3bvuC4G7zeWRvP1TLq60CpGW235XoF8uSS4X06OW5Gc+mll0rlciWlp+/ZGsQxRzxzKwgHoZ/rG4D67Q5uB2UM3elrmNU5PF3h8Wrl+IlT5rtDbqaN6qdkC+lx4okcD6tZbR2THf+rw1UEAzqtc+tGKWQzEkAu/vjjT+NMVCiaOzPfbpwlk+/JHFKhVFH1GboKq7jCBR6/uB9A/gbyaD3ue4bOH8QnNL28jr0tJwN4+lpD3RTIm8vPYxqkpbwO6Npabg8SzxH3wAotrdvxjDGDugWMvYhlp/AIeaFuVeObwMSiXFND704tlNeRU91BHib0ai/PPJ1XnSOdB19/y9V7wwvIJ7S78CAHXvQi5AVyo1m8emOq9wxJL7oJ/RGE5dAyW94nO5OH91FhrJzABmADurVR1wGU3drNO1L0c8IBm5VhZRjAO45P6Cn1q1d0XJ6N98b7Tx3Zz+zX+Qb1smrDNlc58lKX//jrL/VuIG+uxxzl57ZbbpJ61ZL7AdBhvoGQcRZ5MD7APhcaN10HDyd4pur8Qf9mx17/1aOBdNYD2bKqczvoywzztduB+t9eQLrrVfNerkg3+meR0A9piRc5Sw2R0LE4X7flNbPvizwfO7i7SruTvsSz8Ew8O9JEqs7Be3WBb4yk74VxSc/2zRzrCYBMj+jXWXkAA7Qd8MxEWxIM1IE2Teo4yjH0L7wyrX085O2gHm0d6xbugXtZ61WgeLm7oOcMj1Dcf0jPdqq/7CQHl12WQb0XyjQccP9hfTpKkQJ5HOUB7zK8XyeznUF/KpL7bXkhntowTTzIBsoEfQC0A05lh++eK1ZQ9SV0WtAWONmhrP0PAB3evH4Nx/fDfUsVfVp512a67RbjW+/EWuYiMTYF2AjT6rmNsXfPDs38yt0Knofn4vmR4sczP/vG+EmrIyD3bl7Gdm6/9aY084q2gzqB/ksg7397PUI/Gx7eduztAeo9bEWB2mU8d9Kw3qofEi6x7OOkVfsXTTuGF6CvsQJJj/WgD2CzcwNhlq69Ntn29oNlo2CN46hqpaUyu4GNUS4GsMymf5dWKZQllE2Pdk0dlRmUJxoIq3LXirVNk9rGN0kgZrreUdsNrTCcGjykxyrkGqcdrBEfDN9b6dWhuW8wWilFhcZz8LzBPdr7NXZOg34s+8RO6FbQKPTt3FKlz4puRJ2M2E5E896pBTIChauVnQZ5CeOX3QD23ekf1YZbVn786RfZvvuAcZZEsMEqlDs6EV6UOBRCpTLFU6QP97cuYwwV7PRvXUKHZYFOabUCJd2oVmWVbigz1AttGAJoILbu3GcaRXFdvWoVVEc/GHgu7qtBeAe3DQuT8sQ/nIMVyFHxQk+Zhi4o4tNn/MPzZLr9Ft9gLsk4DLDpYiBjKyYjtuzYa5yl3JQR74781PmBZbmF8+dWn4PxcPZsUqdqOfUZjeJrvjS4NW4wokYjxh+AvF1jmfz599+zcv58yoYf9aNZ/WqybeVsdaDOhos95I3TAAJ5ojfpASWL5Hc1IKARtoa6eTpvLr+ORlrK67O+/AkmrwD3sG4Q27pxbXMgEQiUAwZfbktKY1Gu4YI6W/7ZZ1K0k1bQnln1JwY5wXbl90o4ehHyUqdKOVMeMdhHqL3UdJjTi26y9kdQJjAGwBAWDPwOmUUbguPPv/6W71MZIgh5s2HLTlNvVK/4nBpMBZMhgD5g7cpljDPEk92r2vNgeKnL11x9tdl/Offff3Lu3Dn12U7Bp3Kbda1j8wZy1VUpB2WRIFxdh0m7BjUqmmUFHfX3P8mhHcLRWdDBrRrXcr3OCdQx1DW3dD+d9wlpWi+574b6YXcoiHe8yFm4RErHoq6gzmiQ58j7YOCZMNSX8z0/kkSqziH2ug5linxqXLuKq7xd979rVZ9Aj7MQxvJ731ghEEhnNZ+OCmT4ABi76bBnAG3xww8EDjeBe1nHBNic1z4BgTAG23Ylj1VqPl9GKpYuHlQO8BvKyj4+9QLS77ZpJByyShTOb5yJfPxZaCF1Uku8tGGaeJGNxrUry6MPJoUnC0bBfE8qOQJoX/Yd9g/hau9/QJbQB3GrU3AkQdsTKrGWuUiMTQEMr3ryE/17yGUwmQB335VZmjeobpxFFowhLr00eH8KuI1f0wqMk7XzVyDs9chpHGdtD6AfOjSv76ofYONp3aR2WDasWPdx0qL9A9G0Y7hx7tx/snztJjWpqIFOchvrabLcEbxflsJIj0oRyqYuJ97/0Ph0YVO6eCG5/15nj2wo0eKFkysngCJEw+DUYVGK1Xd9tqxZjG+ScDPOaOC9EazzCiFv2bCWKeQAjZp10w4ApW01jKISVSlXUinQQOCaks/4zxAhtrrViHDk2Nt+hjEMoJrWqxa0UcBA3xojMBDRvHdqKVowrxlfyg4MVvZ8c5qYgVEGXr5W0IELNgiFEm9Y09v7YcZ34dThZlwxdKrhkW+XxVDAxIn2oofHQVHfvYN11DXw0ERsvVeWTFcH0qb59bc//DYZRKcw0GyzFTwXg0GkA6DT4ubZEMiz1Qpm/a3ewnZvBZRvuVLPmA3pq7sPyEefOG/SCJ2JyS2A+mTflBHvrmdkQbFC+dT93UC9hc7RaXj75Cn59ffAsV+hH7w2JOGSJ9djZnqWrNkoy17e6BhzE/KNMsMRjsHFCspTG5oP+WQIsd6soMHHKhaAPMD1eXI9aqbzgE8v2o1r0M1axpG+fE88rj5r0lJeczyc3VVe4dWvZQj31kZfN9Ax92J0jUW5hgMGhre6dIKRl1b9l2RASn1c2HD1IhwBqlUoZZwhhu9x+fnX34yz0EkvusnaH8FgJ3fOR9RnN1B/dBuyePqogF6LXvnONzixOh7AS8dtQAsw+IBBRMvS/iNv+vI5+AZ4Xuoy+pfw5gVwUMCG8tZ+m+bmjDeYdQ0DSi/lEg7h6jqkB84UuqzgQXrVlVcav4avs7JlvVvJVqjg/tY+ciAgu5AxXa5YbQXPxPSEFzkLl0jpWGvfF+UCg2WwsYjm1pszKnmJJJGqc9j3Ro9XsEFc5kwpPWKdeCj7farMADavDiZv2FPgzky3GWfOXHfttfKgpa+HNgYODcHANbp+/PTzr37jLvCDrx+FMYzm+bIlXO8JMD5F2YYK8uOG6/5nnDmD/EcfRvPdD6cj0pZ7JV7aME08yAauQ5/Si/6xO9mgn2AtP3v/A3YZ1H830DY/57tW9z+8EmuZi8TY9N+zZ5UO0cBOcU+WO4yz4MAZSW9sGQus+h9e+PDGjwWwTbj1p9CXscriZ8ZqGivW9gD6Ieejziub7eR67GHBHmihEus+Tlq0f3i/aNsxAnH+/HlloLfuOQabWv0aFT31XbyQ4i5eZresOC1riwUwAGAAgM0HvB5oOLxm5FW+xiPQtfYOKHBbmnNzxhvlDpfGMxCo3G7pxuxuflultioNNCIffeo/gIRBIftTpeWKzDkCHjdmy5diIxHMuGmvGNzX3kigobnnruCNAhSg1UjrRDTvHQluuP46FVImEHf5GlK92VkgEPLIOhjA9V6WnOH9vDSmkFVchw1PsHM1Ng8JFALDKx99mmzwwY7tN96Q+hnNX379Vb75Nikf0LFy8+61gucjHZpTHwXe2Ah40QFXXHG53HRjcn3G7Kkd6yaN8AKBt6HdAxZemhu3JW8q6tRwnT5zRj79PNmwXLlhO8e66HQ8U7GB2Vh8+fV3jsunNLfclDFVYY68kO+JHKYHLQbhDdv2kqxPlJBazbvIlDmLlUd7JJf2AmvIGyw5hGe8FWxYhk4IyOtLHwZE2e/LanaCYCj4xOYNYw11A+OltVMO0lJeM97ovkzUOnkeSp2ErrSmJRCxKNdwyJAhg6fBoTVP4TWJzldqSY1ehPzoAaVbPfZCvOsmDCThkad58P6sIQ+oIwU2b9ITe2hXs3oc0AIMlnV4Pbzrx585T4ZovNRlGMAb1Kxk5sfYafPk7lzFpUSVxjJ4zDS10gJ9MHt5RovU6LpghKuzYNjxOti1cv+9WTzpBhBqucYbXuQsXCKlY61934d9g3T7ZtXB8LIJYShEos7Zx1gw9F9795OOutF+ZH60sHoGiIS8oY+LMWwooG+IPmIgvvnutLlZbqh6EuMa+5jZDdgavMiZ1VAGp8M0Uotx1YaFQrRlA3X6qedqOsq509GgTU/1f+BjXx/K2o+09j9CbXswYRGqHSDWMheJselff/0jX1jkEuP9YHYKK9aNLWMB3kvbTDBuw4RQLPBSDm7Y24NQ9AO8xHM+6h920gux7OOkVfuXFnYMJzBeGjt9vrTtmaDSCGAbGNyjbYqoHqkhheShQEMx2unZjliT5Y7b1bJXHa/by4Flkl6VVaSBYr3Sd4QKFJaXmWO74gbWGV40ItFYSuR0X3TGvCije+8J7j0QzXunBahbl14SXNnryq7JfNstcrOHCo/wE/+zxLZKS6xLHW+75WY/77hwwaSP7pyhcxWKgQvPRzrSGjSkCA+hQXgUuwcsPFjhyQogt6VLFE6hgyDjTpMAoXI+EZ3FNBqhBAAeqAm9OqhYdLqeYh+GVeu3Ssc+w6VQ+bqSJWcxadqxn+eVRG7YvXHg0arrFfJj574j5jmMl/CUQdlhuSLAoEIvDQXwEH/9zWTPWnuoGxBv8mqVn1DrpJcOTyzKNb2RGr0I+YEcRYp41024L5b5a9yWgEYT9JG0sRCesph89wrKOFzni0Cg34CQO7PGDjIHUQChYWAwLF+npaprxSs3UqsgIjHBFIzU6LpgRFtn2QmlnKJRrsQfa98Xk+BeVq9o/u/qq4xPkSESdc5pvBIukbpPJPnv/H9mmYWqJ+EM8H9Xey/f9EA8tWGxxiobqQF5ag2liHPdToTa9oRrc0nvnPvvnPxu061eueyyyyKuW0PhJjiz3p7kKAJ5sq6iSG/Y24NQ9EM4E2kgln2cC7n9gywOHj1Neg0ZZ+o5GOhnjB4Y8orxz78KvmLB0WJY4/nSxid30JEhJBjowEbLWBjNe8cD9k4KCZ20lBHrJo2IxYZQUFasHtnwbLVvyhhJMCHkdRY9msDgN6hHOzm5/xW1AXXVCqXEui8CGjlsQli8SmN5OUIb1gQKeQPDpC4TpOup3I+rPEInqFD+3KbBGRtb/vJrUjgOxBbXMeydQt1EmvSi02JRrhcjkZpso25KeyI1uNB7GR3ZtkLFv0ZMWbv3OGJ3lq7RTAaMnBwXq1guZOJt0EgiTzzVuUh4chKS3oDchxrdgcQn8MrXHsZYrfjzL+5e8VbnKbB15361GWlqwUQP7oNwwzjseyqQ+OrjxFP7hxV07XoNlZGTZxvfYF+7AjJv0rCQV/RhXIc93TTa/mDF8c27tGqkYvS6gViUtaskLXknaQPivFmXLgUCBW+Pd271TsL42C746JD+8N5+FQYllAMxzXUMO6f7YiMVL0rw40+Db7gSzXvHC/ZKihBF336f5NUXDDR4aPhigdWLLVLx+K668grTkyrUZW54PtKh8bpkMRJYN2mEkXLTq7vVMliATsHGV/eoz8C+KaMGMq43V8GqGYQm+ubE7pCPbatmy713x26poh28CzaFQszNUwc3ymfHtsvciQlmJwxhU7oMGO0XPzFc7CFvdAx6hLFBjG+Apa8PWWa98Vkvhz1+4n35/KskD3DEp9YeHE6hbkC8yat1c55Q62SoHda0LNf0RGr0IuQHcgQiZdCOZ92E+1o9tty8S6IJ+kg6jqfXwaQGZWwNyWfvr6QW9LMQ/3rs4O5yeOty1V+DARGrQjXDJ74ocxavjtqkWGp0XTBSo7O+DyMc1BmXTX2tRLtciX/fF6uvQjF6Wz2YI024dc4+XsEeDE660MtRqUwJ4y7xQ4ZLM5hlFqqeRB/jhx9TF8It3oinNizWWGUDfeoj25Y7yrXbsXDaSL9+CPJYtxOhtj1/+GTut98vPmPsZRkuk2ttutUrCEsWKd2KVZnXX58U3x8rFbUTVDBQ3lbnKTiX7D14VH1ODbsPvi7Z8pSSTI8Uluz5SpvhT6OJvT0IRT+gnx6KftXEso9zIbZ/73/4iTRp30cWrVxvfCNSp2p5mT1hSMjh2wBW6es8h4w7hRALWArLZo0NGgsbBvrFM0YbZyQt2XfoDddZLhiVDhx50zhL4m6LlyMGovfd4y9UVsNUuDgtmYGRy755ox10yHfuP2ycORPNe8cL1g15AAyM2CvAbcD99runYmYEu88Ssw5p+Pa71O8Kfv111ymjEoBBKZRlbui4nTyV5BEKst93t/Ep+mAZr3WTRnhkf/3N9+qzfadz+6aMmpszZjQ39oHxARsZInRVqAc6t1bDRzwBQzTkHA3cluWzpHmD6up7GHSXrtloGg/DxR7yBuFq0CBCX+hQFk89+bjcenPyhnf4jO+AjmWPuHPY/EbjFOoGxJu8PvJA8ow+7u11MIP3taYlVKJdrukJq14MpQwA5AdyBO7MfJvc4iHEnRvxrJswgLNufob6GgmPqXBA6Aa9zw3as08sMfjdQExibIAKkM/RniS9/rprlQFx/uRhMm/SULNsV67bolYARQO7roMcRWJCIFydBb2u8zwUsPeI13SnRblisOZlYiKaBulYYu37vuvTR17lF2Vo3cA02nitc/YxFsoXIV6c9KHboZ2g4olMt92sNg0GoepJGDt0P+xCIZ7asFhjlY0PPv5MjcGd5NrtQD8BfUqNtf8Raj/7S1/903HyLyauvvpKtR+e5q0T73nuh2Oj1g8+Dr1tdcKaDtR9+56IgcD+V9iUGqDMJ724SO1fEi6//va7vLRindm/jvaqUY29PXjvg0/MNLiBDZP1KtdQiGUf50Jq/5CHCIFbp0U32bZrv/GtSLc2TWTi0F5qD5tw+OXX38y9y+6/9265+aaU+5gGNNKjsdm5dr5MGtZH7QZ92y03qQ7Uc8UKKi+5NfMnxWXH4WJg4Yr1su/wG8ZZSqCE5i5Z7dcJQtk9/MB9xlkSWC2hO5gAhqmXfPeGoSYQMHJly/uc3yYPK9ZtNn5NIk+ux/zuC+Uya+GKoJ4xqABrXnnVOAtMNO8dDyBchH2fB5QlOrWBwPKbeUtfNs6CA2WDQfVz1ZuqsqvWpKNqtFMDOmM6jAImFBCj00vDgGvL1mqhDmxAeOK95I3jrvvfNabBFGzavsezt9zOfYdl1/4j6jNCnqR2Y9xQsW7SiHd8bd8hNam2bed+s1GGR6s2dNjBu1s3REKjgDrtBvIceYh3x4FyjeWStTffeU/FbkX5VqrfRt4+ecr4xR/U5+oVk73SYDD02nkJhjXkDfICm1AhTRp42ltn+vFZe98DXAvPe+g8ECzUTbzJK+Lo6nfHvfEML1jjkgci1uWaXrDqxVDKAHID+dFgc2PsmxAJ4lk3wSlEG+kwYaBXv7iBtl23I9jAODUDOHCbr8MN3aHB/WEIdgPvs/HV3aofBbAR9X1heNdYgbdZ6+6DzfeDt64TMGSUfOZpX9klyRvaeBglooFd1+GdvRjckIf9hk803wXex2fPnjN+DV9nvfv+h+YmY6GA+3tJt71ckcdZ7oxMvGnr5BsmJX5zqUuoR2lpkE5LrH1flAt0i5f+yw8//qRixUeKSNY56xhrz8GjPhkKvim85rsffjR1JfYPicd285abbzI3dQRe9SQM1+u2vGacXVjESxsWa6yyAdnd7OvPeNnLBtdgwkfLvt0wae9/eO1nw6axdtOOi6r/qcHkkXXD3Nf2Hvbb/D8Y0GnoJ0YCpMO6v8fxE6c86Xc4RTWuXdkMpYl2e/CYaWoPrFCBfC1csU7ZuTSVy5ZwXDUaDaztAfSDdTwajANHjqm91UIl1n2cC6H9Q/jZ1Ru2Sf3WPc1JQbzTuCE9pG+Xlub7hQNWA2nnWkw+YhLSTkAjvQaecFuWvyhfvPWaWsa+7qWpykuOxA54JfZKGKc6AHZjKDqs3QeN8VNCoOQzBeSBbFmNsyQeyn6fPFv0aeMsiXHT58vISbMdjd4wFPcbPkk9XwMjRJ6cSZ1UDYwT8DSxMn3eMqVYkT4rqABQDths0EtFjOa944GbbrxeihfOb5wlgUayc7+RjoZ6lAV2l8Z7egGNbr1WPWTH3kPqfO2m7WpAopVzOCC0CGJyaabNXRp0UgFAvjD5gIEYjgwZMkjmTEkbxACEd3jWNwDSm3Bs2LpTxbd26+ihUzdlzmLjTHx5+VRIG+VEAjT49k0aka4de5LyHEodHq3wbHUC74781J19dC5Rvm4TH8jz5l36S8mqTdSBzlgs4zlmvOE65QGA8kX6vQ5YIgXKvXD+3Ooz6tDWnftMA0f+3DlNg4AVfIffAK7FJrOoMwC6LpAsxZu8Wt8d4Bn2AY8d1Mnp85e6Xhfrck0v2PWilzKAvEBuID8AA5NSvjY6EuFuQDzrJjgRoJ8C0F5Pmb1YTUAHA78jv3Q7kvGG6+WmjCm9UULBvuIAK0DQTrq9IxwnZi1caZyJr29VULXnqQHeZwihoN8PDgfBHBLSAruuw0AQTgJuug4rk+YvW2u+y4PZ7pHLL7/M+DV8nfXiSyv9+qReQboXrdrgmm57uZYqVjBszyk7cH7Sy6TRzmzbdSConKEerVy/xTi7sLD3fZHnwZyRAPJq2cubTH0ZCSJZ5zDG0pOikGU3hyKA3xPGTTd15fgZC+Tcf+4GzrQGTnoliySXlxc9id8QZm3V+q3GNxcW8dKGxRq7bCxevVHVIzdwTfUXOpmyb9f/9v6H1372/sNvKvm8WMGYRjuMwNA4efYiVz2EsDgz5i83ziIDYnbrfhXGDd+f/lF9dgOOI3pVLpi9aJWyibjVLSuQkYUr1ioblqZK+WflueLJ/eFoY20PYDyHbodBOhh4R7xvODasWPdx0nv7h8m9afOWSrPO/c0+JsZkM8cMlJYNawYcJ3kBbSHsErpc7896t1+IVI2rkT698PlX38rUOUtk1JQ5IR+pMVDGChRuqepNlSc0PJLwHjC2PlaogqrQVqAUET/RHqYBA3bMUGqlqYHBO1+pGtJzyFgzjxq16y1FKtRXnQgr5UsVVcYIK4hVW61CKeMsmbHT5kn+0rWkx+Ck+yLdVRq1V96YXpcrR/Pe8QA6Ic+XKZEi1BTyvUCZ2lKvVXezTFDeeZ+toTrGXsBs6aZX96QY0EKWXtm2yzgLHXggY+LO2gl4oWNfx0kkAKU0bPxMU04hfyhT+2w2Zt0R61ozfMIsGTHpxYBKHgNYNNx4HwAjQt1q5VOlSMPl6bxPmB4f+w8fk5dWrjONvWi03JbX5Xgku/nuyK8+wyaq2VxMPDmBMoXs63fHs8uUKBwx4x6wxzZE58JevlbPCLtHKsrbyeiC8rSuxoEXDrwuUgvKvXD+J40zkTFT55rLXZEupM/O7b7Ov5ZjlFe7XgnqMyjuKzenUDeaeJJX3KNO1XKm9wme0aLLQLUqwKlOwgMOOhQTnm7EulxjibWtxCbEv/3m33FG3uo6Cr2IdlfrAbcyQIdwxoJlSm40tSuXifhKoHjVTXaPKRj/W3QZoFazOIHBS88h49R1AP9XtcKzjvUHS2/1wB6xkO3xUK3lBrDEunrF59RnvGOH3sNkwfK1jqsM8b8wLnTqO8JsW595Oq9UKVcy1foX9QUhtjTwuFy/5bUU8oPz7bsPmLFVYfi19+swUMMgCSshceCz2+AtEHD4aFCjknEmauPMYRNmOuo6nT9dB4wyvboqli4uT+dL1s3ASWcFck4A8KRDH9Xe3w0F1DUM3J3ywalc0S9D/8ypXENtHwGWlBewlO8wX3qwKsD+fwDetb2HTjDr0YWGve+LPEfeowyc8gP1dcnqV5TsRZJI1jn0aetXr2B+j/YVfV89MLfj1A6npadnqBSBniv/rPrs1hagjsF5DGUaa9A2W+PHI6yHXbfjHZzkLhjRbMPSG1bZcKvLAB6lcD7Uurbcs8/4dGNyyEqNtf8BoMPRbwrUNkOftu05xJNH8YUKJsBrVErOMzc9hLamU7+k8ookWbPcaXr1Q2+edBg3OIHQiK0a1ZIWDWsY34iKC16iSmNZvGpD0BU8kAHUv2ad+0nzzgPMd8Y4r3+XVmmqW+3tAdoV9I0DTTagLoTihOlELPs46bn9QxoHj57m5+SLscuCKcPU/p3WMFzhgPEjnKQ09pX9mgwDfBif0xXwcnx5Y7Jx8vSZpCWPWEIS6lGtYik/Q/Nff//tu/erZqwggEGX1cvHjj09iEeLOFrWjoAVp2egYjz2kL93JzaJDRTKBJvDYgDz2t5D6j3QyDkpq9aNa0vDWs/7KljK+NSIUw8FiP+3gk1nEdNe5xFintvvjTyBkkOMRjsYfCDeEuLyWcF3MAjgnkg34tUFAhvdopG3exVE895eyyUcGUGervJ1YKHYNCgX+yQH8hOeCAg7Yd35GZ8RMkCXSaDy1tjTg07M2s07zOU1VqCYg6XdjRtvuE51UHWav/rme1m+drO8/9Encrlv0HP5ZZepTXBXrNsi7XsPlTW+vNOgU9vS1wDbDXhQgjB8YhINEy24L5Y/bd990GhwEn1/z8lR3/tMfnGRamhOvJ8UMgeNwvC+naRYwXwpGht72TmVgR08D57YOu+cZMLKtdf+n3z48eeqIwJjgnV/iE4tG5je2oFAXUWH5sibb6u8hGyv2rBVjr190tcI/qcaRvzF+aKVG6Sj792tHs3YpKVE4fwp3t0qg4HqQCBQhtAD2rvtfWPy67prr1UbOC9Z84ocOnrcJ0tJ74Z3+D+fTCCUBjYKw3sgFNJZX5lBvrERDnR2b1/HfNnLycbcNk1q+xos57AyoQLj3KE3jqtnw4Ck61Nr3zNyPprSGIkVHX8q47K/tyIa57Yv1AmaV/EkrwDlCxmBIQGgLUGd1BuP6zqJ8oEBTa++gjFTbx4LYzxWW+FaTazK1Ws+WNtip/Q7YW1ng7Xd0KHQvajTeO+vfQM/6Gvk895DR2XCzIXKgKnlBH/xu9aLwcqgS/+RynCqZRQT4AO7t1VLvCNJvOomkOm2W5UcIY8B2nAMwjDARjuSwVfHEMsT3nPteg3120SsW9smUrVcqYCdZ8gkvMMgs4iHev3/rlX1Hf0IeOugrHRMYbRFMLodPX5CvSPaWQyojhoTzzj+9n2HJcjDJ86S/iMmm/GoYWgZM6ibXygITTh1OeONNyh9onUKZAn5cTXk0zc+esOXzxN9cgcDrx5MYCVA7crl/NpUTMS36TFEfjiNSYrf1PJjyDq820IFqyDwv1rXAeQvnvHv2XOqnKDrDvrag6HjZ6TIn6G9O8jD2f3DLwLsC2It/48/+0KWrdnkV/6QCbRhnfoONwevbjoL2PMe/4P2EN6YWHaOvIJMYiyBNm7g6KnSfeAYs78GHT2wexs/A64VPC+U9hHoZyJsA8oW5YL06H4T8vmDjz5T/wv9YN9nKpCuCrfNiLWOtfd9kfcoa0yuo+yBHuthwmLU5DnqOmv5h9qvcSJSdQ5A3lHnjhg6EANzyO1vvvSeP5+oVpNgHId+grUdBpi0b9e0rlx5pb/RNpx+HOo7+iPAbawCvDwDZQXHqZ2++0J2rW2BtT6hvPoMmyCjp85VuhT/Az2NvA2W/lDTDLzIJvQ+2gKtP2AsPOvTW9f52gR492KFGOoBjMT6f73WjWi2YV6IJ9nIfPstsmPvYSUXuNZel1FnX3/rhFpx0GPQWNNhErp2RL/Ovn53yjEW+h+Iba7bZsjQlh17ZY9PxpLyzb9tHjpuhsp7q8yBQOUXLZnzck04etvL2BT1EPvsWNtrux6CgfC4r/1COJj2vYfJEV//EOWAUIt68t1rfgQCMcmRBrSRKIcbrrtOnimYV5WpG9AnBfM9KX/88ZdKG4Acvrxpu69+vSLf++Tr73/+lT//+ku9L5wFoXcGjZlmRp7QYDw3PqGn4xgQhDreB/b/CZRX6GNCx2j7FfoMqBdoRyCf1nJA3/mwb/zqtRzisY+TJU7av1BAVA5MUk2ft9T4JsmIPmZQd3nw/qyqDUNeBzswVrzyiqT8dgK2ypGTXlT1AGOulg1r+Po/KcOaht8SkJgA5TIhoZcSfC80qVNFurd7IeDMvJ6h7OG7JhQwCzlucA+12ZMTMNj27thCVR4v4H16dWhmnAUnmveOB1Cpq5Yv5Utzc6U03cA17ZvV89so0wkoaCyldMJpmU0oIM3wPJwyoq/5DDQ6ULhYzZD9qdKSp2R16exTfOisaOCB37dzK1WmTmA5+LjB3ZXnpQaNLTwjsLIAHoGIr4/NZGBwAsiPwT3bqTwMpCCjDeqbNWSCBt7SgeKa24GRaMbogaZnN4ARBKtakJdZnyyploMNHDXFnAUHfTu3lNpVykX83dF5KFoon/lOeCYa1ZxFn5dC5etK74TxKTa+Q+PT5oU6xpmoDiL+B+nPUaSS1GzW2dexSV6dU6ty2Yju5I7Y2lgqaQU6FJ3kQDz8QDZ1jRU00F6MHPEkrygveJ5YdTvq5MwFy/3qZJMOfWTvoaROF67v3bG5+hyMWJdrrLB7vq5av1XFkX20UAW1qs26yTDQunz0wK5mvXErA4Al3RikQp4iTTzrJvRH0Ckf2qejmT7UE9QX1BvUH+ga+30h4+jHWDejtQLjr7V9xCAc5fVIwfJqfwWUh937E/kxcWhvv3eETOt3RNmhDNHGaUMd+hqjB3SRQk/5e4mnBhgXOjSvb/b58Kypc5eo/EUakC8412nARCHywt6mYuCnrwH4jIklL5uVOgHZnDSst18/DG072nit65C3oeSP7o9iQlRjL3/c29qP8Kqz7Iz1Dbh02iG70FnQXTggG6jbGq2jK5ctaXyTknDaRwBjDZZOa5BXut/0UIFy5v9CxyL/0lNfNlS0vrT2fZEfCGmDMtHlg7JCmQGUIcoykkSqzgEYKyE71nANkN1eQ8ZJsecbyt25iiuZtrcB6EMM6NpaGY7jGdRlTLpZxxb2+mQtL+hTjBMw0RtLsKIMOh6gHBPGzZCnnqupdDv6a/B0DIdotWHpEfQTZ40d5FePrHUZdalMzeYp+sPoLwXavB6g3Ib16aTqnQZ1B3XIqW3G82E3yZ87Ms4/6RG018N8MmldpW/VQ+iz4a+WS9RnlAOcRSKFvY3E5Cf2wfIK/m94v06qLK36BukdOXm2KnOU/12PF5VnKjZQfbV1m/33Iihbsogsmj7Sb1+dtARthN1+hfQj353KAfRo31ReqFtVfQ6FeOjjpMf279Vd+9VKDSuYfChaqYFkeqSwp6Ney24BNw6HAX/Dlp2mXBbz5ekdmZI3d7ZCI306BHGLx/iUp7WBsoNK1N8n4KMHdgtomNVAacC7bfmLYx29v+xgdmvpzDGuHlhoFNAZ69q6sVmpnYBBDEtIMFjxSjTvHQ+gowbDO95Rd3CcgAxMHdlPuvjyIdCqDQ0UbKUyxVPcD51mNFypBbPACO+wZsEkyZ3zUeNbZ7R8Th7eR2656UbjW2eQ3jkTE1zLGuC5K+dMUIPeWHd2rZs0auBBel9W77umo47hfRrWfN74JjCQhYVTR0jP9s1UwxgNCj+V2/PkEdADFmyy4vY/eEfoq0gubYNB0hryBuR87EFfg3ibcZYSGOOtG8gCGPi85mk8yavqEHZqIbPGDQraDiCdGCQm9OrgqWxjXa6xAvmJHf2tAx03kFfIA5SzF70IuVk4bYQyhEeLeNZNqLOdWjRQg/tgfRwAmYZsQ8adDGSapAmrmuaAxStYmo2+Dvo8bqANXTV3vJqMivQEKQxh6MfYJw/tQL5mjx/it1GcBl6NdrCZtlOIGq/ASQPxOaE73PQA6syyWWNUqJtg+YNyhIHLPhC3E6rOsoMJXC99SC27LRrUcNXRobaPAPLes0MzZXx3SwfSm976sqGCPEbfF/3aYPUfeYWyQ54E2ug6NUSizmmQVrSBbjIN9HuhDxGs7x8voC4jXKWXfj90JPIKsYpjzUPZ75U+vnbDrTzCIRptWHoEslG0YF55ZfE0T2NM3R9Gn8JN16J+rpo7wfW+uOfCqcNV+J2LHcjiS76+pVt/Bnm2xNfvqV6xdKpWdThh7XsiTOb6ra+JPQxcMFC3WjaqKYe2LFOTWl7bWti20I4v9fVB7PszpjWwX3npN0E3oM3AWAurcEIlXvo4uPZCbf/CAZMUevN0rHYoU6KIGqM4QSN9OgUDnV3rFqglOwiRoCtMzkcfVAb3N7avUp1+r40+FDEGl/teWSyvLJku9WtUVPfS4HObJnVk9/qF8uK4weaycDdQIYf0au+77yKVHn1PpBcb4s0YM0B2rJkbktFDE817xwNQloi9t3/jYlXOenmTfj8MYiADMIw7KVaEMrKDgQQ6LJAZADmCEo+UQQidMnhPvPbyPNm2craSGasc4R0wAA9VPnVZ4//w/1aZRwMDeYXcbl89V3UKI20kCQcYJa2bNCK98GBFJyMUsIELZPmdPetSvDsasaq+AdJL00YqOQkkC5EC9+7Yor6smT9JrYLQDS7KGHoHz7eD90Vn6vVtK/zqKcBnyAg6XNNH93edsAkHLGm0esMi3nQwuYMR0ep1i0F6MM97J+JJXpH/iB99fPdaPz0CMBmBMoEORWgVnUYvxLpcYwXKb8Wc8crTXcsVyhsGYCwVdVpxoQerKG/dvuI+AHkO+dB6EXKj61W0iHfdhP4I4j5Cfl5eMDlFfun2DzIN2faSbgyM0HdBepFOoO811zcgeOQBZwcF9HXwf8d2rFZybp3A03K+c+18WTF7fMT3D9BAftCPQX8G6Uea7fkMJwvIl32CUfN82ZLKQ13/H7j7rjvUEvTUgPsN6tFO3vaVxaRhfZTRRMsv0lavegUl85uWzVR61IuuQ3liIH7stdWqnlnlCnkers6yo/V0oD7kginDlezCq8uLsSKc9hGgPYLTAtIBD26dDvw/8lOnA2m6GEA+Iq+cxjjWPmQ0dWUk6pwVLdMn9q1X5Yn/x30A7pvWbUAkQV6h34+8QJ5Y303rgI1LZ8SFgUyDNGNyYevKF1VIWJ1etDOog03rVZOrbGEWQiEabVh6BWUOg/rBzUtTtKHIk3D7w7gv2l2MNa35i7qD/hj6HvgNsunxlhc8uj8DWw5Czeo80/XUWg6RNtAD9D3rVClrnCG2/AZ591RS2M9QQJx9tA0fHtkiS2aMVuVvlSvIAMLCQN7Qdh7esly1KfFSz6AD0H/R4ydr2nUbd2TbctVmpCbNyId46ONcyO1fKCBUzpzFq81VEth7I5hz9CWJCK5JCIk52KylV8J4Fb9Lg6VFHZs3CDjLptmx95BUa9zBb1kXOsXwjiSEEEIIQZzWxu37qJV3WCXw8AOx92pNCxDHtEHrHmYYLkyspFcHjguNf8+elbfeeV/FEwY3Z7xRyaUXYx2GsNgLpNvA0eochoOJQ3upgT8hhFzMfPXNd2asf+zFgNjqTvsIOvHp519J3VbdzM3KI2lT2LZrv9Rs2lnZLKCr4SwIA6/XCRqSEvZx4hvsg9KgTU9zQ+aOLRrIoB5tg0zCiPw/baXdsEpx5sIAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "felRgJY3r3zj",
        "outputId": "f379e880-7e04-4af1-97c1-af32d42ca23c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1984' 'catcher' 'expectations' 'gatsby' 'great']\n",
            "[0 0 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
        "\n",
        "# Initialize Bag-of-words with the list of book titles\n",
        "vectorizer = CountVectorizer()\n",
        "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
        "\n",
        "# Extract and print the first five features\n",
        "print(vectorizer.get_feature_names_out()[:5])\n",
        "print(bow_encoded_titles.toarray()[0, :5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aEqeUmbsnxj",
        "outputId": "3cf50a48-26df-40e5-f266-49a78ecdc99f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow_encoded_titles.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SatMN1K5sn0L",
        "outputId": "6e717311-638a-42ea-9cf3-7f76d4cba95f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['1984', 'catcher', 'expectations', 'gatsby', 'great', 'hobbit',\n",
              "       'in', 'kill', 'mockingbird', 'rye', 'the', 'to'], dtype=object)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "dCQuYHWPsn20",
        "outputId": "c302cc0a-2eca-46a0-d7f6-629bf91646f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"1984\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"catcher\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expectations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gatsby\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"great\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hobbit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kill\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mockingbird\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rye\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"the\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"to\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-642a9876-22f2-44fb-9abc-6de330652858\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1984</th>\n",
              "      <th>catcher</th>\n",
              "      <th>expectations</th>\n",
              "      <th>gatsby</th>\n",
              "      <th>great</th>\n",
              "      <th>hobbit</th>\n",
              "      <th>in</th>\n",
              "      <th>kill</th>\n",
              "      <th>mockingbird</th>\n",
              "      <th>rye</th>\n",
              "      <th>the</th>\n",
              "      <th>to</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-642a9876-22f2-44fb-9abc-6de330652858')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-642a9876-22f2-44fb-9abc-6de330652858 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-642a9876-22f2-44fb-9abc-6de330652858');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6564553e-88e6-47f2-8752-4b6b0cfb3f47\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6564553e-88e6-47f2-8752-4b6b0cfb3f47')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6564553e-88e6-47f2-8752-4b6b0cfb3f47 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9cfc14b3-597d-405d-ac44-9782227e5e73\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9cfc14b3-597d-405d-ac44-9782227e5e73 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   1984  catcher  expectations  gatsby  great  hobbit  in  kill  mockingbird  \\\n",
              "0     0        0             0       1      1       0   0     0            0   \n",
              "1     0        0             0       0      0       0   0     1            1   \n",
              "2     1        0             0       0      0       0   0     0            0   \n",
              "3     0        1             0       0      0       0   1     0            0   \n",
              "4     0        0             0       0      0       1   0     0            0   \n",
              "5     0        0             1       0      1       0   0     0            0   \n",
              "\n",
              "   rye  the  to  \n",
              "0    0    1   0  \n",
              "1    0    0   1  \n",
              "2    0    0   0  \n",
              "3    1    2   0  \n",
              "4    0    1   0  \n",
              "5    0    0   0  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Create a DataFrame from the encoded titles\n",
        "df = pd.DataFrame(bow_encoded_titles.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the DataFrame\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6RMiD9X34nM",
        "outputId": "86657fb5-3d91-45d8-ae45-3221ee8145f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "fKD5vf5e3qn1",
        "outputId": "70a6bcb4-00f3-4941-e9b5-6c238f86ff8c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'stopwords' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e98ed2cafd2e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0lyPg0x-MVk"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QsvXVlf-CAv"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer and stemmer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "k6RiKmt4-W5l",
        "outputId": "881a9bfb-4fbc-48d8-bfd5-ff9854db96f6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'shakespeare' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-cdf191c2a8d1>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprocessed_shakespeare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshakespeare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_shakespeare\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shakespeare' is not defined"
          ]
        }
      ],
      "source": [
        "# Create a list of stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Initialize the tokenizer and stemmer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Complete the function to preprocess sentences\n",
        "def preprocess_sentences(sentences):\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        tokens = ____(sentence)\n",
        "        tokens = [token for token in tokens if token not in ____]\n",
        "        tokens = [____.____(token) for token in tokens]\n",
        "        processed_sentences.append(' '.join(tokens))\n",
        "    return processed_sentences\n",
        "\n",
        "processed_shakespeare = preprocess_sentences(shakespeare)\n",
        "print(processed_shakespeare[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m83Ufcrosn50"
      },
      "outputs": [],
      "source": [
        "# Define your Dataset class\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpvFjnOL4Yxt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znThpswk5t_V",
        "outputId": "81e68ca0-c737-4625-b42f-8f2dd56ea3be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'This': 0,\n",
              " 'book': 1,\n",
              " 'was': 12,\n",
              " 'fantastic': 3,\n",
              " 'I': 4,\n",
              " 'really': 5,\n",
              " 'love': 6,\n",
              " 'science': 7,\n",
              " 'fiction': 8,\n",
              " 'but': 9,\n",
              " 'the': 10,\n",
              " 'protagonist': 11,\n",
              " 'rude': 13,\n",
              " 'sometimes': 14}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qaz=[\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
        "{word: i for i, word in enumerate(qaz)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iiOKvu34WcF",
        "outputId": "50305489-0c64-4618-c72b-0389196005d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word_to_idx ===> {'This': 0, 'book': 1, 'was': 12, 'fantastic': 3, 'I': 4, 'really': 5, 'love': 6, 'science': 7, 'fiction': 8, 'but': 9, 'the': 10, 'protagonist': 11, 'rude': 13, 'sometimes': 14} \n",
            " [0, 1, 12, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "tensor([[ 5.4715e-01,  9.0476e-01, -3.7999e-01,  1.6081e+00,  8.4616e-01,\n",
            "          1.3017e-01,  3.6683e-01, -4.3003e-01, -5.5476e-01,  1.3151e+00],\n",
            "        [-5.2081e-02,  6.7382e-01, -1.0992e+00,  1.2877e+00, -1.4509e+00,\n",
            "         -3.6348e-03, -7.6918e-01,  1.5176e+00,  1.0031e-01,  9.0068e-02],\n",
            "        [-7.0125e-01, -1.2098e-01, -9.8028e-02, -5.5523e-01,  1.0597e+00,\n",
            "          2.1145e+00, -7.5703e-01, -4.1483e-01,  2.2740e-01, -1.6133e+00],\n",
            "        [ 1.1986e+00, -3.4982e-01,  1.6487e+00,  5.7771e-01, -4.9913e-01,\n",
            "          2.0131e+00,  1.8081e+00,  1.2187e+00, -7.2654e-02,  1.0613e+00],\n",
            "        [-1.4804e+00,  5.0358e-01, -1.0904e+00, -9.3451e-01,  9.1722e-01,\n",
            "          9.8841e-01,  6.8718e-01, -5.9597e-01,  1.7992e-01, -2.2600e-01],\n",
            "        [ 4.3228e-01, -1.7226e+00, -2.9078e-01,  5.2588e-01,  1.3805e-01,\n",
            "         -1.8331e-01,  1.1360e+00,  1.1327e+00, -9.6854e-01,  1.0296e+00],\n",
            "        [ 3.2336e-02, -4.0429e-01, -1.0471e+00,  1.8719e-01,  5.8638e-01,\n",
            "          3.4017e-01,  7.8159e-01,  1.2911e+00,  5.8820e-01, -8.8937e-01],\n",
            "        [-2.8688e-01, -1.0094e+00,  1.3352e+00, -2.7095e-01,  4.2177e-01,\n",
            "          1.8541e+00,  4.8599e-01,  9.3485e-01, -8.5431e-01, -7.3165e-01],\n",
            "        [-5.2008e-01, -5.5482e-01,  7.8480e-01,  4.1386e-01,  8.3669e-01,\n",
            "          1.3538e-01, -2.9770e-01,  1.8939e+00, -4.6496e-01, -8.6353e-01],\n",
            "        [ 5.7105e-01, -1.7110e+00, -7.3287e-01,  1.9581e+00, -5.4695e-01,\n",
            "         -5.0165e-02,  4.6378e-01,  9.8054e-01,  1.0617e+00,  1.5666e+00],\n",
            "        [ 5.1068e-01, -1.4967e+00,  7.7157e-01, -1.1626e+00, -1.1852e+00,\n",
            "         -6.2616e-01, -7.8406e-01, -9.4925e-01,  6.3415e-01,  3.9790e-01],\n",
            "        [ 1.8420e-01,  9.5402e-01,  7.7466e-02, -1.4099e-01, -3.5860e-01,\n",
            "          4.2342e-01, -5.9907e-01,  1.6630e+00, -3.1322e-01,  3.6933e-01],\n",
            "        [-7.0125e-01, -1.2098e-01, -9.8028e-02, -5.5523e-01,  1.0597e+00,\n",
            "          2.1145e+00, -7.5703e-01, -4.1483e-01,  2.2740e-01, -1.6133e+00],\n",
            "        [-2.2897e+00,  3.4069e-01, -5.8262e-01,  9.9045e-01,  2.7705e+00,\n",
            "          6.8371e-01,  1.5472e-01, -1.2659e+00,  5.6332e-01, -5.4818e-01],\n",
            "        [ 2.0609e-01,  6.8719e-01,  7.5895e-01,  1.5631e+00,  1.5947e+00,\n",
            "         -7.0578e-02, -2.4257e-03,  4.3381e-01,  9.6314e-01, -3.9954e-01]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Map a unique index to each word\n",
        "words = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
        "word_to_idx = {word: i for i, word in enumerate(words)}\n",
        "\n",
        "\n",
        "# Convert word_to_idx to a tensor\n",
        "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
        "\n",
        "\n",
        "print(\"word_to_idx ===>\",word_to_idx,'\\n',[word_to_idx[w] for w in words])\n",
        "# Initialize embedding layer with ten dimensions\n",
        "embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
        "\n",
        "# Pass the tensor to the embedding layer\n",
        "output = embedding(inputs)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYTMB0tU89R1"
      },
      "outputs": [],
      "source": [
        "## So read the verbatim below regarding embeddings.. Initially a random table is created with the index in this table pointing to index from the list of words\n",
        "## Then as the model gets trained these values are also updated accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKlUyxCb8_ft"
      },
      "source": [
        "The embedding table is essentially the embedding object that you created with the line embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10). This object is a lookup table that stores the embeddings for each word in your vocabulary. The embeddings are initially random, but theyre meant to be learned during training.\n",
        "\n",
        "When you call embedding(inputs), the embedding object looks at each index in inputs and returns the corresponding embedding from the lookup table. The mapping from words to indices is determined by the word_to_idx dictionary that you created earlier.\n",
        "\n",
        "For example, in your word_to_idx dictionary, the word This is mapped to the index 0, the word book is mapped to the index 1, and so on. So when you pass the index 0 to the embedding object, it returns the embedding for the word This. Similarly, when you pass the index 1, it returns the embedding for the word book.\n",
        "\n",
        "So, to answer your question, the embedding object doesnt directly know that 0 maps to This or 1 maps to book. It only knows that it has an embedding for index 0, an embedding for index 1, etc. The mapping from indices to words is handled by the word_to_idx dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyPwaPId_v1e"
      },
      "outputs": [],
      "source": [
        "# Create a list of stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Initialize the tokenizer and stemmer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Complete the function to preprocess sentences\n",
        "def preprocess_sentences(sentences):\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        tokens = ____(sentence)\n",
        "        tokens = [token for token in tokens if token not in ____]\n",
        "        tokens = [____.____(token) for token in tokens]\n",
        "        processed_sentences.append(' '.join(tokens))\n",
        "    return processed_sentences\n",
        "\n",
        "processed_shakespeare = preprocess_sentences(shakespeare)\n",
        "print(processed_shakespeare[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4tLaBnP9SC6"
      },
      "outputs": [],
      "source": [
        "# Define your Dataset class\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Complete the encoding function\n",
        "def encode_sentences(sentences):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    return X.toarray(), vectorizer\n",
        "\n",
        "# Complete the text processing pipeline\n",
        "def text_processing_pipeline(sentences):\n",
        "    processed_sentences = preprocess_sentences(sentences)\n",
        "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
        "    dataset = ShakespeareDataset(encoded_sentences)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "    return dataloader, vectorizer\n",
        "\n",
        "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
        "\n",
        "# Print the vectorizer's feature names and the first 10 components of the first item\n",
        "print(vectorizer.get_feature_names_out()[:10])\n",
        "print(next(iter(dataloader))[0, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsSkfSolFAjU",
        "outputId": "a6228c63-81ab-46f9-cc53-b4ed8e7685a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['m', 'V', 't', 'y', 'h', 'o', 's', 'u', 'a', 'i', '.', 'v', ' ', 'r', 'e', 'j']\n",
            "{'m': 0, 'V': 1, 't': 2, 'y': 3, 'h': 4, 'o': 5, 's': 6, 'u': 7, 'a': 8, 'i': 9, '.': 10, 'v': 11, ' ': 12, 'r': 13, 'e': 14, 'j': 15}\n",
            "{0: 'm', 1: 'V', 2: 't', 3: 'y', 4: 'h', 5: 'o', 6: 's', 7: 'u', 8: 'a', 9: 'i', 10: '.', 11: 'v', 12: ' ', 13: 'r', 14: 'e', 15: 'j'}\n"
          ]
        }
      ],
      "source": [
        "data = \"hari is sarvothama . Vayu is jeevothama\"\n",
        "chars=list(set(data))\n",
        "print(chars)\n",
        "char_to_idx = {char: i for i, char in enumerate(chars)}\n",
        "print(char_to_idx)\n",
        "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
        "print(idx_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBMUGunyIb6n",
        "outputId": "4832fb24-8063-46a1-ab2e-f2dc0ac4170f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00lao9GH0U3"
      },
      "outputs": [],
      "source": [
        "! pip install -q torchview\n",
        "! pip install -q -U graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BBPSfQFYH5OF",
        "outputId": "5419c510-d1ff-4571-be02-9273c3c0ebae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'svg'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchview import draw_graph\n",
        "from torch import nn\n",
        "import torch\n",
        "import graphviz\n",
        "\n",
        "# when running on VSCode run the below command\n",
        "# svg format on vscode does not give desired result\n",
        "graphviz.set_jupyter_format('png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NUuZPt2E8my",
        "outputId": "1345abae-f8bc-4ee5-b47e-448d06956b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNmodel(\n",
            "  (rnn): RNN(16, 16, batch_first=True)\n",
            "  (fc): Linear(in_features=16, out_features=16, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Include an RNN layer and linear layer in RNNmodel class\n",
        "import torch\n",
        "from torch import nn\n",
        "class RNNmodel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNmodel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "      h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "      out, _ = self.rnn(x, h0)\n",
        "      out = self.fc(out[:, -1, :])\n",
        "      return out\n",
        "\n",
        "# Instantiate the RNN model\n",
        "model = RNNmodel(len(chars), 16,  len(chars))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fYsWjANHJBj"
      },
      "outputs": [],
      "source": [
        "model_graph_1 = draw_graph(\n",
        "    RNNmodel(len(chars), 16,  len(chars)), input_size=(len(chars), 16,  len(chars)),\n",
        "    graph_name='RecursiveNet',\n",
        "    roll=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "BTc3bFXBIRFv",
        "outputId": "59638894-eb08-4a84-9092-bf6f2e048fce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7cc158f9a0b0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_graph_1.visual_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1KrP5J_GjjN",
        "outputId": "a206efe6-6a04-4971-921a-4b7ab8475f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4, 8, 13, 9, 12, 9, 6, 12, 6, 8, 13, 11, 5, 2, 4, 8, 0, 8, 12, 10, 12, 1, 8, 3, 7, 12, 9, 6, 12, 15, 14, 14, 11, 5, 2, 4, 8, 0]\n",
            "[8, 13, 9, 12, 9, 6, 12, 6, 8, 13, 11, 5, 2, 4, 8, 0, 8, 12, 10, 12, 1, 8, 3, 7, 12, 9, 6, 12, 15, 14, 14, 11, 5, 2, 4, 8, 0, 8]\n",
            "torch.Size([38, 1, 16]) tensor([[[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "inputs=[char_to_idx[ch] for ch in data[:-1]]\n",
        "targets=[char_to_idx[ch] for ch in data[1:]]\n",
        "\n",
        "print(inputs)\n",
        "print(targets)\n",
        "inputs=torch.tensor(inputs,dtype=torch.long).view(-1,1)\n",
        "inputs=nn.functional.one_hot(inputs,num_classes=len(chars)).float()\n",
        "targets = torch.LongTensor(targets)\n",
        "print(inputs.shape,inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifG0Xk27Hcb7",
        "outputId": "e8dad985-c418-4b06-9808-e5adfb75247d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 2.384908437728882\n",
            "Epoch 20/100, Loss: 1.9949780702590942\n",
            "Epoch 30/100, Loss: 1.5139051675796509\n",
            "Epoch 40/100, Loss: 1.090847134590149\n",
            "Epoch 50/100, Loss: 0.847396969795227\n",
            "Epoch 60/100, Loss: 0.7351223826408386\n",
            "Epoch 70/100, Loss: 0.6884651184082031\n",
            "Epoch 80/100, Loss: 0.6680712699890137\n",
            "Epoch 90/100, Loss: 0.6577922105789185\n",
            "Epoch 100/100, Loss: 0.6518296003341675\n",
            "Epoch 110/100, Loss: 0.6479564905166626\n",
            "Epoch 120/100, Loss: 0.6452126502990723\n",
            "Epoch 130/100, Loss: 0.6431455612182617\n",
            "Epoch 140/100, Loss: 0.6415226459503174\n",
            "Epoch 150/100, Loss: 0.6402112245559692\n",
            "Epoch 160/100, Loss: 0.6391292214393616\n",
            "Epoch 170/100, Loss: 0.6382220387458801\n",
            "Epoch 180/100, Loss: 0.637451708316803\n",
            "Epoch 190/100, Loss: 0.6367904543876648\n",
            "Epoch 200/100, Loss: 0.6362177729606628\n",
            "Epoch 210/100, Loss: 0.6357178688049316\n",
            "Epoch 220/100, Loss: 0.6352781057357788\n",
            "Epoch 230/100, Loss: 0.6348893642425537\n",
            "Epoch 240/100, Loss: 0.634543240070343\n",
            "Epoch 250/100, Loss: 0.6342338919639587\n",
            "Epoch 260/100, Loss: 0.6339557766914368\n",
            "Epoch 270/100, Loss: 0.6337049007415771\n",
            "Epoch 280/100, Loss: 0.6334776878356934\n",
            "Epoch 290/100, Loss: 0.6332709789276123\n",
            "Epoch 300/100, Loss: 0.6330823302268982\n",
            "Epoch 310/100, Loss: 0.632909893989563\n",
            "Epoch 320/100, Loss: 0.6327515244483948\n",
            "Epoch 330/100, Loss: 0.6326057314872742\n",
            "Epoch 340/100, Loss: 0.6324712634086609\n",
            "Epoch 350/100, Loss: 0.6323469281196594\n",
            "Epoch 360/100, Loss: 0.6322315335273743\n",
            "Epoch 370/100, Loss: 0.6321244239807129\n",
            "Epoch 380/100, Loss: 0.6320247650146484\n",
            "Epoch 390/100, Loss: 0.6319317817687988\n",
            "Epoch 400/100, Loss: 0.6318448781967163\n",
            "Epoch 410/100, Loss: 0.6317636370658875\n",
            "Epoch 420/100, Loss: 0.631687343120575\n",
            "Epoch 430/100, Loss: 0.6316157579421997\n",
            "Epoch 440/100, Loss: 0.6315484046936035\n",
            "Epoch 450/100, Loss: 0.6314851641654968\n",
            "Epoch 460/100, Loss: 0.6314254403114319\n",
            "Epoch 470/100, Loss: 0.6313689947128296\n",
            "Epoch 480/100, Loss: 0.6313157081604004\n",
            "Epoch 490/100, Loss: 0.6312652826309204\n",
            "Epoch 500/100, Loss: 0.6312174797058105\n",
            "Epoch 510/100, Loss: 0.6311721205711365\n",
            "Epoch 520/100, Loss: 0.6311290860176086\n",
            "Epoch 530/100, Loss: 0.6310882568359375\n",
            "Epoch 540/100, Loss: 0.6310493350028992\n",
            "Epoch 550/100, Loss: 0.6310123205184937\n",
            "Epoch 560/100, Loss: 0.6309769749641418\n",
            "Epoch 570/100, Loss: 0.6309434175491333\n",
            "Epoch 580/100, Loss: 0.6309112310409546\n",
            "Epoch 590/100, Loss: 0.6308805346488953\n",
            "Epoch 600/100, Loss: 0.6308512091636658\n",
            "Epoch 610/100, Loss: 0.6308228969573975\n",
            "Epoch 620/100, Loss: 0.6307960748672485\n",
            "Epoch 630/100, Loss: 0.6307704448699951\n",
            "Epoch 640/100, Loss: 0.6307457089424133\n",
            "Epoch 650/100, Loss: 0.6307220458984375\n",
            "Epoch 660/100, Loss: 0.6306992173194885\n",
            "Epoch 670/100, Loss: 0.6306774020195007\n",
            "Epoch 680/100, Loss: 0.6306564211845398\n",
            "Epoch 690/100, Loss: 0.6306362152099609\n",
            "Epoch 700/100, Loss: 0.6306167244911194\n",
            "Epoch 710/100, Loss: 0.6305980086326599\n",
            "Epoch 720/100, Loss: 0.6305800080299377\n",
            "Epoch 730/100, Loss: 0.6305626034736633\n",
            "Epoch 740/100, Loss: 0.6305457949638367\n",
            "Epoch 750/100, Loss: 0.6305297017097473\n",
            "Epoch 760/100, Loss: 0.6305141448974609\n",
            "Epoch 770/100, Loss: 0.630499005317688\n",
            "Epoch 780/100, Loss: 0.630484402179718\n",
            "Epoch 790/100, Loss: 0.6304703950881958\n",
            "Epoch 800/100, Loss: 0.630456805229187\n",
            "Epoch 810/100, Loss: 0.6304436326026917\n",
            "Epoch 820/100, Loss: 0.6304309368133545\n",
            "Epoch 830/100, Loss: 0.630418598651886\n",
            "Epoch 840/100, Loss: 0.6304066181182861\n",
            "Epoch 850/100, Loss: 0.6303950548171997\n",
            "Epoch 860/100, Loss: 0.6303837895393372\n",
            "Epoch 870/100, Loss: 0.6303730607032776\n",
            "Epoch 880/100, Loss: 0.630362331867218\n",
            "Epoch 890/100, Loss: 0.6303521990776062\n",
            "Epoch 900/100, Loss: 0.6303423047065735\n",
            "Epoch 910/100, Loss: 0.6303326487541199\n",
            "Epoch 920/100, Loss: 0.6303232312202454\n",
            "Epoch 930/100, Loss: 0.6303141713142395\n",
            "Epoch 940/100, Loss: 0.630305290222168\n",
            "Epoch 950/100, Loss: 0.6302966475486755\n",
            "Epoch 960/100, Loss: 0.630288302898407\n",
            "Epoch 970/100, Loss: 0.6302802562713623\n",
            "Epoch 980/100, Loss: 0.630272388458252\n",
            "Epoch 990/100, Loss: 0.6302646994590759\n",
            "Epoch 1000/100, Loss: 0.6302571296691895\n",
            "Epoch 1010/100, Loss: 0.6302499771118164\n",
            "Epoch 1020/100, Loss: 0.6302428245544434\n",
            "Epoch 1030/100, Loss: 0.6302358508110046\n",
            "Epoch 1040/100, Loss: 0.630229115486145\n",
            "Epoch 1050/100, Loss: 0.6302225589752197\n",
            "Epoch 1060/100, Loss: 0.630216121673584\n",
            "Epoch 1070/100, Loss: 0.6302099227905273\n",
            "Epoch 1080/100, Loss: 0.6302037239074707\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Instantiate the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1080):\n",
        "    model.train()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6t0DFJFHmjq",
        "outputId": "27b1ed11-a238-4172-f6e2-9140b7691599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Input: 'r', Predicted Output: 'v'\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_input = char_to_idx['r']\n",
        "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
        "predicted_output = model(test_input)\n",
        "predicted_char_idx = torch.argmax(predicted_output, 1).item()\n",
        "print(f\"Test Input: 'r', Predicted Output: '{idx_to_char[predicted_char_idx]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw6NebjeHRq5",
        "outputId": "b121a755-ad49-4b99-b2a8-288ca751c90d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2461,  0.6200,  0.0456, -2.0940, -2.2405, -0.4771,  0.7846, -3.0683,\n",
              "         -2.6158,  9.4467,  0.2153,  9.4467,  0.2900, -2.4124,  0.4942,  0.3130]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmCgWG79ewJ8",
        "outputId": "1affdd36-99dd-44c8-96e7-07f38f1eb6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "600901/600901 [==============================] - 1s 2us/step\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import io\n",
        "path = keras.utils.get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - 40, 3):\n",
        "    sentences.append(text[i: i + 40])\n",
        "    next_chars.append(text[i + 40])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ES03_2oe6VD"
      },
      "outputs": [],
      "source": [
        "\n",
        "x = np.zeros((len(sentences), 40, len(chars)))\n",
        "y = np.zeros((len(sentences), len(chars)))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY1EoSeheyN0",
        "outputId": "45e3ef37-94b5-414f-94c8-cdb176457cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentenecs==> preface\n",
            "\n",
            "\n",
            "supposing that truth is a woma h 40\n",
            "n\n",
            "sentenecs==> face\n",
            "\n",
            "\n",
            "supposing that truth is a woman-- h 40\n",
            "w\n",
            "sentenecs==> e\n",
            "\n",
            "\n",
            "supposing that truth is a woman--wha h 40\n",
            "t\n",
            "sentenecs==> \n",
            "supposing that truth is a woman--what t h 40\n",
            "h\n",
            "sentenecs==> pposing that truth is a woman--what then h 40\n",
            "?\n",
            "sentenecs==> sing that truth is a woman--what then? i h 40\n",
            "s\n",
            "sentenecs==> g that truth is a woman--what then? is t h 40\n",
            "h\n",
            "sentenecs==> hat truth is a woman--what then? is ther h 40\n",
            "e\n",
            "sentenecs==>  truth is a woman--what then? is there n h 40\n",
            "o\n",
            "sentenecs==> uth is a woman--what then? is there not  h 40\n",
            "g\n"
          ]
        }
      ],
      "source": [
        "for i,a in enumerate(sentences[0:10]):\n",
        "  print(\"sentenecs==>\",a,\"h\",len(a)),\n",
        "  print(next_chars[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "CLLsK6R4fJj7",
        "outputId": "2eada89c-8f4f-4701-cbcc-667aff3e6a2d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'preface\\n\\n\\nsupposing that truth is a woman--what then? is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to truth, have been unskilled and unseemly methods for\\nwinning a woman? certainly she has never allowed herself to be won; and\\nat present every kind of dogma stands with sad and discouraged mien--if,\\nindeed, it s'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzmjyBdOgoJD",
        "outputId": "b491b808-555a-446a-929b-37677da2d4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "sentenecs==> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]] 40 (40, 57)\n",
            "y==> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "for i,a in enumerate(x[0:10]):\n",
        "  print(\"sentenecs==>\",a,len(a),a.shape),\n",
        "  print(\"y==>\",y[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N2pArxSoqKN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import urllib.request\n",
        "import io\n",
        "\n",
        "# Download the text file\n",
        "url = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
        "response = urllib.request.urlopen(url)\n",
        "data = response.read().decode('utf-8').lower()[0:10000]\n",
        "\n",
        "chars = sorted(list(set(data)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(data) - 40, 3):\n",
        "    sentences.append(data[i: i + 40])\n",
        "    next_chars.append(data[i + 40])\n",
        "\n",
        "x = np.zeros((len(sentences), 40), dtype=np.int64)\n",
        "y = np.zeros((len(sentences)), dtype=np.int64)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t] = char_indices[char]\n",
        "    y[i] = char_indices[next_chars[i]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA3hbXFPDWH5"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "model = RNN(len(chars), 40, len(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6xU40ZtHvi9"
      },
      "outputs": [],
      "source": [
        "# Instantiate the loss function\n",
        "criterion = nn.NLLLoss()\n",
        "# Instantiate the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyluqQP0IDEN"
      },
      "outputs": [],
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "x = torch.from_numpy(x)[0:10000]\n",
        "y = torch.from_numpy(y)[0:10000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkjXJp04HdOO",
        "outputId": "6ae27402-526b-44c2-bf34-60dc86b7c685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss: 121.1191752904869\n",
            "Epoch 20/100, Loss: 121.86349084463464\n",
            "Epoch 30/100, Loss: 122.89712682057576\n",
            "Epoch 40/100, Loss: 124.77204613972859\n",
            "Epoch 50/100, Loss: 125.16993550220168\n",
            "CPU times: user 2h 12min 2s, sys: 4.21 s, total: 2h 12min 6s\n",
            "Wall time: 33min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def clip_grads(model, clip):\n",
        "    parameters = [p for p in model.parameters() if p.grad is not None]\n",
        "    for p in parameters:\n",
        "        p.grad.data.clamp_(-clip, clip)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):  # Number of epochs\n",
        "    total_loss = 0\n",
        "    for i in range(x.shape[0]):\n",
        "        sentence = x[i]\n",
        "        target = y[i]\n",
        "        hidden = model.initHidden()\n",
        "        model.zero_grad()\n",
        "        loss = 0\n",
        "        for j in range(sentence.size()[0]):\n",
        "            input_char = torch.zeros(1, len(chars))  # Create a one-hot vector for each character\n",
        "            input_char[0][sentence[j]] = 1.0  # Set the index of the current character to 1\n",
        "            output, hidden = model(input_char, hidden)\n",
        "            l = criterion(output, torch.tensor([target]))\n",
        "            loss += l\n",
        "        loss.backward()\n",
        "        clip_grads(model, 5)  # Clip gradients to prevent them from exploding\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print(f'Epoch {epoch+1}/100, Loss: {total_loss/x.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73NWoGrJZ4DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81c4eb8-8fbf-44ea-ea1a-731d9cd9294e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pposing that truth is a woman--what theneeeeeeeeee\n"
          ]
        }
      ],
      "source": [
        "def generate_text(seed_string, generate_length=500):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    sentence = seed_string.lower()\n",
        "    generated = '' + sentence\n",
        "\n",
        "    for i in range(generate_length):\n",
        "        x_pred = torch.zeros(1, 1, len(chars))\n",
        "        for t, char in enumerate(sentence):\n",
        "            if char in char_indices:\n",
        "                x_pred[0, 0, char_indices[char]] = 1.\n",
        "\n",
        "        hidden = model.initHidden()\n",
        "        output, hidden = model(x_pred[0], hidden)\n",
        "        output_dist = output.data.view(-1).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0].item()\n",
        "        predicted_char = indices_char[top_i]\n",
        "        generated += predicted_char\n",
        "        sentence = sentence[1:] + predicted_char\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Generate text with a seed string\n",
        "print(generate_text(\"pposing that truth is a woman--what then\", 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrnaTE91IW3Q",
        "outputId": "06063df3-bf02-4246-af9d-718ff60c8170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: nan\n",
            "Epoch 20/100, Loss: nan\n",
            "Epoch 30/100, Loss: nan\n",
            "Epoch 40/100, Loss: nan\n",
            "Epoch 50/100, Loss: nan\n",
            "Epoch 60/100, Loss: nan\n",
            "Epoch 70/100, Loss: nan\n",
            "Epoch 80/100, Loss: nan\n",
            "Epoch 90/100, Loss: nan\n",
            "Epoch 100/100, Loss: nan\n"
          ]
        }
      ],
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "x = torch.from_numpy(x)\n",
        "y = torch.from_numpy(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):  # Number of epochs\n",
        "    total_loss = 0\n",
        "    for i in range(x.shape[0]):\n",
        "        sentence = x[i]\n",
        "        target = y[i]\n",
        "        hidden = model.initHidden()\n",
        "        model.zero_grad()\n",
        "        loss = 0\n",
        "        for j in range(sentence.size()[0]):\n",
        "            input_char = torch.zeros(1, len(chars))  # Create a one-hot vector for each character\n",
        "            input_char[0][sentence[j]] = 1.0  # Set the index of the current character to 1\n",
        "            output, hidden = model(input_char, hidden)\n",
        "            l = criterion(output, torch.tensor([target]))\n",
        "            loss += l\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/100, Loss: {total_loss/x.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rijO8y2I3Wv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Ae_LtzExFhNI",
        "outputId": "f4e2da07-4123-49f5-e551-030362f538ad"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sequences' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-36ad6ce70e54>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Prepare the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sequences' is not defined"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# Instantiate the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Prepare the data\n",
        "inputs = [torch.tensor(sequence[:-1], dtype=torch.long) for sequence in sequences]\n",
        "targets = [torch.tensor(sequence[1:], dtype=torch.long) for sequence in sequences]\n",
        "\n",
        "# Pad the sequences\n",
        "inputs = pad_sequence(inputs, batch_first=True)\n",
        "targets = pad_sequence(targets, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8DUgX_5Fhjg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4snmW_gDiQf"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1):\n",
        "    for i in range(x.shape[0]):\n",
        "        sentence = torch.tensor(x[i], dtype=torch.long)  # Change dtype to long for indices\n",
        "        hidden = model.initHidden()\n",
        "        model.zero_grad()\n",
        "        loss = 0\n",
        "        for j in range(sentence.size()[0]):\n",
        "            input_char = torch.zeros(1, len(chars))  # Create a one-hot vector for each character\n",
        "            input_char[0][sentence[j]] = 1.0  # Set the index of the current character to 1\n",
        "            output, hidden = model(input_char, hidden)\n",
        "            target_index = y[i]  # Use the index of the target character\n",
        "            l = criterion(output, torch.tensor([target_index]))\n",
        "            loss += l\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "WBFuhwgtppo9",
        "outputId": "46366c25-145c-4b0d-d61b-7ef0ee3b8185"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-fcabb3d51355>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutput_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove the division by diversity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtop_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpredicted_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
          ]
        }
      ],
      "source": [
        "# Text generation\n",
        "start_index = random.randint(0, len(data) - 40 - 1)\n",
        "generated = ''\n",
        "sentence = data[start_index: start_index + 40]\n",
        "generated += sentence\n",
        "for i in range(400):\n",
        "    x_pred = torch.zeros(1, 1, len(chars))\n",
        "    for t, char in enumerate(sentence):\n",
        "        x_pred[0, 0, char_indices[char]] = 1.\n",
        "    hidden = model.initHidden()\n",
        "    output, hidden = model(x_pred[0], hidden)\n",
        "    output_dist = output.data.view(-1).exp()  # Remove the division by diversity\n",
        "    top_i = torch.multinomial(output_dist, 1)[0]\n",
        "    predicted_char = indices_char[top_i]\n",
        "    generated += predicted_char\n",
        "    sentence = sentence[1:] + predicted_char\n",
        "\n",
        "with open('example.txt', 'w') as f:\n",
        "    f.write(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obJj08OC5EmO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import urllib.request\n",
        "import io\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn import Embedding\n",
        "\n",
        "# Download the text file\n",
        "url = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
        "response = urllib.request.urlopen(url)\n",
        "data = response.read().decode('utf-8').lower()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = data.split()\n",
        "\n",
        "# Create a vocabulary of unique words and their indices\n",
        "vocab = {word: i for i, word in enumerate(Counter(words))}\n",
        "\n",
        "# Convert the sentences into sequences of word indices\n",
        "sequences = [[vocab[word] for word in sentence.split()] for sentence in sentences]\n",
        "\n",
        "# Define the model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = Embedding(input_size, hidden_size)\n",
        "        self.i2h = nn.Linear(hidden_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(hidden_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        combined = torch.cat((embedded, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "model = RNN(len(vocab), 128, len(vocab))\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training\n",
        "for epoch in range(1):\n",
        "    for i in range(len(sequences)):\n",
        "        sentence = torch.tensor(sequences[i], dtype=torch.long)\n",
        "        target = torch.tensor(sequences[i][1:] + [vocab['<eos>']], dtype=torch.long)\n",
        "        hidden = model.initHidden()\n",
        "        model.zero_grad()\n",
        "        loss = 0\n",
        "        for j in range(sentence.size()[0]):\n",
        "            output, hidden = model(sentence[j].view(1), hidden)\n",
        "            l = criterion(output, target[j].view(1))\n",
        "            loss += l\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import urllib.request\n",
        "import io\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn import Embedding"
      ],
      "metadata": {
        "id": "qFQMxLuZYMXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the text, transforming everything to lower case\n",
        "text = open('/content/drive/MyDrive/2Stanford/0importantcodes/holmes.txt').read().lower()\n",
        "print('our original text has ' + str(len(text)) + ' characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Kq-BWawwmp",
        "outputId": "8c02bf69-8a6d-4b9b-83cc-656d0e833923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our original text has 581864 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### find and replace '\\n' and '\\r' symbols - replacing them\n",
        "text = text[1302:]\n",
        "text = text.replace('\\n',' ')    # replacing '\\n' with '' simply removes the sequence\n",
        "text = text.replace('\\r',' ')\n",
        "\n",
        "\n",
        "\n",
        "### find and replace '\\n' and '\\r' symbols - replacing them\n",
        "### find and replace '\\n' and '\\r' symbols - replacing them\n",
        "text = text[1302:]\n",
        "text = text.replace('\\n',' ')    # replacing '\\n' with '' simply removes the sequence\n",
        "text = text.replace('\\r',' ')\n",
        "\n",
        "\n",
        "### list all unique characters in the text and remove any non-english ones\n",
        "# find all unique characters in the text\n",
        "unique = set(text)\n",
        "#create list of characters to remove from corpus\n",
        "non_english = [c for c in unique if c not in \" abcdefghijklmnopqrstuvwxyz:!.;,?'\"]\n",
        "for r in non_english:\n",
        "    text = text.replace(r,'')\n",
        "\n",
        "# shorten any extra dead space created above\n",
        "text = text.replace('  ',' ')\n",
        "\n",
        "\n",
        "\n",
        "# count the number of unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# print some of the text, as well as statistics\n",
        "print (\"this corpus has \" +  str(len(text)) + \" total number of characters\")\n",
        "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
        "\n",
        "\n",
        "def window_transform_text(text,window_size,step_size):\n",
        "    # containers for input/output pairs\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    #initial input string list\n",
        "    initial_seq = [x for x in text[0:window_size]]\n",
        "    #create input/outputs from sliding window over text\n",
        "    for i in range(0,len(text)-window_size,step_size):\n",
        "        outputs.append(text[i+window_size])\n",
        "        inputs.append(initial_seq)\n",
        "        initial_seq = initial_seq[step_size:]\n",
        "        initial_seq.extend(text[i+window_size:i+window_size+step_size])\n",
        "\n",
        "\n",
        "    return inputs,outputs\n",
        "\n",
        "\n",
        "\n",
        "# run your text window-ing function\n",
        "window_size = 100\n",
        "step_size = 5\n",
        "inputs, outputs = window_transform_text(text,window_size,step_size)\n",
        "\n",
        "\n",
        "# print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from\n",
        "print('input = ' + str(inputs[2]))\n",
        "print('output = ' + str(outputs[2]))\n",
        "print('--------------')\n",
        "print('input = ' + str(inputs[100]))\n",
        "print('output = ' + str(outputs[100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI8I0l74wwpe",
        "outputId": "a6fee65d-f898-4da5-c5cf-b887689c3943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this corpus has 566796 total number of characters\n",
            "this corpus has 34 unique characters\n",
            "input = ['.', ' ', 'y', 'o', 'u', ' ', 'd', 'i', 'd', ' ', 'n', 'o', 't', ' ', 't', 'e', 'l', 'l', ' ', 'm', 'e', ' ', 't', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'i', 'n', 't', 'e', 'n', 'd', 'e', 'd', ' ', 't', 'o', ' ', 'g', 'o', ' ', 'i', 'n', 't', 'o', ' ', 'h', 'a', 'r', 'n', 'e', 's', 's', '.', ' ', 't', 'h', 'e', 'n', ',', ' ', 'h', 'o', 'w', ' ', 'd', 'o', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', '?', ' ', 'i', ' ', 's', 'e', 'e', ' ', 'i', 't', ',', ' ', 'i', ' ', 'd', 'e', 'd', 'u', 'c']\n",
            "output = e\n",
            "--------------\n",
            "input = ['u', 'c', 'e', ' ', 'i', 't', '.', ' ', 'a', 's', ' ', 't', 'o', ' ', 'm', 'a', 'r', 'y', ' ', 'j', 'a', 'n', 'e', ',', ' ', 's', 'h', 'e', ' ', 'i', 's', ' ', 'i', 'n', 'c', 'o', 'r', 'r', 'i', 'g', 'i', 'b', 'l', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'w', 'i', 'f', 'e', ' ', 'h', 'a', 's', ' ', 'g', 'i', 'v', 'e', 'n', ' ', 'h', 'e', 'r', ' ', 'n', 'o', 't', 'i', 'c', 'e', ',', ' ', 'b', 'u', 't', ' ', 't', 'h', 'e', 'r', 'e', ',', ' ', 'a', 'g', 'a', 'i', 'n', ',', ' ', 'i', ' ']\n",
            "output = f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run your text window-ing function\n",
        "window_size = 10\n",
        "step_size = 1\n",
        "inputs, outputs = window_transform_text(text,window_size,step_size)\n",
        "print('input = ' + str(inputs[0]))\n",
        "print('output = ' + str(outputs[0]))\n",
        "print('--------------')\n",
        "window_size = 4\n",
        "step_size = 2\n",
        "inputs, outputs = window_transform_text(text,window_size,step_size)\n",
        "# print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from\n",
        "print('input = ' + str(inputs[0]))\n",
        "print('output = ' + str(outputs[0]))\n",
        "print('--------------')\n",
        "print('input = ' + str(inputs[1]))\n",
        "print('output = ' + str(outputs[1]))\n",
        "print('--------------')\n",
        "print('input = ' + str(inputs[2]))\n",
        "print('output = ' + str(outputs[2]))\n",
        "print('--------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YYKXkTUwwsQ",
        "outputId": "b7b97708-3a23-41f3-838d-5372ab042f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = [' ', 'i', ' ', 'o', 'b', 's', 'e', 'r', 'v', 'e']\n",
            "output = .\n",
            "--------------\n",
            "input = [' ', 'i', ' ', 'o']\n",
            "output = b\n",
            "--------------\n",
            "input = [' ', 'o', 'b', 's']\n",
            "output = e\n",
            "--------------\n",
            "input = ['b', 's', 'e', 'r']\n",
            "output = v\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the number of unique characters in the dataset\n",
        "chars = sorted(list(set(text)))\n",
        "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
        "print ('and these characters are ')\n",
        "print (chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OtGSNCZwwuv",
        "outputId": "a275007c-ed30-4127-87b8-a97b66bdb44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this corpus has 34 unique characters\n",
            "and these characters are \n",
            "[' ', '!', \"'\", ',', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this dictionary is a function mapping each unique character to a unique integer\n",
        "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
        "\n",
        "# this dictionary is a function mapping each unique integer back to a unique character\n",
        "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
      ],
      "metadata": {
        "id": "bZM5AgdKwwxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform character-based input/output into equivalent numerical versions\n",
        "def encode_io_pairs(text,window_size,step_size):\n",
        "    # number of unique chars\n",
        "    chars = sorted(list(set(text)))\n",
        "    num_chars = len(chars)\n",
        "\n",
        "    # cut up text into character input/output pairs\n",
        "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
        "\n",
        "    # create empty vessels for one-hot encoded input/output\n",
        "    X = np.zeros((len(inputs), window_size, num_chars))\n",
        "    y = np.zeros((len(inputs), num_chars))\n",
        "\n",
        "    # loop over inputs/outputs and tranform and store in X/y\n",
        "    for i, sentence in enumerate(inputs):\n",
        "        for t, char in enumerate(sentence):\n",
        "            X[i, t, chars_to_indices[char]] = 1\n",
        "        y[i, chars_to_indices[outputs[i]]] = 1\n",
        "\n",
        "    return X,y"
      ],
      "metadata": {
        "id": "b_HWmamxqDs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=text[0:3000]\n",
        "window_size = 5\n",
        "step_size = 1\n",
        "inputs, outputs = window_transform_text(data,window_size,step_size)\n",
        "print(data)\n",
        "X,y = encode_io_pairs(data,window_size,step_size)\n",
        "for index,inp in enumerate(inputs):\n",
        "  print(inp)\n",
        "  print(outputs[index])\n",
        "  print(X[index])\n",
        "  print(y[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "mu73vJ5Sq2jw",
        "outputId": "87a4c684-f4c1-4cc4-f590-77b3dc966e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " i observe. you did not tell me that you intended to go into harness. then, how do you know? i see it, i deduce it. how do i know that you have been getting yourself very wet lately, and that you have a most clumsy and careless servant girl? my dear holmes, said i, this is too much. you would certainly have been burned, had you lived a few centuries ago. it is true that i had a country walk on thursday and came home in a dreadful mess, but as i have changed my clothes i can't imagine how you deduce it. as to mary jane, she is incorrigible, and my wife has given her notice, but there, again, i fail to see how you work it out. he chuckled to himself and rubbed his long, nervous hands together. it is simplicity itself, said he; my eyes tell me that on the inside of your left shoe, just where the firelight strikes it, the leather is scored by six almost parallel cuts. obviously they have been caused by someone who has very carelessly scraped round the edges of the sole in order to remove crusted mud from it. hence, you see, my double deduction that you had been out in vile weather, and that you had a particularly malignant bootslitting specimen of the london slavey. as to your practice, if a gentleman walks into my rooms smelling of iodoform, with a black mark of nitrate of silver upon his right forefinger, and a bulge on the right side of his tophat to show where he has secreted his stethoscope, i must be dull, indeed, if i do not pronounce him to be an active member of the medical profession. i could not help laughing at the ease with which he explained his process of deduction. when i hear you give your reasons, i remarked, the thing always appears to me to be so ridiculously simple that i could easily do it myself, though at each successive instance of your reasoning i am baffled until you explain your process. and yet i believe that my eyes are as good as yours. quite so, he answered, lighting a cigarette, and throwing himself down into an armchair. you see, but you do not observe. the distinction is clear. for example, you have frequently seen the steps which lead up from the hall to this room. frequently. how often? well, some hundreds of times. then how many are there? how many? i don't know. quite so! you have not observed. and yet you have seen. that is just my point. now, i know that there are seventeen steps, because i have both seen and observed. bytheway, since you are interested in these little problems, and since you are good enough to chronicle one or two of my trifling experiences, you may be interested in this. he threw over a sheet of thick, pinktinted notepaper which had been lying open upon the table. it came by the last post, said he. read it aloud. the note was undated, and without either signature or address. there will call upon you tonight, at a quarter to eight o'clock, it said, a gentleman who desires to consult you upon a matter of the very deepest moment. your recent services to one of the royal houses of europe have sh\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 32 is out of bounds for axis 1 with size 32",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5d4440b9ce24>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_transform_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_io_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2cb1dad0a6ae>\u001b[0m in \u001b[0;36mencode_io_pairs\u001b[0;34m(text, window_size, step_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars_to_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars_to_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 32 is out of bounds for axis 1 with size 32"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use your function\n",
        "window_size = 100\n",
        "step_size = 5\n",
        "X,y = encode_io_pairs(text,window_size,step_size)"
      ],
      "metadata": {
        "id": "DdtM_AEJqDvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0],y[0])\n",
        "print(X[1],y[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIBE-MEWqDyM",
        "outputId": "fbbb8bc3-4917-4b94-ac66-d5b64afe8a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def process_text(file_path):\n",
        "    text = open(file_path, 'rb').read().decode(encoding='utf-8')  # Read, then decode for py2 compat.\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\n",
        "    # Creating a mapping from unique characters to indices and vice versa\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    text_as_int = np.array([char2idx[c] for c in text])\n",
        "    return text_as_int, vocab, char2idx, idx2char\n",
        "\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text, target_text = chunk[:-1], chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "\n",
        "def create_dataset(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\n",
        "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=1000, temperature=1.0):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "    # Low temperatures results in more predictable text, higher temperatures results in more surprising text.\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []  # Empty string to store our results\n",
        "    model.reset_states()\n",
        "    for i in range(generate_char_num):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)    # remove the batch dimension\n",
        "        predictions /= temperature\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        # We pass the predicted character as the next input to the model along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], axis=0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "\n",
        "# path_to_file = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text_as_int, vocab, char2idx, idx2char = process_text(path_to_file)\n",
        "dataset = create_dataset(text_as_int)\n",
        "model = build_model(vocab_size=len(vocab))\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.summary()\n",
        "history = model.fit(dataset, epochs=50)\n",
        "model.save_weights(\"gen_text_weights.h5\", save_format='h5')\n",
        "# To keep this prediction step simple, use a batch size of 1\n",
        "model = build_model(vocab_size=len(vocab), batch_size=1)\n",
        "model.load_weights(\"gen_text_weights.h5\")\n",
        "model.summary()\n",
        "\n",
        "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
        "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=2000)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWS5Dn-8qD0t",
        "outputId": "95b839c9-f219-4cce-8ff8-f56c3e924737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dropout (Dropout)           (64, None, 1024)          0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (64, None, 1024)          4096      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (64, None, 1024)          0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (64, None, 1024)          4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13731137 (52.38 MB)\n",
            "Trainable params: 13727041 (52.36 MB)\n",
            "Non-trainable params: 4096 (16.00 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "172/172 [==============================] - 38s 178ms/step - loss: 2.3549\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 34s 191ms/step - loss: 1.6579\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 33s 186ms/step - loss: 1.5099\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 33s 187ms/step - loss: 1.4373\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 1.3877\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.3473\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.3112\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.2761\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.2425\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.2046\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.1669\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 1.1248\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.0796\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 1.0309\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.9793\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.9289\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.8786\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.8274\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.7818\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.7375\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.6972\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.6602\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.6268\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.5989\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.5732\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.5490\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.5308\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.5139\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.4980\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.4840\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.4728\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.4622\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 34s 190ms/step - loss: 0.4530\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.4433\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 33s 187ms/step - loss: 0.4356\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 34s 190ms/step - loss: 0.4274\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 33s 188ms/step - loss: 0.4216\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.4155\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.4104\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.4065\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 33s 188ms/step - loss: 0.4003\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.3953\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.3902\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.3871\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.3831\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 33s 189ms/step - loss: 0.3796\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.3771\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 34s 188ms/step - loss: 0.3729\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.3694\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 34s 189ms/step - loss: 0.3680\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (1, None, 1024)           0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (1, None, 1024)           4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (1, None, 1024)           0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (1, None, 1024)           4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13731137 (52.38 MB)\n",
            "Trainable params: 13727041 (52.36 MB)\n",
            "Non-trainable params: 4096 (16.00 KB)\n",
            "_________________________________________________________________\n",
            "Write the beginning of the text, the program will complete it. Your input is: he was born in\n",
            "he was born in Margaret:\n",
            "But if I could, by Him that made you ou were this!\n",
            "Look you, sir; I should knock you where you do reach off but MENENIUS:\n",
            "Nay, as I am\n",
            "not bewitch your been at Paulina's eye,\n",
            "But est was when I was descred the thing, and pay thy pity.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Whenever Buckingham do ne'er was thine eyes;\n",
            "Speak to your house and enemy time no come.\n",
            "\n",
            "Nurse:\n",
            "Now, by my troth, you have counsel, Signior Lucentio.\n",
            "Kindness, I confess I wash a part.\n",
            "\n",
            "CATESBY:\n",
            "Return unto thy lord; commend me to him:\n",
            "Tell him the queen hath heartily consented\n",
            "Hath sometimes Kate KING RICHARD II:\n",
            "Give me the crown.\n",
            "\n",
            "KING RICHARD III:\n",
            "As then I dream this is a dead man leave.\n",
            "\n",
            "LEONTES:\n",
            "Go the prince my master and my kingdom and\n",
            "Destroy'd the seal'st of my son's law.\n",
            "\n",
            "CLARENCE:\n",
            "Thy hopes s are dissever'd son.\n",
            "\n",
            "BRUTUS:\n",
            "Last\n",
            "My wife comes forth; asks thee thou and thy lord.\n",
            "\n",
            "JULIET:\n",
            "O comfortable boy!\n",
            "That life refore I'll pay thee, fare you well.\n",
            "\n",
            "ANGELO:\n",
            "Hark, how the virtuous and we will carry no.\n",
            "\n",
            "PAULINA:\n",
            "I do not know--take an in.\n",
            "\n",
            "Love the devil's dam. Come, see that run your ears.\n",
            "My tongue will tell the rest,\n",
            "By thow or I had received and charge him;\n",
            "For now hath time made me his numbering clock:\n",
            "My thoughts are minutes; and with sighs they jar\n",
            "That 'twere not like the bow: she is region\n",
            "As victors at home, change it relish.\n",
            "\n",
            "DORCAS:\n",
            "Well, no more. But if she be not so discovered.\n",
            "\n",
            "ROMEO:\n",
            "Let me sure our defence: yet we have seen her heart\n",
            "A mir vex'd away two of mine\n",
            "Or in the office of the poor duke return,\n",
            "And so my shoot is not there stand will not be good and\n",
            "The bring up from his happiness for ever.\n",
            "\n",
            "LADY ANNE:\n",
            "What, do you dead: the tidle will dissemble down\n",
            "Inger than straight\n",
            "extremity to his majesty.\n",
            "My gracious lord, Here's a good world with one but this:\n",
            "It is my will, the wind stirr'd more beauty than any steel\n",
            "Fillow they cain from ease this realmy garments to my soul:\n",
            "My patiences, bills, and peace\n",
            "With hate the world e for a woman's tongue,\n",
            "That gives not mortal to my knig\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def process_text(file_path):\n",
        "    text = open(file_path, 'rb').read().decode(encoding='utf-8')  # Read, then decode for py2 compat.\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\n",
        "    # Creating a mapping from unique characters to indices and vice versa\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    text_as_int = np.array([char2idx[c] for c in text])\n",
        "    return text_as_int, vocab, char2idx, idx2char\n",
        "\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text, target_text = chunk[:-1], chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "\n",
        "def create_dataset(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\n",
        "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "    return dataset\n",
        "# path_to_file = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text_as_int, vocab, char2idx, idx2char = process_text(path_to_file)\n",
        "dataset = create_dataset(text_as_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RToO-ueeq1QS",
        "outputId": "b1e5bebd-bde9-483c-f1dd-e385ec63354a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oeLIHeAvvaS",
        "outputId": "0cc7542f-3361-4658-9ecb-16a5ea44805e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
        "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=200)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1kUxS3MglxU",
        "outputId": "b4205833-9083-45c4-885e-75df09497fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write the beginning of the text, the program will complete it. Your input is: the tidle will\n",
            "the tidle will do all the world:\n",
            "Supposed the very bond of love, though I thank my life.\n",
            "\n",
            "KATHARINA:\n",
            "Gentlemen, forward to the bridal dinner:\n",
            "I little lord.\n",
            "This is the city Corioli is flad at hand,\n",
            "Ensues his pite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q torchview\n",
        "! pip install -q -U graphviz"
      ],
      "metadata": {
        "id": "H1RuVSbgx8DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "from torch import nn\n",
        "import torch\n",
        "import graphviz\n",
        "\n",
        "# when running on VSCode run the below command\n",
        "# svg format on vscode does not give desired result\n",
        "graphviz.set_jupyter_format('png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9UxavfxuyBiv",
        "outputId": "64aacdac-0b39-4056-f2e8-5303798c94ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'svg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi Layer Perceptron with inplace option.\n",
        "    Make sure inplace=true and false has the same visual graph\"\"\"\n",
        "\n",
        "    def __init__(self, inplace: bool = True) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(inplace),\n",
        "            nn.Linear(128, 128),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "s3iJptggyKOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_graph_1 = draw_graph(\n",
        "    MLP(), input_size=(2, 128),\n",
        "    graph_name='MLP',\n",
        "    hide_inner_tensors=False,\n",
        "    hide_module_functions=False,\n",
        ")"
      ],
      "metadata": {
        "id": "q5OhLDfDyKSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_graph_1.visual_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "yaw9zmfZyQKy",
        "outputId": "57a691a2-0024-4ae1-e1cc-83993db7136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7ea0b9d9ef50>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=64):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "text_as_int, vocab, char2idx, idx2char = process_text(path_to_file)\n",
        "dataset = create_dataset(text_as_int)\n",
        "model = build_model(vocab_size=len(vocab))\n",
        "model = build_model(vocab_size=len(vocab), batch_size=1)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikTUw9Cnz2AC",
        "outputId": "02ab9156-224e-4b58-e091-198f64199c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (1, None, 1024)           0         \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (1, None, 1024)           4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (1, None, 1024)           0         \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (1, None, 1024)           4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_6 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13731137 (52.38 MB)\n",
            "Trainable params: 13727041 (52.36 MB)\n",
            "Non-trainable params: 4096 (16.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Same as process_text function\n",
        "def process_text(file_path):\n",
        "    text = open(file_path, 'r').read()  # Read the text file\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\n",
        "    # Creating a mapping from unique characters to indices and vice versa\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    text_as_int = np.array([char2idx[c] for c in text])\n",
        "    return text_as_int, vocab, char2idx, idx2char\n",
        "\n",
        "# Same as split_input_target function\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Similar to create_dataset function\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length=100):\n",
        "        self.seq_length = seq_length\n",
        "        self.text = text\n",
        "        self.samples = self.process_text()\n",
        "\n",
        "    def process_text(self):\n",
        "        inputs, targets = [], []\n",
        "        for i in range(0, len(self.text) - self.seq_length, 1):\n",
        "            inputs.append(self.text[i:i+self.seq_length])\n",
        "            targets.append(self.text[i+1:i+self.seq_length+1])\n",
        "        return inputs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[0][idx], self.samples[1][idx]\n",
        "\n",
        "# Similar to build_model function\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_size=1024, num_layers=2, dropout=0.1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Similar to generate_text function\n",
        "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=1000, temperature=1.0):\n",
        "    model.eval()\n",
        "    input_eval = torch.tensor([char2idx[s] for s in start_string]).unsqueeze(0)\n",
        "    text_generated = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(generate_char_num):\n",
        "            prediction = model(input_eval)\n",
        "            prediction = prediction[:, -1, :] / temperature\n",
        "            predicted_id = torch.multinomial(torch.exp(prediction), num_samples=1)\n",
        "            input_eval = torch.cat((input_eval, predicted_id), dim=1)\n",
        "            text_generated.append(idx2char[predicted_id.item()])\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Similar to the main part of your code\n",
        "def main():\n",
        "    # Download the file\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "    filename = 'shakespeare.txt'\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    text_as_int, vocab, char2idx, idx2char = process_text(filename)\n",
        "    dataset = DataLoader(TextDataset(text_as_int), batch_size=64, shuffle=True, drop_last=True)\n",
        "    model = RNNModel(len(vocab))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(50):\n",
        "        for inputs, targets in dataset:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.transpose(1, 2), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}/50, Loss: {loss.item()}')\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"gen_text_weights.pth\")\n",
        "\n",
        "    # Load the model\n",
        "    model = RNNModel(len(vocab))\n",
        "    model.load_state_dict(torch.load(\"gen_text_weights.pth\"))\n",
        "\n",
        "    # Generate text\n",
        "    start_string = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
        "    generated_text = generate_text(model, char2idx, idx2char, start_string, generate_char_num=2000)\n",
        "    print(generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "f_YC74Yu4IlZ",
        "outputId": "1b033962-b0de-4662-ea85-d01955d2c1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-dbd84508a5fa>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-dbd84508a5fa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/50, Loss: {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "# Same as process_text function\n",
        "def process_text(file_path):\n",
        "    text = open(file_path, 'r').read()  # Read the text file\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\n",
        "    # Creating a mapping from unique characters to indices and vice versa\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    text_as_int = np.array([char2idx[c] for c in text])[0:1000000]\n",
        "    return text_as_int, vocab, char2idx, idx2char\n",
        "\n",
        "# Same as split_input_target function\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Similar to create_dataset function\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length=100):\n",
        "        self.seq_length = seq_length\n",
        "        self.text = text\n",
        "        self.samples = self.process_text()\n",
        "\n",
        "    def process_text(self):\n",
        "        inputs, targets = [], []\n",
        "        for i in range(0, len(self.text) - self.seq_length, 1):\n",
        "            inputs.append(self.text[i:i+self.seq_length])\n",
        "            targets.append(self.text[i+1:i+self.seq_length+1])\n",
        "        return inputs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[0][idx], self.samples[1][idx]\n",
        "\n",
        "# Similar to build_model function\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_size=1024, num_layers=1, dropout=0.1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Similar to generate_text function\n",
        "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=3, temperature=1.0):\n",
        "    generate_char_num=5\n",
        "    model.eval()\n",
        "    input_eval = torch.tensor([char2idx[s] for s in start_string]).unsqueeze(0).to(device)  # Move input to device\n",
        "    text_generated = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(generate_char_num):\n",
        "            print(\"=============================================================\")\n",
        "            print(\"input_eval ===>\",input_eval)\n",
        "            prediction = model(input_eval)\n",
        "            print(\"=================\",prediction.shape)\n",
        "            print(\"prediction ===>\",prediction)\n",
        "            print(\"prediction[:,-1:,:] ===>\",prediction[:,-1:,:])\n",
        "            prediction = prediction[:, -1, :] / temperature\n",
        "            predicted_id = torch.multinomial(torch.exp(prediction), num_samples=1)\n",
        "            print(\"predicted_id===>\",predicted_id)\n",
        "            print(\"input_eval===>\",torch.cat((input_eval, predicted_id), dim=1))\n",
        "            input_eval = torch.cat((input_eval, predicted_id), dim=1)\n",
        "            text_generated.append(idx2char[predicted_id.item()])\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Similar to the main part of your code\n"
      ],
      "metadata": {
        "id": "xOjxMgFQB7of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    # Download the file\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "    filename = 'shakespeare.txt'\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    text_as_int, vocab, char2idx, idx2char = process_text(filename)\n",
        "    dataset = DataLoader(TextDataset(text_as_int), batch_size=64, shuffle=True, drop_last=True)\n",
        "    model = RNNModel(len(vocab)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    for epoch in range(5):\n",
        "        print(epoch)\n",
        "        for index, inp_tar in enumerate(dataset):\n",
        "            inputs = inp_tar[0].to(device)\n",
        "            targets = inp_tar[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.transpose(1, 2), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if index % 10000 == 0:\n",
        "              print(index)\n",
        "        print(f'Epoch {epoch+1}/1, Loss: {loss.item()}')\n",
        "    torch.save(model.state_dict(), \"gen_text_weights.pth\")\n",
        "\n",
        "\n",
        "def generate_text_from_model():\n",
        "    text_as_int, vocab, char2idx, idx2char = process_text('shakespeare.txt')\n",
        "    model = RNNModel(len(vocab)).to(device)  # Move model to device\n",
        "    model.load_state_dict(torch.load(\"gen_text_weights.pth\"))\n",
        "    start_string = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
        "    generated_text = generate_text(model, char2idx, idx2char, start_string, generate_char_num=5)\n",
        "    print(generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n",
        "    #generate_text_from_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r-2bLPbC9ev",
        "outputId": "a35197a1-055f-4468-ced4-e9d6128b592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "10000\n",
            "Epoch 1/1, Loss: 1.3874250650405884\n",
            "1\n",
            "0\n",
            "10000\n",
            "Epoch 2/1, Loss: 1.3670473098754883\n",
            "2\n",
            "0\n",
            "10000\n",
            "Epoch 3/1, Loss: 1.3716216087341309\n",
            "3\n",
            "0\n",
            "10000\n",
            "Epoch 4/1, Loss: 1.3970739841461182\n",
            "4\n",
            "0\n",
            "10000\n",
            "Epoch 5/1, Loss: 1.3887929916381836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_graph_1 = draw_graph(\n",
        "    RNNModel1(65), input_size=(1,100),\n",
        "    graph_name='MLP',\n",
        "    hide_inner_tensors=False,\n",
        "    hide_module_functions=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DAZHqVnEKEFJ",
        "outputId": "8fbfe86e-d784-41e1-95ce-c9493784cf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to run torchgraph see error message",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/torchview.py\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(model, x, device, model_graph, mode, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/recorder_tensor.py\u001b[0m in \u001b[0;36m_module_forward_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# this seems not to be necessary so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_orig_module_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-bf6b174a7f80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/recorder_tensor.py\u001b[0m in \u001b[0;36m_module_forward_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# this seems not to be necessary so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_orig_module_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m         return handle_torch_function(\n\u001b[0m\u001b[1;32m   2207\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0;31m# implementations can do equality/identity comparisons.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_func_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/recorder_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             out = nn.parameter.Parameter.__torch_function__(\n\u001b[0m\u001b[1;32m    237\u001b[0m                 func, types, args, kwargs).as_subclass(RecorderTensor)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c91b466eae1e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_graph_1 = draw_graph(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mRNNModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MLP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhide_inner_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhide_module_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/torchview.py\u001b[0m in \u001b[0;36mdraw_graph\u001b[0;34m(model, input_data, input_size, graph_name, depth, device, dtypes, mode, strict, expand_nested, graph_dir, hide_module_functions, hide_inner_tensors, roll, show_shapes, save_graph, filename, directory, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     forward_prop(\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_recorder_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_record_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchview/torchview.py\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(model, x, device, model_graph, mode, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown input type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;34m\"Failed to run torchgraph see error message\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         ) from e\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchgraph see error message"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq863CQqJvpi",
        "outputId": "411d5a8b-289c-45e7-9e2b-85775bb5123b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_from_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "GmObQTGBEtiC",
        "outputId": "ec7dd627-f6c7-4f01-ca0c-261900e38068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_text_from_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a7a98026b713>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_text_from_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "rFqGvNdPJciF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same as process_text function\n",
        "def process_text(file_path):\n",
        "    text = open(file_path, 'r').read()  # Read the text file\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\n",
        "    # Creating a mapping from unique characters to indices and vice versa\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    text_as_int = np.array([char2idx[c] for c in text])[0:100]\n",
        "    return text_as_int, vocab, char2idx, idx2char\n",
        "\n",
        "# Same as split_input_target function\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Similar to create_dataset function\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length=3):\n",
        "        self.seq_length = seq_length\n",
        "        self.text = text\n",
        "        self.samples = self.process_text()\n",
        "\n",
        "    def process_text(self):\n",
        "        inputs, targets = [], []\n",
        "        for i in range(0, len(self.text) - self.seq_length, 1):\n",
        "            inputs.append(self.text[i:i+self.seq_length])\n",
        "            targets.append(self.text[i+1:i+self.seq_length+1])\n",
        "        return inputs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[0][idx], self.samples[1][idx]\n"
      ],
      "metadata": {
        "id": "1_i_i39Vck3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel1(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_size=1024, num_layers=1, dropout=0.1):\n",
        "        super(RNNModel1, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        print(x.shape)\n",
        "        print(x[0:2])\n",
        "        x, _ = self.rnn(x)\n",
        "        print(\" x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \")\n",
        "        print(x.shape)\n",
        "        print(x[0:2])\n",
        "        x = self.fc(x)\n",
        "        print(\"x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\")\n",
        "        print(x.shape)\n",
        "        print(x[0:2])\n",
        "        return x"
      ],
      "metadata": {
        "id": "EIZI_AslGqs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar to generate_text function\n",
        "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=5, temperature=1.0):\n",
        "    generate_char_num=5\n",
        "    model.eval()\n",
        "    input_eval = torch.tensor([char2idx[s] for s in start_string]).unsqueeze(0).to(device)  # Move input to device\n",
        "    text_generated = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(generate_char_num):\n",
        "            print(\"=============================================================\")\n",
        "            print(\"input_eval ===>\",input_eval)\n",
        "            prediction = model(input_eval)\n",
        "            print(\"=================\",prediction.shape)\n",
        "            print(\"prediction ===>\",prediction)\n",
        "            print(\"prediction[:,-1:,:] ===>\",prediction[:,-1:,:])\n",
        "            prediction = prediction[:, -1, :] / temperature\n",
        "            predicted_id = torch.multinomial(torch.exp(prediction), num_samples=1)\n",
        "            print(\"predicted_id===>\",predicted_id)\n",
        "            print(\"input_eval===>\",torch.cat((input_eval, predicted_id), dim=1))\n",
        "            input_eval = torch.cat((input_eval, predicted_id), dim=1)\n",
        "            text_generated.append(idx2char[predicted_id.item()])\n",
        "    return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "EVoiWSkNtTaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xAffaAAtxjr",
        "outputId": "a9dced71-cf3c-4e73-b5bf-ab075e1bb899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    # Download the file\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "    filename = 'shakespeare.txt'\n",
        "    if not os.path.isfile(filename):\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    text_as_int, vocab, char2idx, idx2char = process_text(filename)\n",
        "    dataset = DataLoader(TextDataset(text_as_int), batch_size=5, shuffle=True, drop_last=True)\n",
        "    model = RNNModel1(len(vocab)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    for epoch in range(2):\n",
        "        print(epoch)\n",
        "        for index, inp_tar in enumerate(dataset):\n",
        "            inputs = inp_tar[0].to(device)\n",
        "            targets = inp_tar[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.transpose(1, 2), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if index % 10000 == 0:\n",
        "              print(index)\n",
        "        print(f'Epoch {epoch+1}/1, Loss: {loss.item()}')\n",
        "    torch.save(model.state_dict(), \"gen_text_weights.pth\")\n",
        "\n",
        "\n",
        "def generate_text_from_model():\n",
        "    text_as_int, vocab, char2idx, idx2char = process_text('shakespeare.txt')\n",
        "    model = RNNModel(len(vocab)).to(device)  # Move model to device\n",
        "    model.load_state_dict(torch.load(\"gen_text_weights.pth\"))\n",
        "    start_string = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
        "    generated_text = generate_text(model, char2idx, idx2char, start_string, generate_char_num=5)\n",
        "    print(generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n",
        "    #generate_text_from_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs8_o1uJtbRD",
        "outputId": "4a3f54e8-8ada-47fb-f509-d272da1880df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.3956,  0.9045,  0.0760,  ..., -0.3892, -0.3379,  1.4239],\n",
            "         [ 0.3956,  0.9045,  0.0760,  ..., -0.3892, -0.3379,  1.4239],\n",
            "         [-0.5231, -1.3779,  0.6178,  ...,  0.7330,  1.4276,  0.1835]],\n",
            "\n",
            "        [[ 1.3391, -0.1197,  0.3610,  ..., -0.7518,  0.3780,  0.4941],\n",
            "         [ 1.1039,  0.4133,  0.2516,  ...,  1.4361,  1.1678,  0.1342],\n",
            "         [ 1.1039,  0.4133,  0.2516,  ...,  1.4361,  1.1678,  0.1342]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-3.7186e-03,  1.3514e-01, -2.4920e-02,  ...,  1.2197e-01,\n",
            "           2.0576e-02, -1.6881e-02],\n",
            "         [-1.2253e-02,  2.0451e-01, -4.8911e-02,  ...,  1.7000e-01,\n",
            "           2.3864e-02, -1.6939e-02],\n",
            "         [-4.5856e-02,  9.1732e-02, -1.2053e-01,  ...,  2.8371e-02,\n",
            "           2.0980e-02,  1.1112e-01]],\n",
            "\n",
            "        [[ 5.7756e-02,  7.9323e-02, -6.7538e-02,  ...,  7.2580e-03,\n",
            "           6.2735e-02, -3.7719e-02],\n",
            "         [-1.4126e-02,  6.6600e-02, -9.5096e-04,  ...,  4.9767e-03,\n",
            "           3.9582e-02, -4.9682e-02],\n",
            "         [-2.2053e-02,  5.7956e-02,  1.8144e-02,  ..., -1.6591e-04,\n",
            "           1.0110e-02, -4.3275e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 2.7761e-02,  3.9512e-02,  3.3572e-02,  1.3663e-02, -5.1969e-02,\n",
            "           1.5482e-02, -4.2402e-02,  3.5324e-02, -3.0883e-02,  3.4881e-02,\n",
            "           4.3228e-03, -3.6539e-02,  3.8538e-02, -4.0245e-02, -1.0474e-02,\n",
            "           1.1655e-02, -1.6131e-02,  4.1853e-02, -3.5397e-02,  8.9267e-02,\n",
            "          -2.7272e-02,  4.5468e-02,  8.3222e-03, -2.0757e-02, -2.9632e-03,\n",
            "           1.4982e-02,  1.4869e-02,  4.0284e-02,  1.1187e-02, -4.4601e-02,\n",
            "          -4.4791e-02, -9.0279e-02, -2.8160e-02, -3.7559e-02, -4.8305e-02,\n",
            "           1.4565e-02, -1.5369e-02, -6.8290e-02, -5.0265e-02,  2.9323e-02,\n",
            "           4.8189e-02,  3.0267e-02,  3.6584e-03,  5.3372e-02,  6.1622e-02,\n",
            "           1.7214e-02, -4.7684e-02, -2.3830e-02, -5.3792e-02, -7.6184e-03,\n",
            "           6.1141e-02, -2.5274e-02,  1.7165e-03, -6.7147e-02, -6.3887e-03,\n",
            "          -4.9853e-02, -5.7271e-02,  9.9202e-02, -7.4913e-02,  1.6004e-02,\n",
            "           9.2088e-03,  4.9246e-02,  4.6397e-02, -1.9597e-02,  6.9715e-02],\n",
            "         [ 4.2186e-02,  7.5206e-02,  3.6382e-02,  1.4727e-02, -6.1281e-02,\n",
            "           1.1294e-02, -4.9901e-02,  5.0577e-02, -3.7798e-02,  5.1595e-02,\n",
            "           5.5245e-03, -5.1810e-02,  5.9826e-02, -6.3217e-02, -1.8672e-02,\n",
            "           2.3320e-02, -2.7364e-02,  5.2869e-02, -6.0773e-02,  1.0966e-01,\n",
            "          -3.4328e-02,  5.3604e-02,  5.8078e-03, -8.7508e-03, -5.4343e-03,\n",
            "           1.9902e-03,  1.9860e-02,  5.3987e-02,  2.2870e-02, -5.6325e-02,\n",
            "          -5.3170e-02, -1.1335e-01, -3.8197e-02, -4.3839e-02, -6.8237e-02,\n",
            "           1.2634e-02, -5.8912e-03, -9.1518e-02, -6.9105e-02,  3.9961e-02,\n",
            "           7.0247e-02,  5.1822e-02,  1.1523e-02,  6.9879e-02,  8.9757e-02,\n",
            "           2.0278e-02, -7.4184e-02, -4.1243e-02, -7.4037e-02, -1.0755e-02,\n",
            "           8.4274e-02, -3.8600e-02, -4.9191e-04, -9.8012e-02, -1.3725e-02,\n",
            "          -7.2412e-02, -8.0084e-02,  1.3021e-01, -8.9223e-02,  1.8204e-02,\n",
            "          -5.8865e-03,  6.8451e-02,  5.4194e-02, -2.4999e-02,  1.0297e-01],\n",
            "         [ 5.6707e-02,  7.9641e-02,  6.9043e-03,  8.2668e-03,  9.4963e-03,\n",
            "           6.5725e-02, -3.6754e-02,  2.7264e-02,  4.5910e-03,  1.8588e-02,\n",
            "          -6.8245e-02, -1.0851e-01, -4.2777e-03, -5.3455e-02,  4.7030e-03,\n",
            "          -3.2703e-02, -4.0065e-02,  2.9847e-02, -4.6417e-02,  1.0198e-01,\n",
            "          -1.7394e-03,  2.2829e-02,  1.4766e-02,  3.6903e-02, -9.4077e-02,\n",
            "           9.0210e-02,  1.7050e-02,  3.9595e-02,  4.0546e-02, -5.7729e-02,\n",
            "          -8.3104e-02, -4.2729e-02, -1.8873e-02, -4.0932e-03, -7.7474e-02,\n",
            "           8.9891e-03, -8.2328e-03, -4.8970e-02, -5.3358e-02,  4.0158e-03,\n",
            "           1.5194e-01,  5.4290e-02,  6.8059e-02,  7.5827e-03,  4.4688e-02,\n",
            "           2.4734e-02, -8.9109e-03, -1.1722e-02, -1.1239e-01, -1.6990e-02,\n",
            "           3.1429e-02, -2.3448e-02, -4.8557e-02, -6.0157e-02,  3.0229e-02,\n",
            "          -8.4546e-02, -5.8837e-02,  2.8118e-02, -3.3741e-02,  5.1242e-02,\n",
            "           7.4127e-02, -8.9849e-04,  5.5923e-02, -2.3051e-02,  7.4742e-02]],\n",
            "\n",
            "        [[-2.2701e-02, -4.5914e-02,  1.3844e-02,  8.6566e-05,  2.8683e-03,\n",
            "           6.2825e-02, -5.6031e-02,  1.7247e-02,  1.7441e-02, -9.0814e-03,\n",
            "          -7.6525e-02, -6.5334e-02, -3.3591e-02,  4.1547e-02, -3.8069e-02,\n",
            "          -3.1084e-03, -2.4886e-02,  1.5351e-02, -4.0567e-02,  1.6854e-02,\n",
            "          -4.6444e-02,  4.3708e-02, -1.6941e-02, -4.4364e-04,  9.2894e-03,\n",
            "          -2.9594e-02,  6.3982e-03,  1.0826e-02,  1.7944e-02,  1.4115e-02,\n",
            "          -1.0208e-01, -7.4210e-02, -5.9220e-02, -3.9166e-02, -6.0744e-03,\n",
            "           3.2412e-02, -7.8457e-02, -5.0560e-02,  1.4251e-03, -2.2963e-02,\n",
            "          -9.2387e-02,  8.9058e-03, -5.8662e-02,  3.9497e-02,  6.2704e-02,\n",
            "          -9.9595e-02, -6.1595e-02, -2.0710e-03,  1.4415e-02,  6.1637e-02,\n",
            "           3.0139e-02, -4.3298e-02,  1.3630e-02, -5.6857e-03, -3.3210e-02,\n",
            "           3.0067e-02, -1.2060e-03,  3.0282e-02, -7.3567e-02, -1.4714e-02,\n",
            "          -1.8687e-02,  1.0166e-02,  5.0204e-02, -1.1082e-02,  6.3334e-02],\n",
            "         [-1.7854e-02, -1.4375e-02,  4.5970e-02, -5.7647e-02,  4.6034e-03,\n",
            "           8.1008e-03, -2.5570e-02,  4.2893e-03,  2.1783e-02,  2.6570e-02,\n",
            "          -6.4546e-02, -3.3202e-02, -1.2731e-02, -2.1813e-02,  5.5497e-02,\n",
            "          -2.6242e-02,  3.2865e-02,  6.2574e-02, -5.9749e-02, -4.1791e-03,\n",
            "          -4.6686e-02,  2.9223e-02, -2.1084e-02,  3.4159e-03, -8.4789e-03,\n",
            "          -1.3081e-01, -5.8543e-02, -1.2855e-02,  4.9678e-02, -2.8848e-02,\n",
            "          -5.8910e-02, -4.8892e-02,  1.9876e-03, -8.9704e-03, -5.6172e-02,\n",
            "           6.3008e-02, -3.2023e-02, -1.1504e-02, -4.1152e-02,  1.0450e-02,\n",
            "          -6.3744e-02, -8.2234e-03, -7.9864e-02,  3.1822e-02,  2.1776e-02,\n",
            "          -6.6281e-02,  4.3946e-02, -5.0429e-02,  5.3704e-03,  3.6829e-02,\n",
            "           3.8844e-02, -1.0799e-02,  3.2381e-02, -4.4739e-03,  7.1533e-02,\n",
            "           4.6592e-03,  1.0204e-02,  1.7964e-02, -4.9734e-02, -8.0400e-02,\n",
            "          -1.0402e-02,  3.4213e-02,  7.0531e-02, -3.7591e-02,  9.2527e-02],\n",
            "         [-7.7704e-03,  4.5905e-03,  4.7388e-02, -7.7233e-02,  6.6284e-03,\n",
            "          -2.5407e-02, -1.0775e-02, -8.0965e-04,  2.9816e-02,  4.4441e-02,\n",
            "          -6.8003e-02, -9.5852e-03,  1.8915e-03, -5.2407e-02,  9.7519e-02,\n",
            "          -3.7345e-02,  6.0845e-02,  9.2401e-02, -7.0783e-02, -6.4205e-03,\n",
            "          -6.3388e-02,  2.8393e-02, -1.0769e-02,  7.6405e-04, -1.1878e-02,\n",
            "          -1.7239e-01, -8.1909e-02, -3.1096e-02,  7.1037e-02, -6.5254e-02,\n",
            "          -4.3360e-02, -3.2203e-02,  2.8967e-02,  1.0096e-02, -8.1069e-02,\n",
            "           7.4472e-02, -3.5379e-03,  1.3294e-02, -6.9672e-02,  3.2585e-02,\n",
            "          -4.6115e-02, -8.4028e-03, -8.3364e-02,  2.1573e-02,  6.8337e-03,\n",
            "          -4.9099e-02,  9.9599e-02, -8.2094e-02, -8.6349e-03,  2.4490e-02,\n",
            "           3.8000e-02, -4.1935e-03,  3.0720e-02, -6.8277e-03,  1.2053e-01,\n",
            "          -3.5915e-03,  1.7808e-02,  1.8758e-02, -3.7476e-02, -1.0483e-01,\n",
            "          -2.6786e-02,  4.3094e-02,  8.0260e-02, -4.5758e-02,  1.0276e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "0\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.3169, -0.3823,  0.3799,  ..., -0.1383,  0.3709,  2.1311],\n",
            "         [ 1.1139,  0.4033,  0.2416,  ...,  1.4261,  1.1778,  0.1442],\n",
            "         [-0.5946, -0.2733, -1.2561,  ...,  0.4878,  1.0752, -1.0269]],\n",
            "\n",
            "        [[ 0.1182,  0.8351,  0.5175,  ...,  0.6335,  0.1248, -0.9831],\n",
            "         [ 1.1139,  0.4033,  0.2416,  ...,  1.4261,  1.1778,  0.1442],\n",
            "         [ 0.8108,  0.2255, -0.4599,  ...,  0.1036,  0.2162,  0.2186]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-0.0007, -0.0362,  0.0027,  ..., -0.0562,  0.1057,  0.1004],\n",
            "         [-0.2914, -0.1433, -0.0728,  ..., -0.1385,  0.1188,  0.0574],\n",
            "         [-0.0106, -0.2160, -0.0893,  ..., -0.0864,  0.0239,  0.0474]],\n",
            "\n",
            "        [[ 0.0572, -0.0980, -0.0250,  ..., -0.0323, -0.1094, -0.0365],\n",
            "         [-0.0822, -0.1482, -0.1133,  ..., -0.1157, -0.1153,  0.0518],\n",
            "         [-0.0483, -0.0909, -0.2106,  ..., -0.0403, -0.0279,  0.0260]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 1.7710e-01, -2.7160e-01, -1.6837e-01, -3.2711e-01, -2.8874e-01,\n",
            "          -4.5532e-01,  7.9446e-01, -1.7340e-01, -2.5997e-01, -1.5337e-01,\n",
            "           3.6845e-01, -3.9432e-01, -3.6817e-01, -1.3641e-01, -2.8741e-01,\n",
            "          -3.2316e-01, -3.0116e-01, -3.2596e-01, -2.0951e-01, -3.3842e-01,\n",
            "          -2.8547e-01, -7.9781e-02, -2.7581e-01, -3.7964e-01, -3.4961e-01,\n",
            "          -3.7151e-01, -1.4854e-01, -3.0959e-01, -2.4392e-01, -3.4491e-01,\n",
            "          -5.4379e-01, -3.6520e-01, -3.9609e-01, -3.9030e-01, -4.4726e-01,\n",
            "          -3.5273e-01, -1.9126e-01, -3.0708e-01, -3.8801e-01, -1.7957e-01,\n",
            "          -3.8893e-01, -2.4733e-01, -4.7191e-02,  2.3921e+00, -3.7296e-01,\n",
            "          -2.2038e-01, -3.7652e-01, -7.6206e-02, -3.8502e-01, -4.7072e-01,\n",
            "           3.4972e-01, -3.2716e-01, -3.2513e-01, -2.9186e-01, -3.8920e-01,\n",
            "          -3.6288e-01,  1.2005e+00, -3.0196e-01, -1.8017e-01, -4.3323e-01,\n",
            "          -2.3460e-01, -2.3887e-01, -4.2310e-01, -3.9099e-01,  3.9388e-03],\n",
            "         [-9.3913e-02, -7.6223e-01, -6.9335e-01, -7.0483e-01, -8.0984e-01,\n",
            "          -1.1719e+00,  2.1264e+00, -9.9512e-01, -8.3352e-01, -6.1847e-01,\n",
            "           2.4916e-01, -1.1645e+00, -7.8226e-01, -8.2944e-01, -9.1679e-01,\n",
            "          -9.1972e-01, -6.7641e-01, -8.6318e-01, -6.1466e-01, -8.7995e-01,\n",
            "          -1.1549e+00, -4.8826e-01, -8.2196e-01, -9.6370e-01, -8.1292e-01,\n",
            "          -1.1799e+00, -6.2864e-01, -8.3742e-01, -7.5275e-01, -1.0143e+00,\n",
            "          -1.2466e+00, -9.8654e-01, -8.8295e-01, -8.4112e-01, -9.1098e-01,\n",
            "          -9.7221e-01, -6.2325e-01, -7.0836e-01, -9.9803e-01, -8.0483e-01,\n",
            "          -1.0873e+00, -6.7767e-01,  2.9280e+00,  3.1574e+00, -9.1880e-01,\n",
            "          -1.1335e+00, -8.8258e-01, -4.8754e-01, -1.0402e+00, -1.0943e+00,\n",
            "           1.3971e-01, -8.5445e-01, -7.7130e-01, -9.1490e-01, -9.9371e-01,\n",
            "          -1.0358e+00,  3.7640e+00, -1.0511e+00, -3.7153e-01, -1.2569e+00,\n",
            "          -6.7185e-01, -8.1929e-01, -9.8919e-01, -1.0607e+00, -4.4323e-01],\n",
            "         [-1.2285e-03, -3.7807e-01, -4.6086e-01, -3.6424e-01, -4.9900e-01,\n",
            "          -5.8461e-01,  2.7057e+00, -9.4009e-01, -4.8132e-01, -3.3118e-01,\n",
            "           2.4022e-01, -9.5103e-01, -3.9088e-01, -5.2362e-01, -5.7054e-01,\n",
            "          -4.5967e-01, -5.2525e-01, -6.5189e-01, -4.8509e-01, -5.9445e-01,\n",
            "          -7.9339e-01, -3.7868e-01, -4.2704e-01, -4.6492e-01, -5.1488e-01,\n",
            "          -8.2698e-01, -2.7952e-01, -6.2202e-01, -5.1542e-01, -6.9928e-01,\n",
            "          -8.7700e-01, -7.1390e-01, -5.5813e-01, -5.0076e-01, -6.5166e-01,\n",
            "          -5.8793e-01, -3.6736e-01, -4.6048e-01, -7.4182e-01, -5.6577e-01,\n",
            "          -8.0333e-01, -4.2522e-01,  2.8968e+00,  1.7486e+00, -5.7099e-01,\n",
            "          -8.0639e-01, -5.2956e-01, -9.4190e-01, -7.1107e-01, -7.2121e-01,\n",
            "          -8.5255e-02, -6.3094e-01, -5.0035e-01, -5.2132e-01, -6.2474e-01,\n",
            "          -6.9633e-01,  2.2942e+00, -7.0819e-01, -2.5998e-01, -8.6195e-01,\n",
            "          -2.4823e-01, -4.7490e-01, -5.3215e-01, -7.0706e-01, -6.8024e-01]],\n",
            "\n",
            "        [[ 2.1691e-01, -9.3833e-02,  9.7742e-03, -1.6731e-01, -5.6110e-02,\n",
            "          -7.3401e-02,  3.0263e-01, -1.6874e-01, -1.2452e-01, -6.0941e-02,\n",
            "           8.0822e-02, -1.3399e-01, -1.0871e-01, -1.0972e-01, -3.3295e-02,\n",
            "          -1.4515e-01, -8.5490e-02, -9.7717e-02, -8.5188e-02, -6.9403e-02,\n",
            "          -1.2197e-01, -9.4340e-02, -8.3933e-02, -1.7421e-01, -1.7655e-01,\n",
            "          -7.9627e-02, -9.0703e-02, -1.1492e-01, -1.2726e-01, -1.6389e-01,\n",
            "          -2.1670e-01, -1.4794e-01, -2.0259e-01, -2.1070e-01, -1.8679e-01,\n",
            "          -9.7385e-02, -9.9716e-02, -1.8032e-01, -2.2737e-01, -1.3576e-01,\n",
            "          -1.0416e-01, -1.5441e-01,  3.2762e-01,  2.9374e-01, -1.0789e-01,\n",
            "          -1.6631e-01, -5.1845e-02,  1.4308e-01, -2.2270e-01, -1.7645e-01,\n",
            "           5.3582e-04, -2.0309e-01, -9.8856e-02, -6.3974e-02, -1.0203e-01,\n",
            "          -1.3852e-01,  3.8126e-01, -1.8213e-01, -5.5568e-02, -6.6654e-02,\n",
            "          -1.1209e-01, -1.0769e-01, -8.2507e-02, -6.4708e-02,  1.6867e-02],\n",
            "         [-7.5066e-02, -6.7625e-01, -5.8896e-01, -6.8090e-01, -6.9934e-01,\n",
            "          -9.1844e-01,  1.4863e+00, -9.1909e-01, -7.8672e-01, -5.7760e-01,\n",
            "           1.1300e-01, -1.0458e+00, -6.8396e-01, -7.2419e-01, -7.8738e-01,\n",
            "          -8.7376e-01, -6.1953e-01, -7.5116e-01, -6.0553e-01, -6.9616e-01,\n",
            "          -1.0186e+00, -5.2286e-01, -7.7298e-01, -8.5271e-01, -7.3630e-01,\n",
            "          -1.0224e+00, -6.0059e-01, -7.4905e-01, -6.9217e-01, -9.2986e-01,\n",
            "          -1.0463e+00, -8.0142e-01, -7.6429e-01, -7.6773e-01, -7.8174e-01,\n",
            "          -8.0454e-01, -5.2440e-01, -7.0051e-01, -9.2952e-01, -7.7792e-01,\n",
            "          -8.2471e-01, -6.3665e-01,  3.3232e+00,  2.3833e+00, -7.8450e-01,\n",
            "          -9.9434e-01, -6.7016e-01, -3.8356e-01, -8.5040e-01, -9.0249e-01,\n",
            "          -3.3619e-03, -7.7787e-01, -6.0583e-01, -7.7697e-01, -8.6701e-01,\n",
            "          -8.7308e-01,  2.8068e+00, -9.3032e-01, -3.2377e-01, -1.0458e+00,\n",
            "          -6.0401e-01, -6.4550e-01, -7.3378e-01, -9.4238e-01, -3.6239e-01],\n",
            "         [ 5.1658e-02, -3.3822e-01, -2.3280e-01, -2.1836e-01, -3.6151e-01,\n",
            "          -4.8654e-01,  2.0380e+00, -7.1339e-01, -4.0786e-01, -2.6254e-01,\n",
            "           1.3699e-01, -6.5409e-01, -2.0611e-01, -3.6101e-01, -3.6581e-01,\n",
            "          -4.1296e-01, -4.1479e-01, -4.4754e-01, -3.1373e-01, -3.7982e-01,\n",
            "          -6.6183e-01, -3.1930e-01, -3.9299e-01, -2.5843e-01, -4.4894e-01,\n",
            "          -5.6738e-01, -1.7169e-01, -4.8093e-01, -4.0743e-01, -5.5625e-01,\n",
            "          -5.7613e-01, -4.6736e-01, -4.7778e-01, -3.6743e-01, -4.0288e-01,\n",
            "          -4.3464e-01, -2.7299e-01, -3.8906e-01, -6.0398e-01, -4.5231e-01,\n",
            "          -4.7818e-01, -3.5295e-01,  2.6584e+00,  1.1329e+00, -3.3530e-01,\n",
            "          -5.5879e-01, -3.7436e-01, -4.6289e-01, -5.3840e-01, -4.9030e-01,\n",
            "          -7.5586e-02, -5.2014e-01, -3.0788e-01, -4.3217e-01, -5.4612e-01,\n",
            "          -4.8145e-01,  1.3511e+00, -5.4567e-01, -1.3636e-01, -5.8706e-01,\n",
            "          -1.4018e-01, -4.2524e-01, -3.6015e-01, -5.0676e-01, -4.3589e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.5871, -0.2659, -1.2486,  ...,  0.4953,  1.0827, -1.0195],\n",
            "         [-0.4028, -0.2530, -0.5940,  ...,  2.3914, -1.6493,  2.3068],\n",
            "         [-0.5429,  1.3140, -0.7195,  ...,  1.7179, -0.4411, -0.9443]],\n",
            "\n",
            "        [[-0.5202, -1.3714,  0.6107,  ...,  0.7476,  1.4206,  0.2034],\n",
            "         [ 0.6219, -0.9058,  0.4250,  ..., -0.9167, -0.2513,  0.1824],\n",
            "         [-0.1331,  0.6173, -0.5480,  ..., -0.5891,  0.0926, -0.4066]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-0.1357, -0.0021, -0.0571,  ...,  0.0224,  0.0455, -0.0533],\n",
            "         [ 0.4846, -0.1177,  0.2456,  ..., -0.0136, -0.0364, -0.0086],\n",
            "         [-0.0197,  0.0026,  0.4146,  ...,  0.0282,  0.0396, -0.0271]],\n",
            "\n",
            "        [[ 0.0244,  0.2864, -0.6878,  ..., -0.1157, -0.1003,  0.6397],\n",
            "         [ 0.0270,  0.0132, -0.3109,  ..., -0.1283, -0.1835,  0.4178],\n",
            "         [ 0.0145, -0.0636, -0.2083,  ..., -0.1076, -0.1962,  0.1443]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.0810, -0.2352, -0.2896, -0.2785, -0.2578, -0.2744, -0.1945,\n",
            "          -0.3646, -0.2340, -0.1883,  0.3230, -0.3205, -0.1551, -0.3569,\n",
            "          -0.2946, -0.3276, -0.2618, -0.2272, -0.2847, -0.2141, -0.2441,\n",
            "          -0.1776, -0.1121, -0.2896, -0.2410, -0.2903, -0.1793, -0.2717,\n",
            "          -0.2453, -0.1552, -0.2465, -0.2942, -0.1956, -0.3036, -0.2712,\n",
            "          -0.2138, -0.2953, -0.2406, -0.3312,  0.2096, -0.2838, -0.2361,\n",
            "           0.1112,  0.0912,  0.1757, -0.2425, -0.2368, -0.6337, -0.3450,\n",
            "          -0.2763,  0.1677, -0.2159,  0.2093, -0.2028, -0.1446, -0.2715,\n",
            "           1.0933, -0.3727, -0.3085, -0.3600, -0.1560, -0.1058, -0.1808,\n",
            "          -0.3080,  0.0200],\n",
            "         [ 0.6875, -1.0612, -1.1164, -1.0722, -1.0829, -1.1929,  2.7733,\n",
            "          -1.2455, -0.9559, -0.7823,  1.8834, -1.2955, -1.0093, -1.0989,\n",
            "          -0.6627, -0.8639, -1.1887, -1.0641, -1.1873, -1.0777, -1.1870,\n",
            "          -0.9324, -0.9686, -0.9329, -1.0330, -1.2009, -0.9626, -1.1345,\n",
            "          -0.9563, -0.9583, -1.1685, -1.2011, -1.1456, -1.0256, -1.1890,\n",
            "          -1.0861, -0.9377, -0.9546, -1.1467,  0.3402, -1.1785, -1.1657,\n",
            "          -0.1493,  0.3602, -0.2257, -1.1670, -1.0187, -1.0322, -1.1432,\n",
            "          -1.0445,  0.1687, -1.3178, -0.0629, -1.0814, -1.0565, -1.0338,\n",
            "           1.1943,  0.4965, -0.1361, -1.1617, -0.8154, -0.9586, -1.0972,\n",
            "          -0.9519, -0.6837],\n",
            "         [ 1.8695, -1.2726, -1.3103, -1.1648, -1.3658, -1.4539,  1.8479,\n",
            "          -1.2838, -1.1830, -0.9696,  2.1937, -1.5085, -1.0604, -1.2841,\n",
            "          -0.6311, -1.1848, -1.2482, -1.2619, -1.2141, -1.1469, -1.3278,\n",
            "          -1.1492, -1.1463, -1.1741, -1.2519, -1.3481, -1.1568, -1.1723,\n",
            "          -1.2226, -1.1556, -1.3929, -1.2483, -1.2928, -1.1729, -1.3505,\n",
            "          -1.2254, -1.1874, -1.0893, -1.3699,  0.2418, -1.3204, -1.4203,\n",
            "          -0.2252, -0.2400, -0.4246, -1.2987, -1.2350, -0.6556, -1.3601,\n",
            "          -1.1861,  0.2263, -1.4979, -0.2418, -1.2627, -1.0368, -1.3197,\n",
            "           1.5795, -0.1584,  0.2807, -1.3596, -0.9774, -1.1352, -1.2895,\n",
            "          -1.1657, -0.5137]],\n",
            "\n",
            "        [[ 6.5378, -0.8481, -0.8842, -0.8981, -0.9376, -0.8201, -0.1249,\n",
            "          -0.9473, -0.9494, -0.8140,  0.7386, -1.2309, -0.8963, -1.0362,\n",
            "          -0.4188, -1.2158, -0.9387, -1.1954, -0.9688, -0.6947, -0.7792,\n",
            "          -1.0022, -1.1090, -1.0458, -1.1354, -0.9532, -1.1166, -0.7974,\n",
            "          -0.8915, -0.8150, -1.2228, -1.0285, -1.1122, -0.7598, -1.2557,\n",
            "          -0.8840, -0.9905, -0.9014, -1.0435, -0.5317, -0.8112, -0.8121,\n",
            "          -0.2213,  0.2714, -0.3115, -0.9134, -0.7777,  0.0911, -1.0826,\n",
            "          -1.0076,  0.5175, -0.9825, -0.4959, -1.0377, -0.7597, -1.1394,\n",
            "          -0.0650, -0.4992, -0.0188, -1.1403, -0.6938, -1.1858, -1.0546,\n",
            "          -0.9564,  0.1646],\n",
            "         [ 4.3060, -0.7533, -0.8316, -0.9741, -0.8598, -0.8546, -0.4040,\n",
            "          -0.8146, -0.7245, -0.6691,  1.2916, -1.0828, -0.7798, -0.9110,\n",
            "           1.2061, -1.1474, -0.8420, -1.0123, -0.8342, -0.6458, -0.7552,\n",
            "          -0.8412, -1.1332, -1.0008, -0.9315, -0.8431, -0.8384, -0.6674,\n",
            "          -0.6818, -0.7308, -1.0155, -0.9907, -1.0209, -0.5809, -1.0833,\n",
            "          -0.7481, -0.8405, -0.8245, -0.8498, -0.5294, -0.8058, -0.7009,\n",
            "          -0.6011,  1.1769,  0.0162, -0.7390, -0.6442,  0.6639, -0.9691,\n",
            "          -0.7457,  0.5962, -0.8578, -0.3537, -0.9750, -0.6457, -1.0153,\n",
            "          -0.2074, -0.2158,  0.1604, -0.8761, -0.7127, -0.8078, -0.9699,\n",
            "          -0.7881,  0.0684],\n",
            "         [ 2.2795, -0.7687, -0.7086, -0.8465, -0.7239, -0.8985, -0.5127,\n",
            "          -0.6920, -0.7022, -0.5961,  1.1932, -0.9691, -0.6404, -0.6963,\n",
            "           0.3966, -1.0218, -0.5718, -0.7952, -0.7153, -0.6706, -0.6778,\n",
            "          -0.6754, -0.9488, -0.9025, -0.8879, -0.7706, -0.6682, -0.6753,\n",
            "          -0.7190, -0.6561, -0.9084, -0.8359, -0.7870, -0.7457, -0.7836,\n",
            "          -0.7351, -0.7355, -0.8460, -0.7815, -0.1350, -0.7885, -0.6808,\n",
            "          -0.6577,  1.4947,  0.2672, -0.6958, -0.6464,  0.3757, -0.8678,\n",
            "          -0.7593,  0.1664, -0.7901, -0.1337, -0.8665, -0.5872, -0.8243,\n",
            "           0.1363, -0.0917,  0.6565, -0.8294, -0.6282, -0.6104, -0.8687,\n",
            "          -0.6653,  0.2931]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.5925, -0.2591, -1.2536,  ...,  0.4910,  1.0844, -1.0257],\n",
            "         [-1.5638,  0.1785, -0.2444,  ...,  1.5550, -0.2014, -0.9398],\n",
            "         [-2.2760, -0.2954,  1.0098,  ...,  0.3443,  0.9712,  0.2938]],\n",
            "\n",
            "        [[-0.5493,  1.3204, -0.7259,  ...,  1.7115, -0.4475, -0.9507],\n",
            "         [ 1.0342,  1.4470, -0.5164,  ...,  1.3472,  1.5365,  1.4108],\n",
            "         [ 0.2472,  1.6882,  1.6060,  ..., -1.4584,  1.5459,  0.0732]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-0.0606,  0.0058, -0.1214,  ...,  0.0543,  0.0102,  0.0015],\n",
            "         [-0.1504,  0.1469, -0.1733,  ...,  0.3653,  0.0603,  0.1541],\n",
            "         [-0.1694,  0.1190, -0.3570,  ..., -0.0667, -0.0390,  0.1581]],\n",
            "\n",
            "        [[-0.0666, -0.0623,  0.2099,  ...,  0.0037,  0.0920, -0.0027],\n",
            "         [-0.3011, -0.2853,  0.2080,  ..., -0.0439, -0.3948,  0.2458],\n",
            "         [ 0.0072, -0.3089, -0.0292,  ..., -0.0138, -0.5756,  0.0688]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-3.2835e-01,  3.6263e-01, -1.1816e-01, -3.9768e-02, -4.5523e-02,\n",
            "          -7.3776e-02, -6.9944e-01, -1.4139e-01,  1.3425e-02, -1.3289e-02,\n",
            "          -2.8422e-01, -9.7302e-02,  2.0992e-02, -2.0703e-01, -1.3761e-01,\n",
            "          -1.6208e-01, -7.5717e-02,  3.4689e-02, -2.8700e-02, -1.5965e-02,\n",
            "          -3.6808e-02, -2.6314e-03,  8.1201e-02, -1.5066e-01, -3.5878e-02,\n",
            "          -6.8367e-02,  3.9953e-03, -9.7573e-02, -8.2338e-02,  1.0743e-01,\n",
            "          -2.6726e-02, -8.9606e-02,  9.7152e-03, -1.5018e-01, -7.3266e-02,\n",
            "          -3.9798e-02, -6.9165e-02, -1.4155e-01, -1.0338e-01,  4.9093e-01,\n",
            "          -1.0521e-01, -4.3926e-02, -3.7161e-01, -3.7961e-01,  3.1262e-01,\n",
            "          -8.3375e-02, -5.8863e-02, -6.1203e-01, -2.0370e-01,  5.6180e-01,\n",
            "          -5.4951e-02,  4.4750e-01,  4.5148e-01,  9.7114e-02,  1.5824e-01,\n",
            "          -9.5181e-02,  1.6842e+00, -2.5094e-01, -2.3211e-01, -7.8367e-02,\n",
            "           7.8815e-02,  5.9245e-02,  4.7583e-02, -1.0529e-02,  2.6572e-01],\n",
            "         [ 1.7925e+00, -8.0663e-01, -1.9676e+00, -1.8316e+00, -1.7937e+00,\n",
            "          -2.1559e+00, -6.9263e-01, -1.8981e+00, -3.3735e-01, -1.8715e+00,\n",
            "           1.8425e+00, -2.0550e+00, -1.7086e+00, -1.9465e+00, -8.8709e-01,\n",
            "          -2.1161e+00, -1.8031e+00, -1.9369e+00, -1.9493e+00, -1.9055e+00,\n",
            "          -1.9332e+00, -1.8538e+00, -2.0325e+00, -2.1370e+00, -1.8683e+00,\n",
            "          -2.1011e+00, -1.8921e+00, -1.7844e+00, -1.9364e+00, -1.8727e+00,\n",
            "          -2.1038e+00, -2.1237e+00, -2.0256e+00, -2.0256e+00, -2.0466e+00,\n",
            "          -1.8823e+00, -1.9512e+00, -1.1906e+00, -2.0181e+00, -1.2014e-01,\n",
            "          -1.8977e+00, -1.8437e+00,  7.5577e-02,  8.7171e-01, -3.0571e-01,\n",
            "          -1.9962e+00, -1.8499e+00, -7.5243e-02, -1.9993e+00, -5.4938e-01,\n",
            "           1.9168e+00, -7.9258e-01, -1.2185e-01, -1.2846e+00, -1.7736e+00,\n",
            "          -2.0576e+00,  1.0214e+00, -4.7931e-01, -1.3647e+00, -2.0726e+00,\n",
            "          -1.8618e+00, -1.7371e+00, -1.9232e+00, -1.9512e+00, -3.2274e-01],\n",
            "         [ 4.7252e+00, -4.1865e-01, -2.1811e+00, -1.9741e+00, -2.1288e+00,\n",
            "          -2.2179e+00, -5.0449e-01, -2.0275e+00, -7.8355e-01, -1.9366e+00,\n",
            "           3.1536e+00, -2.4503e+00, -1.7983e+00, -2.1696e+00, -1.0381e+00,\n",
            "          -2.2841e+00, -2.1161e+00, -2.2944e+00, -2.0423e+00, -1.9324e+00,\n",
            "          -2.1887e+00, -2.1916e+00, -2.2539e+00, -2.3101e+00, -2.0838e+00,\n",
            "          -2.4089e+00, -2.0904e+00, -2.0720e+00, -2.1660e+00, -1.9787e+00,\n",
            "          -2.5609e+00, -2.3875e+00, -2.3121e+00, -1.9766e+00, -2.3439e+00,\n",
            "          -2.1814e+00, -2.1465e+00, -5.3800e-01, -2.3875e+00,  1.2780e-01,\n",
            "          -2.1566e+00, -2.0951e+00, -6.4807e-01,  5.3206e-01, -5.7021e-01,\n",
            "          -2.2032e+00, -2.0134e+00, -8.3913e-01, -2.2208e+00,  1.1645e-02,\n",
            "           1.4707e+00, -4.8206e-01, -3.2709e-01, -7.2542e-01, -1.8845e+00,\n",
            "          -2.3866e+00,  1.8219e-01,  1.2556e-02, -1.4348e+00, -2.3315e+00,\n",
            "          -1.9551e+00, -2.2073e+00, -2.2322e+00, -2.1620e+00, -4.5376e-01]],\n",
            "\n",
            "        [[-4.6107e-01,  4.2702e-01,  4.9347e-01,  4.8626e-01,  4.1307e-01,\n",
            "           5.6972e-01,  1.5315e-01,  4.9983e-01,  3.0924e-01,  4.0337e-01,\n",
            "          -4.7650e-01,  4.9350e-01,  4.6769e-01,  4.4316e-01,  2.7816e-01,\n",
            "           4.7025e-01,  4.1644e-01,  6.4394e-01,  5.2328e-01,  5.6803e-01,\n",
            "           5.2404e-01,  4.8970e-01,  6.2405e-01,  5.5705e-01,  4.0096e-01,\n",
            "           6.5384e-01,  4.9817e-01,  5.6068e-01,  4.3844e-01,  4.8418e-01,\n",
            "           5.2469e-01,  5.3696e-01,  4.5970e-01,  5.0264e-01,  6.5962e-01,\n",
            "           5.0323e-01,  5.1288e-01,  2.9485e-01,  4.9579e-01,  2.5873e-01,\n",
            "           5.3372e-01,  4.4912e-01, -1.3493e-01, -5.4492e-01,  1.8117e-01,\n",
            "           5.5718e-01,  4.7941e-01, -3.9692e-02,  5.0826e-01,  3.9555e-01,\n",
            "          -1.4319e-01,  1.0168e+00,  2.3385e-01,  3.0538e-01,  5.8015e-01,\n",
            "           5.3560e-01, -4.7676e-02,  6.7715e-01, -1.5138e-01,  5.7652e-01,\n",
            "           5.1078e-01,  5.5030e-01,  5.8672e-01,  5.9368e-01, -1.5907e-01],\n",
            "         [-5.5355e-01, -5.5882e-01,  2.5064e-02,  2.0722e-01, -1.0933e-01,\n",
            "          -1.7606e-01,  2.7944e-03, -1.8196e-01,  2.4419e-01, -2.8406e-01,\n",
            "          -5.9809e-01, -7.4098e-02, -1.2802e-01,  9.1684e-02,  3.9130e-01,\n",
            "          -4.8285e-02,  8.2266e-02, -8.1577e-02, -3.4569e-02, -7.1990e-02,\n",
            "          -3.6173e-01, -2.4289e-01, -1.0433e-02, -9.2327e-02,  4.2365e-02,\n",
            "          -1.1543e-01, -8.5573e-02, -8.8867e-03, -1.5791e-01, -1.8506e-01,\n",
            "          -4.3172e-02,  1.9589e-04, -3.4132e-01, -1.2818e-01,  5.2062e-02,\n",
            "          -8.3944e-02, -9.1907e-02, -4.2897e-02,  6.5378e-03, -9.0541e-01,\n",
            "           4.1868e-02,  1.8885e-01,  2.7688e-02, -3.7983e-01, -5.2969e-01,\n",
            "          -3.5880e-02, -1.3680e-01,  4.2643e+00, -1.2545e-02, -6.8468e-01,\n",
            "          -7.2129e-02, -1.9694e-02, -4.4085e-01, -3.3845e-01,  1.5693e-01,\n",
            "          -2.8292e-01,  3.8945e-01,  7.8996e-01,  1.6633e+00,  7.3760e-02,\n",
            "          -1.2722e-01, -6.3173e-02, -1.3496e-01,  2.8216e-01, -9.3050e-01],\n",
            "         [-1.4642e+00, -2.1527e+00, -1.9745e+00, -1.9474e+00, -2.0455e+00,\n",
            "          -2.1811e+00, -5.0149e-01, -2.1753e+00, -1.9054e+00, -2.2499e+00,\n",
            "          -7.7432e-01, -2.2606e+00, -2.2579e+00, -1.8572e+00, -1.1809e+00,\n",
            "          -2.3539e+00, -1.8749e+00, -2.0996e+00, -2.0421e+00, -1.8379e+00,\n",
            "          -2.3611e+00, -2.2978e+00, -1.9925e+00, -1.9453e+00, -2.1074e+00,\n",
            "          -2.3431e+00, -2.0463e+00, -2.0962e+00, -2.1069e+00, -2.2342e+00,\n",
            "          -2.4218e+00, -1.9221e+00, -2.0141e+00, -2.2271e+00, -2.2397e+00,\n",
            "          -2.1627e+00, -2.1893e+00, -1.5719e+00, -2.1966e+00, -1.2100e+00,\n",
            "          -2.0047e+00, -1.9694e+00, -2.9702e-01, -3.6631e-01, -1.1755e+00,\n",
            "          -2.4749e+00, -2.0927e+00,  3.1568e+00, -2.5167e+00, -2.5488e+00,\n",
            "          -1.0575e+00, -2.0661e+00, -1.0587e+00, -1.6680e+00, -2.1151e+00,\n",
            "          -2.4956e+00,  3.3922e+00,  1.7096e+00,  6.1724e+00, -2.0712e+00,\n",
            "          -2.0435e+00, -2.0163e+00, -2.3713e+00, -2.2222e+00,  1.4617e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 7.9294e-01,  2.4345e-01, -4.4200e-01,  ...,  8.5739e-02,\n",
            "           1.9828e-01,  2.3653e-01],\n",
            "         [-5.3329e-01, -1.3830e+00,  6.0172e-01,  ...,  7.3700e-01,\n",
            "           1.4100e+00,  1.9422e-01],\n",
            "         [ 6.1230e-01, -9.1809e-01,  4.1594e-01,  ..., -9.3171e-01,\n",
            "          -2.6202e-01,  1.7383e-01]],\n",
            "\n",
            "        [[-1.1663e+00, -8.3751e-02, -3.1152e-04,  ...,  1.4297e+00,\n",
            "           1.2259e+00,  4.6308e-01],\n",
            "         [-4.1657e-01, -2.5004e-01, -6.0607e-01,  ...,  2.3745e+00,\n",
            "          -1.6532e+00,  2.2969e+00],\n",
            "         [ 7.7766e-01, -9.8065e-01,  3.0708e-01,  ...,  1.9497e+00,\n",
            "          -1.0637e+00, -3.2731e-01]]], grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-0.2143,  0.1102, -0.0262,  ...,  0.3188, -0.0990, -0.2265],\n",
            "         [-0.0244,  0.2307, -0.8050,  ..., -0.1222, -0.2459,  0.7124],\n",
            "         [ 0.0255, -0.2020,  0.0319,  ..., -0.8912, -0.4497, -0.0055]],\n",
            "\n",
            "        [[ 0.0089, -0.1337,  0.1363,  ..., -0.1327, -0.0237, -0.1096],\n",
            "         [ 0.6466, -0.5287,  0.3063,  ..., -0.0235, -0.4420, -0.0339],\n",
            "         [ 0.0509, -0.2073,  0.1625,  ..., -0.0900, -0.8397,  0.1726]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 1.5843e+00, -1.0759e-01, -2.7400e+00, -2.6834e+00, -2.9332e+00,\n",
            "          -3.0945e+00, -1.9812e-01, -2.9235e+00, -1.4411e+00, -2.8968e+00,\n",
            "           3.8081e+00, -3.2239e+00, -2.5428e+00, -2.7474e+00, -1.2258e+00,\n",
            "          -2.8393e+00, -2.8964e+00, -3.0408e+00, -2.9768e+00, -2.7601e+00,\n",
            "          -3.0654e+00, -2.7844e+00, -2.9000e+00, -3.0259e+00, -2.9622e+00,\n",
            "          -3.1215e+00, -2.8441e+00, -2.9335e+00, -2.8792e+00, -2.8354e+00,\n",
            "          -3.1945e+00, -2.9924e+00, -2.9859e+00, -2.7500e+00, -3.0179e+00,\n",
            "          -3.0439e+00, -3.0145e+00, -9.8674e-01, -3.0334e+00,  9.1183e-02,\n",
            "          -2.8373e+00, -2.8001e+00, -5.8501e-01,  4.0966e-01, -6.3573e-01,\n",
            "          -2.8957e+00, -2.9318e+00, -5.8173e-01, -2.9990e+00,  4.9162e-02,\n",
            "           1.1181e+00, -5.1357e-01, -5.4534e-01, -5.5169e-01, -2.9165e+00,\n",
            "          -3.1575e+00, -2.2084e-01,  9.6023e-01, -7.7957e-01, -2.6311e+00,\n",
            "          -2.6797e+00, -2.9837e+00, -2.9678e+00, -2.9337e+00, -8.9726e-01],\n",
            "         [ 9.2489e+00,  5.7591e-01, -2.3302e+00, -2.2927e+00, -2.4409e+00,\n",
            "          -2.3538e+00, -4.1002e-01, -2.3036e+00, -5.0757e-01, -2.2225e+00,\n",
            "           5.8426e-01, -2.8714e+00, -2.2525e+00, -2.5255e+00, -1.2578e+00,\n",
            "          -2.7522e+00, -2.3804e+00, -2.6376e+00, -2.2845e+00, -2.0989e+00,\n",
            "          -2.2697e+00, -2.4857e+00, -2.5447e+00, -2.4895e+00, -2.6086e+00,\n",
            "          -2.4256e+00, -2.4601e+00, -2.1612e+00, -2.3497e+00, -2.1998e+00,\n",
            "          -2.8250e+00, -2.4333e+00, -2.5870e+00, -2.1085e+00, -2.6641e+00,\n",
            "          -2.4592e+00, -2.4682e+00,  2.3234e+00, -2.6028e+00, -8.6219e-01,\n",
            "          -2.3660e+00, -2.2676e+00, -1.0051e+00,  1.1406e-01, -1.3034e+00,\n",
            "          -2.2731e+00, -2.1941e+00, -4.0018e-01, -2.4925e+00,  2.7135e-01,\n",
            "          -1.7857e-01,  2.4160e-01, -1.3423e+00,  1.5744e+00, -2.0594e+00,\n",
            "          -2.7576e+00, -5.3046e-01,  6.6182e-01, -7.6056e-01, -2.3435e+00,\n",
            "          -2.2221e+00, -2.6388e+00, -2.5368e+00, -2.4619e+00, -7.8757e-01],\n",
            "         [ 2.0418e+00,  1.7219e+00,  1.9625e+00,  1.8023e+00,  1.9888e+00,\n",
            "           1.6161e+00, -7.9335e-01,  1.8827e+00,  1.9633e+00,  2.0768e+00,\n",
            "          -1.1879e+00,  2.1586e+00,  1.8014e+00,  1.8154e+00,  2.8271e+00,\n",
            "           1.9011e+00,  1.8980e+00,  1.7154e+00,  2.1034e+00,  2.0317e+00,\n",
            "           1.6560e+00,  1.7220e+00,  1.8203e+00,  1.8306e+00,  1.7110e+00,\n",
            "           2.0844e+00,  2.1384e+00,  1.9968e+00,  2.0119e+00,  1.9832e+00,\n",
            "           1.8603e+00,  1.8355e+00,  1.7422e+00,  2.0029e+00,  1.8649e+00,\n",
            "           2.0612e+00,  2.0134e+00,  5.0987e+00,  1.6356e+00, -1.9925e-01,\n",
            "           2.0701e+00,  1.8030e+00, -1.1182e+00,  2.2074e-01,  3.6664e-01,\n",
            "           1.9658e+00,  1.9103e+00, -2.2807e-01,  1.9325e+00,  1.5257e+00,\n",
            "          -7.8077e-01,  1.2383e+00, -2.6908e-01,  4.3825e+00,  2.0978e+00,\n",
            "           1.9782e+00, -1.0138e+00,  2.2027e-01, -1.3182e-01,  1.8796e+00,\n",
            "           1.6288e+00,  1.8461e+00,  1.7346e+00,  2.1087e+00, -5.3623e-01]],\n",
            "\n",
            "        [[ 1.6876e-01, -8.2484e-02, -2.3452e-01, -2.8357e-01, -3.3704e-01,\n",
            "          -2.7087e-01,  2.3794e-01, -2.8298e-01, -2.4827e-02, -2.7380e-01,\n",
            "          -2.5663e-02, -2.6628e-01, -3.4074e-01, -4.0319e-01,  2.9085e-02,\n",
            "          -1.8195e-01, -2.1751e-01, -2.3745e-01, -2.2764e-01, -2.8782e-01,\n",
            "          -3.5481e-01, -2.6298e-01, -2.2925e-01, -3.3223e-01, -2.3400e-01,\n",
            "          -3.6601e-01, -3.6351e-01, -2.1949e-01, -2.9662e-01, -2.4902e-01,\n",
            "          -2.7071e-01, -2.6627e-01, -3.2798e-01, -2.2403e-01, -2.3568e-01,\n",
            "          -3.1144e-01, -2.6396e-01, -1.5172e-01, -3.6314e-01, -3.0156e-01,\n",
            "          -2.6971e-01, -2.5970e-01, -8.7006e-02,  2.0476e-01, -3.9115e-01,\n",
            "          -2.3906e-01, -2.2737e-01,  2.8084e-01, -2.1915e-01, -8.6158e-02,\n",
            "           1.7210e-01,  1.7972e-02, -1.5270e-01, -8.0544e-02, -1.7742e-01,\n",
            "          -3.1497e-01,  7.7645e-02, -7.0112e-02, -2.5536e-01, -1.4735e-01,\n",
            "          -2.7099e-01, -2.1984e-01, -2.8430e-01, -2.7658e-01,  8.5013e-03],\n",
            "         [-5.0605e-02,  2.0293e+00,  1.8316e-01,  1.5293e-01,  1.4161e-01,\n",
            "           1.2863e-01,  9.2647e-01,  6.3263e-02, -2.8477e-02,  3.2269e-01,\n",
            "           4.4668e-01,  1.8167e-01,  7.1412e-02,  1.5976e-01,  9.1733e-02,\n",
            "           6.9558e-01,  8.0745e-02,  1.3581e-01,  7.5373e-02,  2.2906e-01,\n",
            "           1.2729e-01,  1.1771e-01,  2.6682e-01,  1.6018e-01,  3.4365e-01,\n",
            "           2.6364e-01,  1.6954e-01,  9.1280e-02,  1.5033e-01,  2.6370e-01,\n",
            "           1.3293e-01,  1.4819e-02,  1.6079e-01,  1.9976e-01,  2.0429e-01,\n",
            "           9.2753e-02,  2.8493e-01,  4.4729e-01,  3.0004e-01,  6.7141e-01,\n",
            "           1.7448e-01,  5.6135e-02, -1.0553e+00, -4.6540e-01,  1.4208e-01,\n",
            "           1.6504e-01,  2.5242e-01, -6.6057e-01,  1.5747e-01,  4.5750e-01,\n",
            "          -5.4796e-01,  1.1524e+00,  1.1920e-02,  5.9139e-01,  2.4152e-01,\n",
            "           2.3945e-01, -4.3607e-01,  2.2783e+00, -7.5305e-02,  5.7370e-01,\n",
            "           3.0604e-01,  2.2679e-01,  2.6684e-01,  3.6751e-01, -9.6422e-01],\n",
            "         [-9.6362e-01, -1.4121e+00, -1.3682e+00, -1.2259e+00, -1.4425e+00,\n",
            "          -1.4939e+00,  2.1767e-01, -1.5710e+00, -4.3622e-01, -1.5260e+00,\n",
            "           6.3628e-03, -1.4967e+00, -1.5626e+00, -1.2739e+00,  2.0889e-01,\n",
            "          -1.5456e+00, -1.0894e+00, -1.3967e+00, -1.4448e+00, -1.5816e+00,\n",
            "          -1.5810e+00, -1.6374e+00, -1.4405e+00, -1.5003e+00, -1.2723e+00,\n",
            "          -1.5702e+00, -1.3153e+00, -1.2041e+00, -1.2176e+00, -1.5454e+00,\n",
            "          -1.4665e+00, -1.5880e+00, -1.5496e+00, -1.3125e+00, -1.5202e+00,\n",
            "          -1.4295e+00, -1.1001e+00, -6.9855e-01, -1.4524e+00, -1.5048e+00,\n",
            "          -1.3520e+00, -9.7321e-01, -6.1784e-01, -2.8793e-01, -1.1863e+00,\n",
            "          -1.4429e+00, -1.2623e+00,  6.6676e+00, -1.3328e+00, -2.2710e+00,\n",
            "          -3.1931e-01, -6.6630e-01, -7.6140e-01, -1.1419e+00, -1.2686e+00,\n",
            "          -1.8753e+00,  2.7344e-01,  1.6050e+00,  2.5220e+00, -1.5808e+00,\n",
            "          -1.4702e+00, -1.5200e+00, -1.2898e+00, -1.1908e+00,  8.3170e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 1.7148, -1.3972, -0.1281,  ...,  0.5853,  0.5994, -0.6176],\n",
            "         [-0.5474,  1.3344, -0.7218,  ...,  1.7181, -0.4528, -0.9578],\n",
            "         [-0.6035, -0.2677, -1.2661,  ...,  0.4847,  1.0872, -1.0330]],\n",
            "\n",
            "        [[-0.4228, -0.2525, -0.6032,  ...,  2.3661, -1.6514,  2.2971],\n",
            "         [ 0.7721, -0.9864,  0.3035,  ...,  1.9452, -1.0687, -0.3325],\n",
            "         [ 0.3384, -0.4080,  0.3884,  ..., -0.1648,  0.3989,  2.1584]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 0.0802, -0.1372, -0.1031,  ...,  0.0656,  0.1139, -0.1150],\n",
            "         [-0.0896, -0.1390,  0.0533,  ...,  0.0031,  0.0369,  0.0104],\n",
            "         [-0.0600, -0.0121, -0.0458,  ...,  0.0486,  0.0019,  0.1145]],\n",
            "\n",
            "        [[ 0.6478, -0.5490,  0.0826,  ..., -0.0072, -0.4789, -0.0303],\n",
            "         [ 0.1011, -0.2206, -0.0309,  ..., -0.0697, -0.8199,  0.0401],\n",
            "         [ 0.0072, -0.4768, -0.2429,  ..., -0.0571, -0.3594,  0.2752]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-7.2801e-01,  2.5840e-01, -9.7460e-01, -1.0689e+00, -1.0200e+00,\n",
            "          -1.0052e+00, -2.2017e-01, -9.7097e-01, -7.9619e-01, -1.1153e+00,\n",
            "           3.8347e-02, -9.4717e-01, -9.7556e-01, -9.8254e-01, -5.7442e-01,\n",
            "          -1.1128e+00, -1.0257e+00, -1.0262e+00, -9.7789e-01, -1.0377e+00,\n",
            "          -1.0160e+00, -1.0051e+00, -1.0643e+00, -9.8686e-01, -1.0472e+00,\n",
            "          -9.9192e-01, -1.0301e+00, -9.6009e-01, -1.1012e+00, -1.0501e+00,\n",
            "          -1.1479e+00, -1.0835e+00, -1.1448e+00, -1.0431e+00, -1.0178e+00,\n",
            "          -9.9487e-01, -1.0047e+00, -9.1832e-01, -1.1601e+00,  6.1268e-01,\n",
            "          -1.0810e+00, -1.0283e+00, -2.7217e-01,  1.5930e+00,  5.4392e-01,\n",
            "          -1.1205e+00, -9.9227e-01, -4.1436e-01, -1.1224e+00, -8.0304e-01,\n",
            "          -5.3136e-01, -9.8629e-01,  5.4568e-01, -6.9101e-01, -1.0914e+00,\n",
            "          -1.1316e+00,  4.5127e-01,  2.4937e-01,  5.9597e-01, -1.0916e+00,\n",
            "          -1.0041e+00, -9.3330e-01, -1.0349e+00, -1.0103e+00,  1.0087e-01],\n",
            "         [-7.4795e-01,  4.2661e-01, -8.0881e-01, -9.4509e-01, -9.3324e-01,\n",
            "          -7.9510e-01, -4.3939e-01, -8.3614e-01, -7.6550e-01, -1.0762e+00,\n",
            "          -3.2708e-01, -9.8755e-01, -1.0023e+00, -9.3747e-01, -3.9949e-01,\n",
            "           5.1918e-01, -1.0542e+00, -7.7267e-01, -9.3971e-01, -8.0701e-01,\n",
            "          -9.2656e-01, -8.3457e-01, -6.2706e-01, -8.2330e-01, -9.9115e-01,\n",
            "          -9.2243e-01, -8.4608e-01, -8.2066e-01, -9.5176e-01, -8.9853e-01,\n",
            "          -1.0485e+00, -8.0929e-01, -1.0267e+00, -8.8269e-01, -8.7862e-01,\n",
            "          -9.1813e-01, -8.5253e-01, -4.1771e-01, -1.0709e+00,  5.1328e-01,\n",
            "          -9.4851e-01, -9.5211e-01, -5.0962e-01,  3.3696e-01,  1.2768e+00,\n",
            "          -8.9729e-01, -7.3541e-01,  1.0517e-01, -8.9444e-01,  9.1867e-02,\n",
            "          -5.8559e-01,  8.3915e-01,  1.6552e-01, -1.2936e-02, -8.5184e-01,\n",
            "          -9.4592e-01,  4.8651e-01,  1.6580e+00,  4.6323e-01, -5.8259e-03,\n",
            "          -8.5614e-01, -7.1833e-01, -8.4083e-01, -8.9258e-01, -8.4419e-02],\n",
            "         [-1.2582e+00,  1.6195e+00, -6.4092e-01, -6.9830e-01, -7.0085e-01,\n",
            "          -7.9403e-01, -7.1035e-01, -6.8980e-01, -3.7993e-01, -8.1785e-01,\n",
            "          -1.5892e+00, -7.9424e-01, -8.0813e-01, -9.4189e-01, -1.0988e-01,\n",
            "           5.8430e-01, -8.5249e-01, -6.0910e-01, -6.8163e-01, -6.2535e-01,\n",
            "          -6.9120e-01, -6.1718e-01, -4.9523e-01, -8.0813e-01, -7.0039e-01,\n",
            "          -8.6529e-01, -5.8053e-01, -7.1781e-01, -8.0730e-01, -5.5523e-01,\n",
            "          -7.4081e-01, -7.6622e-01, -6.7737e-01, -7.5879e-01, -7.9761e-01,\n",
            "          -7.3780e-01, -5.5968e-01, -1.6187e-01, -8.3673e-01,  2.4109e-01,\n",
            "          -8.8599e-01, -8.4647e-01, -1.1197e+00, -1.6987e-01,  1.1236e+00,\n",
            "          -7.6060e-01, -5.3993e-01, -4.5419e-01, -9.8765e-01,  2.8414e+00,\n",
            "          -8.9452e-01,  1.6335e+00, -6.6202e-02,  1.0273e+00, -5.0644e-01,\n",
            "          -7.8729e-01,  2.5472e+00,  9.5652e-01,  3.7458e-01,  5.5436e-01,\n",
            "          -5.3078e-01, -5.7125e-01, -6.4793e-01, -6.4691e-01,  3.0233e-01]],\n",
            "\n",
            "        [[-1.5745e-01,  2.3670e+00,  2.7532e-02, -1.0162e-02, -5.3322e-02,\n",
            "          -1.3145e-01,  2.1596e-01, -8.3595e-02, -3.8680e-01,  1.0204e-01,\n",
            "           3.4110e-01, -5.5086e-02, -8.2807e-02, -1.3921e-02,  9.4881e-02,\n",
            "           7.8044e-01, -1.2479e-01, -5.8001e-02, -2.0644e-01,  1.3589e-03,\n",
            "          -5.9818e-02, -8.7515e-02,  4.2386e-02, -7.9350e-02,  8.5975e-02,\n",
            "           4.1422e-02, -8.6074e-03, -2.2678e-01, -1.5173e-02,  4.4230e-02,\n",
            "          -1.3575e-01, -9.1214e-02, -1.0511e-01, -1.2676e-01, -3.9952e-02,\n",
            "          -7.7121e-02,  3.0454e-02,  7.2770e-01,  7.4229e-02,  5.1609e-01,\n",
            "          -3.4075e-02, -1.6056e-01, -1.1722e+00, -2.8212e-01,  2.9168e-01,\n",
            "          -9.6153e-02,  5.4784e-01, -1.0224e+00, -9.1560e-02,  4.0659e-01,\n",
            "          -7.8827e-01,  1.2254e+00, -2.0807e-01,  8.3294e-01,  8.9748e-03,\n",
            "          -4.2518e-02, -4.3482e-01,  2.6381e+00,  6.5500e-01,  5.5254e-01,\n",
            "           1.0575e-01,  2.9201e-02,  6.9746e-02,  5.8251e-02, -9.3987e-01],\n",
            "         [-1.0775e+00, -5.3204e-01,  1.1340e-01,  3.5860e-01, -1.9204e-02,\n",
            "           1.0791e-01, -3.5378e-02, -9.6111e-02,  5.8451e-01, -1.2312e-01,\n",
            "          -1.8280e-01,  9.1225e-02, -8.8978e-03,  1.9832e-01,  8.9152e-01,\n",
            "           1.7624e-02,  4.1380e-01,  1.7821e-01,  1.9365e-02,  5.4511e-02,\n",
            "          -3.2800e-03, -1.5542e-01,  1.1833e-01,  1.7855e-01,  2.0929e-01,\n",
            "          -7.1916e-02,  3.4374e-01,  1.6238e-01,  2.0328e-01,  1.5008e-02,\n",
            "           1.1168e-01, -1.2442e-01,  8.5765e-02,  3.6427e-02,  1.0013e-03,\n",
            "           9.3804e-02,  4.0189e-01,  2.8509e-01,  8.7941e-03, -9.5009e-01,\n",
            "           6.5135e-02,  5.0518e-01, -5.7087e-01, -2.7750e-01, -7.0617e-01,\n",
            "           9.2425e-02,  1.9132e+00,  3.2825e+00,  9.6867e-02, -1.4190e+00,\n",
            "          -5.6773e-02,  2.1557e-01, -5.8747e-01,  1.1532e-01,  1.8749e-01,\n",
            "          -3.7441e-01,  6.3451e-02,  1.7412e+00,  1.1838e+00, -7.3571e-02,\n",
            "           3.6924e-02,  7.3709e-02,  1.8720e-01,  1.9157e-01,  9.4166e-01],\n",
            "         [-9.9989e-01, -3.2653e-01, -3.1855e+00, -3.4718e+00, -3.3476e+00,\n",
            "          -3.7220e+00, -7.5472e-01, -3.2386e+00, -1.7187e+00, -3.1830e+00,\n",
            "          -2.0066e-01, -3.3659e+00, -3.5867e+00, -3.2240e+00, -4.7343e-01,\n",
            "          -3.1260e+00, -3.2290e+00, -3.3342e+00, -3.4486e+00, -3.3709e+00,\n",
            "          -3.3975e+00, -3.3052e+00, -3.3027e+00, -3.4533e+00, -3.4327e+00,\n",
            "          -3.5022e+00, -3.1854e+00, -3.3570e+00, -3.2828e+00, -3.3381e+00,\n",
            "          -3.8038e+00, -3.5123e+00, -3.6579e+00, -3.4918e+00, -3.6950e+00,\n",
            "          -3.3942e+00, -3.2219e+00, -9.6524e-01, -3.5923e+00,  2.0723e+00,\n",
            "          -3.3619e+00, -3.2013e+00, -2.7266e+00,  7.8800e+00,  9.6025e-01,\n",
            "          -3.4311e+00, -2.5390e+00, -2.5746e-01, -3.2583e+00, -2.3273e+00,\n",
            "          -1.1946e+00, -1.9486e+00,  3.4013e-01, -4.2213e-01, -3.4759e+00,\n",
            "          -3.5915e+00,  2.2128e-01,  1.5197e+00,  2.2963e-01, -3.3556e+00,\n",
            "          -3.2460e+00, -3.1893e+00, -3.5262e+00, -3.4046e+00,  5.8740e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 1.0830,  0.4233,  0.2252,  ...,  1.3862,  1.1513,  0.1541],\n",
            "         [-0.6062, -0.2664, -1.2657,  ...,  0.4862,  1.0922, -1.0357],\n",
            "         [-1.5499,  0.1836, -0.2361,  ...,  1.5736, -0.1872, -0.9281]],\n",
            "\n",
            "        [[ 0.6032, -0.9314,  0.4192,  ..., -0.9464, -0.2708,  0.1725],\n",
            "         [ 1.6262,  1.1113, -1.6746,  ...,  0.3698,  1.2602, -1.1518],\n",
            "         [ 0.2421,  1.6828,  1.6103,  ..., -1.4495,  1.5604,  0.0690]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 4.9720e-02, -7.4734e-02, -4.8556e-01,  ...,  3.1191e-04,\n",
            "           1.4950e-02,  2.6692e-04],\n",
            "         [ 1.8963e-02,  6.4740e-04, -1.7479e-02,  ...,  5.1426e-02,\n",
            "           1.8749e-02, -9.5888e-02],\n",
            "         [-2.1035e-02,  1.7527e-02, -3.6378e-03,  ...,  4.0294e-02,\n",
            "           2.5646e-02,  4.3829e-01]],\n",
            "\n",
            "        [[ 7.2622e-02, -1.8905e-01,  4.0871e-02,  ..., -6.4984e-01,\n",
            "          -2.9350e-01, -9.6033e-04],\n",
            "         [ 1.3392e-01, -3.5543e-01,  1.0676e-01,  ..., -7.6829e-01,\n",
            "          -4.4816e-03, -3.7773e-04],\n",
            "         [ 7.3663e-03, -2.1722e-02, -4.9853e-02,  ..., -3.0547e-01,\n",
            "          -3.2679e-02,  5.9648e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.7107,  2.5812, -2.0550, -2.0635, -2.0911, -2.0652, -1.0249,\n",
            "          -1.9983, -1.3838, -2.0417, -0.1091, -1.9859, -2.1930, -2.2157,\n",
            "          -0.9295, -1.4281, -2.1090, -2.2194, -2.0943, -2.0512, -2.1622,\n",
            "          -2.0535, -2.0572, -2.0618, -2.2230, -2.1359, -2.1034, -1.9836,\n",
            "          -2.0921, -2.1464, -2.0827, -2.1070, -2.0902, -2.1084, -2.1366,\n",
            "          -2.1157, -2.0348, -0.6981, -2.0918,  2.0700, -2.1840, -2.0882,\n",
            "           0.4010,  1.6147,  2.6534, -2.0046, -1.5486, -0.9175, -2.0719,\n",
            "           0.3856, -1.7052, -0.3903,  1.4253,  0.4829, -1.6297, -2.1399,\n",
            "          -0.4549,  0.6845, -1.0380, -1.2823, -1.9683, -1.9653, -2.0233,\n",
            "          -2.0902, -1.3353],\n",
            "         [-1.4792,  3.6860, -2.3217, -2.2073, -2.2655, -2.3009, -1.1064,\n",
            "          -2.1279, -1.1163, -2.2888, -1.5924, -2.2188, -2.2759, -2.4771,\n",
            "          -0.6270, -0.7910, -2.2992, -2.2775, -2.1494, -1.9692, -2.2298,\n",
            "          -2.3018, -2.2829, -2.3110, -2.3154, -2.3969, -2.1699, -2.2955,\n",
            "          -2.4800, -2.2729, -2.2726, -2.4185, -2.2982, -2.2003, -2.4533,\n",
            "          -2.2202, -2.1521, -0.3862, -2.4260,  1.3059, -2.2755, -2.2359,\n",
            "          -0.6783,  0.1488,  1.6094, -2.4108, -1.9607, -1.2067, -2.5685,\n",
            "           3.7996, -2.2179,  0.9583,  1.2481,  1.8101, -0.7627, -2.2539,\n",
            "           2.0383,  1.5054, -1.0672, -0.6297, -2.0420, -2.1410, -2.2614,\n",
            "          -2.1672, -0.8330],\n",
            "         [-0.4470,  2.2080, -3.9591, -3.6687, -3.8903, -3.9738,  1.4351,\n",
            "          -3.5859, -0.7236, -3.7308, -1.2589, -3.8889, -3.9351, -3.9823,\n",
            "          -1.2274, -3.2049, -3.7432, -3.8736, -3.6916, -3.8526, -3.8811,\n",
            "          -3.9186, -3.9644, -4.0160, -3.7053, -4.1138, -3.7914, -3.8173,\n",
            "          -3.9610, -3.6353, -3.9699, -4.0849, -4.0288, -3.7844, -4.0205,\n",
            "          -3.8212, -3.6856, -0.6149, -3.8596,  0.7913, -3.9736, -3.9057,\n",
            "          -0.1584,  1.9770,  0.1289, -3.7537, -2.9518, -1.1140, -4.0814,\n",
            "           0.8190, -2.4743, -0.2816,  0.2020,  0.4015, -2.6025, -4.1752,\n",
            "           2.7153,  0.7459, -1.7974, -2.7028, -3.7910, -3.7876, -4.0312,\n",
            "          -3.6207, -1.6355]],\n",
            "\n",
            "        [[ 2.1467,  0.3944, -1.2320, -1.3608, -1.2218, -1.2327, -0.5435,\n",
            "          -1.3297,  0.3593, -1.1990, -0.9780, -1.2936, -1.3152, -1.2557,\n",
            "           4.7748, -0.9758, -1.2941, -1.4109, -1.2286, -1.1381, -1.3837,\n",
            "          -1.1016, -1.2711, -1.1624, -1.2723, -1.2305, -1.2295, -1.1290,\n",
            "          -1.2649, -1.2862, -1.4385, -1.4831, -1.4348, -1.0494, -1.3653,\n",
            "          -1.2434, -1.0669,  5.0247, -1.4422, -0.4921, -1.2679, -1.1700,\n",
            "          -1.1794,  2.8227, -0.1428, -1.2539, -0.2892,  0.1642, -1.2690,\n",
            "          -0.3830, -0.9922, -0.3834, -0.6090,  1.6886, -0.8197, -1.3992,\n",
            "          -0.9978, -0.1989, -0.3576, -0.5886, -1.1573, -1.0943, -1.3597,\n",
            "          -1.3793, -0.2782],\n",
            "         [-0.2107,  1.5008, -0.2221, -0.4278, -0.4005, -0.3924, -1.1541,\n",
            "          -0.1644,  0.3000, -0.2609, -0.8567, -0.3058, -0.4588, -0.3789,\n",
            "           2.7237,  0.3819, -0.2692, -0.4544, -0.1705, -0.4578, -0.2917,\n",
            "          -0.2661, -0.3253, -0.3070, -0.4468, -0.3306, -0.3117, -0.2104,\n",
            "          -0.4004, -0.1755, -0.3323, -0.4445, -0.4194, -0.3569, -0.4096,\n",
            "          -0.3485, -0.3319,  2.7785, -0.4440,  1.5018, -0.3319, -0.3413,\n",
            "          -1.0520,  2.5969,  1.9395, -0.1762,  0.4362, -0.2623, -0.4511,\n",
            "           0.4083, -1.5468, -0.0220,  0.4866,  2.6132,  0.1406, -0.4051,\n",
            "          -1.2277,  0.2399, -0.5622,  0.6344, -0.2012, -0.3915, -0.5716,\n",
            "          -0.3907, -0.6544],\n",
            "         [-1.0165, -0.8768, -1.5817, -1.5590, -1.4562, -1.6009, -1.1150,\n",
            "          -1.4776, -0.8413, -1.4588, -1.1089, -1.6184, -1.7052, -1.6584,\n",
            "           0.3638, -1.7178, -1.6231, -1.5139, -1.4089, -1.4724, -1.7948,\n",
            "          -1.5808, -1.4606, -1.3495, -1.5153, -1.5870, -1.3488, -1.4779,\n",
            "          -1.5422, -1.4211, -1.7429, -1.6666, -1.4296, -1.5587, -1.6606,\n",
            "          -1.8434, -1.4880, -0.0918, -1.6362,  0.5296, -1.5972, -1.4625,\n",
            "          -0.9299,  1.9957,  0.0274, -1.6514, -0.5396, -0.0220, -1.7911,\n",
            "          -1.2762, -0.9964, -1.2126,  0.2322,  0.2361, -0.9429, -1.7039,\n",
            "           2.6496,  0.5769,  1.3621, -1.6635, -1.4183, -1.5467, -1.8913,\n",
            "          -1.5225,  2.1760]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 2.5269, -0.6177,  0.5045,  ...,  1.2940,  0.4878,  0.5833],\n",
            "         [ 1.0785,  0.4257,  0.2237,  ...,  1.3796,  1.1471,  0.1537],\n",
            "         [-0.5470,  1.3471, -0.7179,  ...,  1.7310, -0.4528, -0.9559]],\n",
            "\n",
            "        [[-0.7187,  0.7874, -0.4639,  ..., -0.4095,  0.7508, -0.2471],\n",
            "         [-0.4189,  1.0034,  0.5723,  ..., -0.3312, -2.8496, -1.9036],\n",
            "         [-0.4365, -0.2597, -0.5974,  ...,  2.3497, -1.6476,  2.3011]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-7.6045e-02, -3.2662e-02,  1.3449e-01,  ..., -1.6530e-01,\n",
            "          -2.3811e-02,  9.2391e-02],\n",
            "         [ 3.0950e-02, -1.6109e-01, -5.2670e-01,  ..., -7.4840e-03,\n",
            "          -1.4849e-02,  2.1838e-04],\n",
            "         [-1.5506e-01, -1.1670e-01, -7.5936e-03,  ..., -4.9888e-03,\n",
            "          -7.5076e-04, -2.0067e-02]],\n",
            "\n",
            "        [[-3.4383e-02, -4.0009e-02,  7.3367e-02,  ..., -6.6653e-02,\n",
            "          -2.2729e-02, -4.2574e-02],\n",
            "         [-4.3439e-01,  3.3668e-02,  3.0006e-03,  ..., -1.4928e-02,\n",
            "           5.0028e-02,  1.3295e-01],\n",
            "         [ 6.6673e-01, -4.1278e-01,  8.5065e-03,  ..., -1.6876e-03,\n",
            "          -6.5017e-01, -7.8940e-04]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-5.1711e-01, -1.4417e-01,  6.5912e-01,  5.1214e-01,  4.4416e-01,\n",
            "           6.8424e-01, -2.6905e-01,  5.7394e-01,  5.1165e-01,  5.3971e-01,\n",
            "          -2.9993e-01,  6.5921e-01,  5.4672e-01,  6.3139e-01,  1.0712e+00,\n",
            "           7.1244e-01,  5.0545e-01,  5.6595e-01,  7.1687e-01,  6.0823e-01,\n",
            "           5.6242e-01,  7.7714e-01,  6.6728e-01,  6.3691e-01,  5.7945e-01,\n",
            "           7.4065e-01,  4.6672e-01,  6.8201e-01,  4.9893e-01,  5.6725e-01,\n",
            "           5.7523e-01,  5.9912e-01,  6.1262e-01,  6.4616e-01,  6.3154e-01,\n",
            "           5.7564e-01,  6.6172e-01,  7.7107e-01,  5.8315e-01, -1.3561e-02,\n",
            "           5.3070e-01,  6.9140e-01, -5.6586e-01,  1.2258e+00, -1.8069e-03,\n",
            "           6.5957e-01,  1.5177e+00,  1.5148e+00,  6.1458e-01, -5.3238e-01,\n",
            "           1.9297e-01,  5.6139e-02, -3.5202e-01,  3.4074e-01,  5.8717e-01,\n",
            "           5.4278e-01, -4.8154e-01,  3.2470e-01,  2.1925e-01,  3.8461e-01,\n",
            "           7.5091e-01,  6.3871e-01,  6.9323e-01,  6.6620e-01,  1.2633e-01],\n",
            "         [-9.2026e-01,  3.0218e+00, -2.1732e+00, -2.2093e+00, -2.2681e+00,\n",
            "          -2.1357e+00, -1.1293e+00, -2.0935e+00, -1.5611e+00, -2.1823e+00,\n",
            "          -3.9717e-01, -2.0375e+00, -2.2585e+00, -2.2868e+00, -7.9453e-01,\n",
            "          -1.1276e+00, -2.1956e+00, -2.2548e+00, -1.9042e+00, -2.1608e+00,\n",
            "          -2.3015e+00, -2.1070e+00, -2.1607e+00, -2.1619e+00, -2.3555e+00,\n",
            "          -2.2095e+00, -2.2909e+00, -2.0739e+00, -2.2950e+00, -2.2539e+00,\n",
            "          -2.1993e+00, -2.1515e+00, -2.2068e+00, -2.1781e+00, -2.2688e+00,\n",
            "          -2.1350e+00, -2.1744e+00, -5.0887e-01, -2.2544e+00,  2.3464e+00,\n",
            "          -2.2431e+00, -2.2378e+00,  3.3003e-01,  2.0108e+00,  2.3462e+00,\n",
            "          -2.0287e+00, -1.1429e+00, -3.3592e-01, -2.1712e+00,  3.0083e-01,\n",
            "          -1.9381e+00, -4.7853e-01,  9.1147e-01,  8.3649e-01, -1.1931e+00,\n",
            "          -2.2822e+00, -1.0387e+00,  7.7531e-01, -8.3408e-01, -1.2682e+00,\n",
            "          -2.1693e+00, -2.1591e+00, -2.1147e+00, -2.1619e+00, -1.3910e+00],\n",
            "         [-1.5212e+00,  2.6103e+00, -2.6525e+00, -2.7034e+00, -2.6290e+00,\n",
            "          -2.6677e+00, -1.1850e+00, -2.4266e+00, -1.7608e+00, -2.9148e+00,\n",
            "          -1.3001e+00, -2.6671e+00, -2.6928e+00, -2.7710e+00, -8.0754e-01,\n",
            "           3.5190e-01, -2.6744e+00, -2.6823e+00, -2.3041e+00, -2.4505e+00,\n",
            "          -2.5480e+00, -2.6331e+00, -2.4287e+00, -2.5261e+00, -2.8609e+00,\n",
            "          -2.6781e+00, -2.5766e+00, -2.6189e+00, -2.8190e+00, -2.6604e+00,\n",
            "          -2.8319e+00, -2.5971e+00, -2.7756e+00, -2.5805e+00, -2.7430e+00,\n",
            "          -2.6975e+00, -2.5410e+00, -4.9421e-01, -2.7676e+00,  2.8534e+00,\n",
            "          -2.5652e+00, -2.6959e+00, -7.3573e-01,  6.0278e-01,  2.2442e+00,\n",
            "          -2.6414e+00, -1.4010e+00, -4.9311e-01, -2.7308e+00,  1.5025e+00,\n",
            "          -2.5676e+00,  1.7878e+00,  1.1333e+00,  1.2195e+00, -1.2996e-01,\n",
            "          -2.7900e+00,  5.5088e-01,  3.1905e+00, -5.7939e-01, -4.4501e-01,\n",
            "          -2.6847e+00, -2.4305e+00, -2.7471e+00, -2.5903e+00, -1.2575e+00]],\n",
            "\n",
            "        [[-1.0432e+00, -7.2170e-02, -1.1829e+00, -1.2030e+00, -1.0686e+00,\n",
            "          -1.3296e+00, -9.9569e-02, -1.1109e+00, -9.0480e-01, -1.3283e+00,\n",
            "           2.0719e-01, -1.3635e+00, -1.2696e+00, -1.2742e+00, -5.2106e-01,\n",
            "          -6.7427e-01, -1.3296e+00, -1.0406e+00, -1.1875e+00, -1.3398e+00,\n",
            "          -1.2316e+00, -1.1694e+00, -1.1756e+00, -1.3169e+00, -1.2520e+00,\n",
            "          -1.3529e+00, -1.2877e+00, -1.2481e+00, -1.2116e+00, -1.4135e+00,\n",
            "          -1.2952e+00, -1.0704e+00, -1.2177e+00, -1.1539e+00, -1.2304e+00,\n",
            "          -1.3073e+00, -1.0738e+00, -8.2365e-01, -1.2723e+00,  3.2589e-01,\n",
            "          -1.1510e+00, -1.4565e+00, -4.8965e-01,  6.6040e-02,  1.7831e-02,\n",
            "          -1.0831e+00, -1.0828e+00, -2.7290e-01, -1.3239e+00,  3.9195e-01,\n",
            "          -8.6885e-01,  3.4570e-01,  5.0728e-01,  7.6343e-01, -2.3875e-01,\n",
            "          -1.3893e+00,  7.0946e-01,  2.4736e-01,  1.7514e-01,  1.7254e+00,\n",
            "          -1.2434e+00, -1.1260e+00, -1.4427e+00, -1.3357e+00, -5.1550e-01],\n",
            "         [-7.9407e-01, -1.4922e-01, -4.6351e+00, -4.4627e+00, -4.6462e+00,\n",
            "          -4.7166e+00,  9.5818e-01, -4.5918e+00, -2.4625e+00, -4.6326e+00,\n",
            "          -5.2113e-01, -4.7758e+00, -4.6754e+00, -4.7601e+00, -2.0872e+00,\n",
            "          -3.6890e+00, -4.6898e+00, -4.5222e+00, -4.1715e+00, -4.4037e+00,\n",
            "          -4.6710e+00, -4.6229e+00, -4.5679e+00, -4.7779e+00, -4.4384e+00,\n",
            "          -4.7728e+00, -4.5350e+00, -4.4892e+00, -4.5620e+00, -4.6194e+00,\n",
            "          -4.6689e+00, -4.6518e+00, -4.6452e+00, -4.4679e+00, -4.7435e+00,\n",
            "          -4.4215e+00, -4.2768e+00, -1.5569e+00, -4.6202e+00,  5.5370e-01,\n",
            "          -4.6390e+00, -4.7281e+00, -2.7459e-01,  1.2400e+00, -8.1691e-01,\n",
            "          -4.5945e+00, -3.2904e+00, -2.7369e-01, -4.5700e+00, -4.5824e-01,\n",
            "          -2.4117e+00, -3.2665e-01,  2.7516e-01, -7.0249e-01, -2.7711e+00,\n",
            "          -4.9069e+00,  3.9437e+00,  5.1998e-01, -7.8507e-01, -2.8776e+00,\n",
            "          -4.5415e+00, -4.3858e+00, -4.8092e+00, -4.5182e+00, -5.1588e-01],\n",
            "         [-8.1020e-01,  2.9992e+00, -2.4476e+00, -2.4231e+00, -2.6720e+00,\n",
            "          -2.7595e+00, -1.6309e-01, -2.5116e+00, -2.0172e+00, -2.4426e+00,\n",
            "           2.4521e-01, -2.6257e+00, -2.4405e+00, -2.6540e+00, -7.1788e-01,\n",
            "          -1.0125e+00, -2.5209e+00, -2.6152e+00, -2.2631e+00, -2.6323e+00,\n",
            "          -2.6732e+00, -2.6332e+00, -2.6027e+00, -2.7494e+00, -2.5282e+00,\n",
            "          -2.7553e+00, -2.3451e+00, -2.8131e+00, -2.6477e+00, -2.5933e+00,\n",
            "          -2.6758e+00, -2.6394e+00, -2.8456e+00, -2.6381e+00, -2.6724e+00,\n",
            "          -2.5633e+00, -2.5917e+00,  3.5363e-01, -2.6422e+00,  6.5352e-01,\n",
            "          -2.7024e+00, -2.6170e+00, -1.2680e+00,  4.1425e-01, -3.9352e-01,\n",
            "          -2.7350e+00, -7.0845e-01, -1.4298e+00, -2.7443e+00,  2.0052e-01,\n",
            "          -1.9649e+00,  1.3625e+00, -2.9946e-01,  1.3796e+00, -1.0336e+00,\n",
            "          -2.7155e+00,  9.0209e-01,  2.5230e+00,  1.2524e+00, -1.0299e+00,\n",
            "          -2.2646e+00, -2.4611e+00, -2.5456e+00, -2.5312e+00, -1.1891e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.6138, -0.2677, -1.2693,  ...,  0.4834,  1.0986, -1.0423],\n",
            "         [-1.5448,  0.1865, -0.2296,  ...,  1.5821, -0.1834, -0.9227],\n",
            "         [-0.9419, -0.6875,  0.2474,  ...,  1.2165, -1.5840,  1.7234]],\n",
            "\n",
            "        [[-0.3761, -1.0792,  0.6157,  ...,  0.7470,  0.0647,  0.0435],\n",
            "         [ 1.0749,  0.4249,  0.2215,  ...,  1.3740,  1.1435,  0.1540],\n",
            "         [-0.6138, -0.2677, -1.2693,  ...,  0.4834,  1.0986, -1.0423]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-6.4255e-03,  3.5227e-03, -4.5975e-02,  ...,  2.1627e-02,\n",
            "          -3.7745e-02,  1.0748e-01],\n",
            "         [-1.6237e-02,  1.3504e-01, -4.3233e-04,  ...,  1.1656e-02,\n",
            "           1.6891e-01,  6.9702e-01],\n",
            "         [-3.7963e-01,  2.7733e-03, -1.4643e-03,  ...,  4.2390e-01,\n",
            "           1.4695e-02,  8.1285e-04]],\n",
            "\n",
            "        [[ 2.3910e-01,  7.4127e-02, -1.6532e-01,  ..., -4.4412e-01,\n",
            "          -7.4259e-02,  4.7630e-02],\n",
            "         [ 8.3206e-02, -2.0218e-01, -5.7776e-01,  ..., -4.2545e-02,\n",
            "          -4.7913e-02,  7.7855e-06],\n",
            "         [ 4.3155e-02,  4.4078e-04, -5.6241e-03,  ...,  3.3281e-02,\n",
            "           1.0629e-02,  2.4290e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.5844e+00,  1.9979e+00, -1.3473e+00, -1.1849e+00, -1.4023e+00,\n",
            "          -1.4748e+00, -5.1753e-01, -1.5092e+00, -7.3459e-01, -1.3277e+00,\n",
            "          -2.5047e+00, -1.4725e+00, -1.4293e+00, -1.5670e+00,  1.3493e-01,\n",
            "          -9.1215e-03, -1.3902e+00, -1.4471e+00, -9.2558e-01, -1.4296e+00,\n",
            "          -1.3363e+00, -1.4560e+00, -1.4266e+00, -1.5694e+00, -1.4010e+00,\n",
            "          -1.4662e+00, -1.3808e+00, -1.5649e+00, -1.4551e+00, -1.3037e+00,\n",
            "          -1.2751e+00, -1.5748e+00, -1.4346e+00, -1.3803e+00, -1.4874e+00,\n",
            "          -1.4606e+00, -1.2989e+00, -1.8457e-01, -1.3308e+00,  4.3947e-01,\n",
            "          -1.4842e+00, -1.3401e+00, -7.2099e-01, -7.6247e-01,  4.7217e-01,\n",
            "          -1.4729e+00, -1.3047e+00, -6.1392e-01, -1.6827e+00,  5.4962e+00,\n",
            "          -1.6435e+00,  1.4754e+00,  1.8866e+00,  1.7268e+00,  1.1680e+00,\n",
            "          -1.4134e+00,  2.2458e+00,  2.4336e-01,  1.0342e-01,  3.9350e-01,\n",
            "          -1.2745e+00, -1.3592e+00, -1.2971e+00, -1.4771e+00, -2.1182e-01],\n",
            "         [-6.1031e-01,  7.7803e-01, -4.0693e+00, -3.7784e+00, -4.0389e+00,\n",
            "          -4.1793e+00,  4.9999e+00, -4.0788e+00, -1.4300e-01, -3.9439e+00,\n",
            "          -1.9109e+00, -4.0837e+00, -4.2768e+00, -4.2691e+00, -1.5511e+00,\n",
            "          -3.3197e+00, -3.7926e+00, -3.9632e+00, -3.3786e+00, -4.2406e+00,\n",
            "          -3.9464e+00, -4.0210e+00, -4.2442e+00, -4.2555e+00, -3.9114e+00,\n",
            "          -4.0369e+00, -3.9499e+00, -3.9398e+00, -4.0361e+00, -3.6541e+00,\n",
            "          -4.0916e+00, -4.3642e+00, -4.1057e+00, -3.8872e+00, -4.0450e+00,\n",
            "          -4.0521e+00, -3.9180e+00, -1.0945e+00, -4.0139e+00, -7.4528e-01,\n",
            "          -4.2945e+00, -4.0998e+00,  4.1845e-01,  1.8542e+00, -1.7427e+00,\n",
            "          -3.8379e+00, -2.6109e+00, -1.0449e+00, -4.0430e+00,  1.1243e-01,\n",
            "          -1.9674e+00, -8.5123e-01, -8.6256e-01, -5.2344e-01, -2.2333e+00,\n",
            "          -4.1651e+00,  2.7291e+00, -4.3517e-01, -1.6232e+00, -2.9549e+00,\n",
            "          -4.0241e+00, -4.0798e+00, -4.0179e+00, -3.8228e+00, -1.6886e+00],\n",
            "         [ 7.2321e-01,  3.8913e+00, -4.3546e+00, -4.0040e+00, -4.3554e+00,\n",
            "          -4.1407e+00,  1.5766e+00, -4.3587e+00, -1.4583e+00, -4.0800e+00,\n",
            "          -2.0175e-01, -4.3291e+00, -4.3611e+00, -4.5006e+00, -2.0138e+00,\n",
            "          -2.9109e+00, -4.3711e+00, -4.3356e+00, -3.4577e+00, -4.1828e+00,\n",
            "          -4.3982e+00, -4.4013e+00, -4.2996e+00, -4.4889e+00, -4.3999e+00,\n",
            "          -4.5259e+00, -4.2954e+00, -4.3033e+00, -4.3339e+00, -4.1843e+00,\n",
            "          -4.4259e+00, -4.4142e+00, -4.4615e+00, -4.0406e+00, -4.4943e+00,\n",
            "          -4.2031e+00, -4.1491e+00, -6.4351e-01, -4.0921e+00,  6.1412e-01,\n",
            "          -4.4158e+00, -4.2247e+00, -1.0896e-02, -9.3292e-02, -6.2051e-01,\n",
            "          -4.4473e+00, -3.5047e+00, -1.6223e+00, -4.3871e+00,  2.0058e+00,\n",
            "          -1.7803e+00,  6.0489e-01, -3.1256e-01,  8.9494e-01, -2.6427e+00,\n",
            "          -4.6174e+00,  9.5480e-01,  1.3716e+00, -1.1402e+00, -2.9695e+00,\n",
            "          -4.1851e+00, -4.3603e+00, -4.4181e+00, -4.0510e+00, -2.1160e+00]],\n",
            "\n",
            "        [[-5.7528e-02,  3.0205e-01, -2.1145e+00, -2.1482e+00, -2.3658e+00,\n",
            "          -2.2138e+00, -6.3225e-02, -2.1199e+00, -8.3380e-01, -1.9945e+00,\n",
            "          -1.5916e+00, -2.1294e+00, -2.1306e+00, -2.2367e+00, -2.2363e-01,\n",
            "          -1.4740e+00, -2.0708e+00, -2.2318e+00, -1.0472e+00, -2.2574e+00,\n",
            "          -2.2506e+00, -2.0579e+00, -2.2120e+00, -2.3318e+00, -2.3110e+00,\n",
            "          -2.3422e+00, -2.2150e+00, -2.2246e+00, -2.2775e+00, -2.1502e+00,\n",
            "          -2.3888e+00, -2.3026e+00, -2.3000e+00, -2.2036e+00, -2.3922e+00,\n",
            "          -2.3153e+00, -2.1059e+00,  4.9924e-01, -2.3505e+00,  6.1277e-01,\n",
            "          -2.3435e+00, -2.1578e+00, -1.8328e+00,  4.4044e+00, -8.1303e-01,\n",
            "          -2.2124e+00, -1.4802e+00,  3.2801e-02, -2.4151e+00, -2.8722e-01,\n",
            "          -1.3067e+00, -7.9221e-01, -1.1754e+00,  1.1012e+00, -9.9692e-01,\n",
            "          -2.4944e+00,  1.1910e+00, -8.1853e-01,  4.5697e-01, -1.5738e+00,\n",
            "          -2.1862e+00, -2.1472e+00, -2.2270e+00, -2.2341e+00,  4.2453e-01],\n",
            "         [-1.0119e+00,  3.6270e+00, -2.3719e+00, -2.4622e+00, -2.5334e+00,\n",
            "          -2.4731e+00, -1.2415e+00, -2.4363e+00, -1.7581e+00, -2.4197e+00,\n",
            "          -1.0216e+00, -2.3323e+00, -2.5169e+00, -2.5159e+00, -8.1830e-01,\n",
            "          -7.5838e-01, -2.2951e+00, -2.5585e+00, -1.4915e+00, -2.5365e+00,\n",
            "          -2.5606e+00, -2.3405e+00, -2.4346e+00, -2.4140e+00, -2.7132e+00,\n",
            "          -2.4949e+00, -2.5637e+00, -2.3730e+00, -2.6000e+00, -2.4658e+00,\n",
            "          -2.4123e+00, -2.4953e+00, -2.4632e+00, -2.3972e+00, -2.5859e+00,\n",
            "          -2.4495e+00, -2.5260e+00, -3.5127e-01, -2.5514e+00,  3.1434e+00,\n",
            "          -2.5386e+00, -2.4616e+00,  4.0291e-01,  1.9454e+00,  2.1244e+00,\n",
            "          -2.2695e+00, -1.1665e+00, -3.7391e-01, -2.5283e+00,  7.7130e-01,\n",
            "          -2.3461e+00, -3.4344e-01,  2.4022e-01,  1.6351e+00, -5.2788e-01,\n",
            "          -2.5949e+00, -8.2801e-01,  3.0827e-01, -7.1957e-01, -9.7093e-01,\n",
            "          -2.4945e+00, -2.4995e+00, -2.5362e+00, -2.3203e+00, -1.5252e+00],\n",
            "         [-2.1628e+00,  3.3445e+00, -2.1828e+00, -2.1016e+00, -2.3372e+00,\n",
            "          -2.3077e+00, -6.1317e-01, -2.2464e+00, -1.1512e+00, -2.3514e+00,\n",
            "          -2.8963e+00, -2.2989e+00, -2.3072e+00, -2.4615e+00, -4.0287e-01,\n",
            "          -2.5874e-01, -2.3766e+00, -2.3177e+00, -1.3809e+00, -2.2392e+00,\n",
            "          -2.2381e+00, -2.4014e+00, -2.3290e+00, -2.3926e+00, -2.4219e+00,\n",
            "          -2.4220e+00, -2.3450e+00, -2.3199e+00, -2.6351e+00, -2.2485e+00,\n",
            "          -2.2545e+00, -2.5873e+00, -2.3143e+00, -2.1930e+00, -2.4056e+00,\n",
            "          -2.2935e+00, -2.1989e+00, -4.1897e-01, -2.3897e+00,  1.5568e+00,\n",
            "          -2.3451e+00, -2.2248e+00, -6.2761e-01, -1.8543e-01,  8.9148e-01,\n",
            "          -2.3929e+00, -1.8612e+00, -5.0626e-01, -2.5835e+00,  5.6069e+00,\n",
            "          -2.7029e+00,  9.5173e-01,  1.5673e+00,  2.5660e+00,  1.3762e+00,\n",
            "          -2.2847e+00,  1.6991e+00,  1.0701e+00, -6.7923e-01,  5.5116e-03,\n",
            "          -2.2628e+00, -2.2211e+00, -2.3822e+00, -2.2565e+00, -8.7338e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.2513, -1.2744, -0.6958,  ...,  1.5575, -0.8097,  1.0337],\n",
            "         [-0.3801, -1.0757,  0.6163,  ...,  0.7445,  0.0644,  0.0413],\n",
            "         [ 1.0708,  0.4223,  0.2186,  ...,  1.3680,  1.1397,  0.1528]],\n",
            "\n",
            "        [[-0.5551,  1.3484, -0.7232,  ...,  1.7378, -0.4569, -0.9572],\n",
            "         [ 0.3496, -0.4226,  0.3935,  ..., -0.1834,  0.4135,  2.1728],\n",
            "         [ 1.0708,  0.4223,  0.2186,  ...,  1.3680,  1.1397,  0.1528]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 1.1246e-01, -4.9338e-01, -2.3742e-01,  ...,  2.2773e-01,\n",
            "           3.6445e-02, -1.5901e-01],\n",
            "         [ 3.1494e-01, -3.4351e-01, -5.5526e-01,  ..., -2.1944e-01,\n",
            "          -2.9256e-01,  3.4708e-02],\n",
            "         [ 2.1429e-02, -1.9460e-01, -7.1808e-01,  ..., -5.9200e-03,\n",
            "          -2.3137e-01,  6.5910e-06]],\n",
            "\n",
            "        [[-4.4762e-01, -3.1751e-01,  9.6465e-03,  ..., -4.9532e-03,\n",
            "          -6.0349e-03,  1.9977e-02],\n",
            "         [-1.8712e-02, -6.2243e-02, -6.4300e-02,  ..., -3.8009e-03,\n",
            "           3.6843e-01,  7.5971e-01],\n",
            "         [-9.6514e-02, -1.0467e-02, -4.5434e-01,  ...,  8.8449e-07,\n",
            "           5.4042e-01,  6.3815e-05]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-2.2554e-01, -4.0929e-01, -1.7975e+00, -1.7666e+00, -1.7428e+00,\n",
            "          -1.7993e+00, -6.5929e-01, -1.7450e+00, -1.7553e+00, -1.6144e+00,\n",
            "          -3.4486e-01, -1.7108e+00, -1.7374e+00, -1.9552e+00, -1.1914e+00,\n",
            "          -1.1140e+00, -1.6376e+00, -1.9061e+00, -1.5137e+00, -1.7755e+00,\n",
            "          -1.7822e+00, -1.7633e+00, -1.7634e+00, -1.8841e+00, -2.0344e+00,\n",
            "          -1.8781e+00, -1.6697e+00, -1.7112e+00, -1.7990e+00, -2.0456e+00,\n",
            "          -1.8325e+00, -1.7764e+00, -1.7642e+00, -1.6209e+00, -1.9767e+00,\n",
            "          -1.6860e+00, -1.8187e+00, -1.1189e+00, -1.8724e+00, -5.5661e-01,\n",
            "          -1.9238e+00, -1.8800e+00, -2.4578e-01,  9.6053e-01, -9.4530e-01,\n",
            "          -1.7768e+00, -1.4380e+00, -7.5078e-01, -1.8147e+00, -1.7397e+00,\n",
            "          -6.8086e-01, -7.3537e-01, -2.2061e-01, -4.3300e-01,  8.2411e-01,\n",
            "          -2.0947e+00,  6.3298e-01, -9.2481e-01,  3.1897e+00, -1.0738e+00,\n",
            "          -1.5376e+00, -1.7028e+00, -1.8177e+00, -1.7937e+00,  1.0442e+00],\n",
            "         [-3.9738e-01, -2.5971e-01, -3.7432e+00, -3.9279e+00, -3.9854e+00,\n",
            "          -3.9710e+00, -9.4357e-01, -3.7971e+00, -2.2265e+00, -3.6592e+00,\n",
            "          -1.4286e+00, -3.8644e+00, -4.0164e+00, -3.8981e+00, -1.1258e+00,\n",
            "          -1.9864e+00, -3.6280e+00, -3.8329e+00, -2.0091e+00, -3.8966e+00,\n",
            "          -3.9237e+00, -3.6088e+00, -3.6908e+00, -3.9209e+00, -4.1894e+00,\n",
            "          -3.8465e+00, -3.6371e+00, -3.6928e+00, -3.9456e+00, -4.0217e+00,\n",
            "          -4.1767e+00, -3.8458e+00, -3.8617e+00, -3.7501e+00, -4.1657e+00,\n",
            "          -3.9455e+00, -3.7717e+00, -3.7336e-01, -4.1476e+00,  7.4828e-01,\n",
            "          -3.8630e+00, -4.1298e+00, -2.1454e+00,  4.9412e+00, -1.3913e+00,\n",
            "          -3.8598e+00, -1.6610e+00, -2.6541e-01, -3.9846e+00, -2.4558e+00,\n",
            "          -2.0134e+00, -1.4507e+00, -7.0717e-01,  4.5762e-01, -1.0081e+00,\n",
            "          -4.1924e+00,  2.4758e+00, -7.8620e-02,  1.4996e+00, -3.2427e+00,\n",
            "          -3.6037e+00, -3.7308e+00, -4.0045e+00, -3.5160e+00,  1.0522e+00],\n",
            "         [-1.1614e+00,  3.3755e+00, -3.4175e+00, -3.4923e+00, -3.4039e+00,\n",
            "          -3.5255e+00, -1.4513e+00, -3.4495e+00, -2.4035e+00, -3.4861e+00,\n",
            "          -1.0480e+00, -3.3735e+00, -3.5471e+00, -3.4845e+00, -1.3026e+00,\n",
            "          -1.0637e+00, -3.2916e+00, -3.5765e+00, -2.0733e+00, -3.6161e+00,\n",
            "          -3.5604e+00, -3.4749e+00, -3.4082e+00, -3.4782e+00, -3.7198e+00,\n",
            "          -3.5382e+00, -3.4857e+00, -3.3865e+00, -3.5791e+00, -3.4846e+00,\n",
            "          -3.4627e+00, -3.3511e+00, -3.4917e+00, -3.4208e+00, -3.5764e+00,\n",
            "          -3.4602e+00, -3.4393e+00, -9.8183e-01, -3.5703e+00,  3.2264e+00,\n",
            "          -3.4634e+00, -3.6394e+00,  3.9914e-01,  1.8298e+00,  1.4070e+00,\n",
            "          -3.2367e+00, -1.5610e+00, -1.7174e-01, -3.5311e+00, -1.9778e-01,\n",
            "          -2.8323e+00, -8.1699e-01,  6.9086e-01,  1.1807e+00, -5.1195e-01,\n",
            "          -3.5829e+00, -4.2674e-01,  3.6459e-01, -4.6411e-01, -1.8929e+00,\n",
            "          -3.5135e+00, -3.5041e+00, -3.4772e+00, -3.0589e+00, -1.4823e+00]],\n",
            "\n",
            "        [[-1.7614e+00,  1.1280e+00, -3.1256e+00, -3.2264e+00, -3.2280e+00,\n",
            "          -3.1912e+00, -1.4153e+00, -3.2056e+00, -2.1953e+00, -3.3156e+00,\n",
            "          -1.1922e+00, -3.2525e+00, -3.2038e+00, -3.2752e+00, -1.8823e+00,\n",
            "           1.4977e+00, -3.3518e+00, -3.0540e+00, -2.6662e+00, -3.1308e+00,\n",
            "          -3.1287e+00, -3.3311e+00, -3.1119e+00, -3.1661e+00, -3.4066e+00,\n",
            "          -3.0426e+00, -3.1034e+00, -3.1114e+00, -3.3716e+00, -3.2285e+00,\n",
            "          -3.4166e+00, -3.1293e+00, -3.3791e+00, -3.1950e+00, -3.3494e+00,\n",
            "          -3.2301e+00, -3.1594e+00, -1.6054e+00, -3.3463e+00,  3.7037e+00,\n",
            "          -3.1419e+00, -3.2254e+00, -8.9352e-01, -5.1754e-02,  2.0803e+00,\n",
            "          -3.2585e+00, -1.7123e+00,  4.4641e-03, -3.2369e+00,  4.5044e-01,\n",
            "          -2.9532e+00,  2.4578e+00,  1.9810e+00,  1.9400e-01,  1.8261e+00,\n",
            "          -3.3277e+00,  8.2182e-01,  1.7979e+00, -3.6199e-01, -6.6547e-01,\n",
            "          -3.3542e+00, -3.1230e+00, -3.3247e+00, -2.6131e+00, -1.2081e+00],\n",
            "         [-2.1356e+00,  2.2531e+00, -6.7652e+00, -6.9334e+00, -6.6718e+00,\n",
            "          -6.9261e+00, -4.5246e-01, -6.4882e+00, -3.1775e+00, -6.8324e+00,\n",
            "          -1.5143e+00, -6.7444e+00, -6.8565e+00, -6.7651e+00, -2.5459e+00,\n",
            "          -3.9788e+00, -6.8631e+00, -6.8634e+00, -4.7589e+00, -6.8008e+00,\n",
            "          -6.8120e+00, -6.7915e+00, -6.8656e+00, -6.9364e+00, -6.8335e+00,\n",
            "          -6.9125e+00, -6.6487e+00, -6.7622e+00, -6.6998e+00, -6.6717e+00,\n",
            "          -6.9052e+00, -6.9108e+00, -7.0169e+00, -6.8472e+00, -6.9407e+00,\n",
            "          -6.6641e+00, -6.7316e+00, -2.1465e+00, -7.0711e+00,  3.8881e+00,\n",
            "          -6.8696e+00, -6.6498e+00, -2.4828e+00,  6.1442e+00,  4.4804e-01,\n",
            "          -6.6352e+00, -4.6597e+00, -2.9664e-01, -6.9089e+00, -3.0909e-01,\n",
            "          -3.9240e+00, -1.9336e+00,  1.6936e+00,  5.7510e-01, -2.5968e+00,\n",
            "          -6.9818e+00,  2.8158e+00,  3.5224e-01, -1.1788e+00, -5.1088e+00,\n",
            "          -6.7901e+00, -6.7950e+00, -7.0258e+00, -5.7890e+00, -2.0514e+00],\n",
            "         [-1.9499e+00,  3.4946e+00, -3.9897e+00, -3.8153e+00, -3.7532e+00,\n",
            "          -4.1096e+00, -1.0096e+00, -3.6757e+00, -2.2642e+00, -3.8463e+00,\n",
            "          -1.3567e+00, -3.6819e+00, -3.9285e+00, -3.9298e+00, -1.8341e+00,\n",
            "          -1.9968e+00, -3.8882e+00, -4.1345e+00, -2.9109e+00, -3.8766e+00,\n",
            "          -4.1001e+00, -3.8053e+00, -3.8736e+00, -3.9165e+00, -4.0319e+00,\n",
            "          -3.8916e+00, -3.9032e+00, -4.0077e+00, -3.9003e+00, -3.9241e+00,\n",
            "          -3.8867e+00, -3.9858e+00, -4.0832e+00, -3.8879e+00, -3.9544e+00,\n",
            "          -3.8035e+00, -3.8504e+00, -1.6164e+00, -3.9204e+00,  4.1188e+00,\n",
            "          -4.0025e+00, -3.8613e+00,  4.8079e-01,  1.4225e+00,  1.6967e+00,\n",
            "          -3.7056e+00, -2.7323e+00, -6.5460e-01, -3.9319e+00,  1.2252e+00,\n",
            "          -3.1966e+00, -5.3788e-01,  1.3365e+00,  8.7856e-01, -7.1932e-01,\n",
            "          -4.0702e+00,  7.8786e-01,  4.8481e-01, -1.1740e+00, -2.3910e+00,\n",
            "          -3.9157e+00, -4.0056e+00, -4.0164e+00, -3.1894e+00, -2.2324e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.6018, -0.9145,  0.4012,  ..., -0.9295, -0.2658,  0.1505],\n",
            "         [ 0.6018, -0.9145,  0.4012,  ..., -0.9295, -0.2658,  0.1505],\n",
            "         [ 1.3745, -0.1774, -0.1502,  ...,  0.2397,  0.7188,  0.2112]],\n",
            "\n",
            "        [[ 0.6018, -0.9145,  0.4012,  ..., -0.9295, -0.2658,  0.1505],\n",
            "         [-0.6140,  1.0388,  0.8241,  ..., -0.3903,  0.6056,  1.9683],\n",
            "         [-0.3834, -1.0748,  0.6187,  ...,  0.7397,  0.0629,  0.0416]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 2.1129e-02, -1.0839e-01,  2.4713e-02,  ..., -5.9159e-01,\n",
            "           8.2189e-02,  3.6198e-03],\n",
            "         [ 8.6026e-03, -1.3193e-01,  8.6608e-02,  ..., -8.6568e-01,\n",
            "           5.5590e-02, -4.5176e-04],\n",
            "         [ 6.2126e-02, -2.5393e-01,  6.0226e-01,  ..., -4.9493e-01,\n",
            "           1.4637e-01,  4.8256e-03]],\n",
            "\n",
            "        [[ 2.1129e-02, -1.0839e-01,  2.4713e-02,  ..., -5.9159e-01,\n",
            "           8.2189e-02,  3.6198e-03],\n",
            "         [ 8.3485e-02, -1.7987e-01,  5.4259e-01,  ..., -5.3473e-01,\n",
            "           9.1158e-02, -1.9464e-02],\n",
            "         [ 3.9245e-01, -1.0824e-02,  1.7957e-01,  ..., -2.6849e-01,\n",
            "           1.0350e-01,  2.2222e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 2.0974e+00,  2.3281e-01, -7.0998e-01, -5.6458e-01, -6.8199e-01,\n",
            "          -6.6940e-01, -2.7039e-01, -8.6000e-01,  1.9116e-01, -8.1428e-01,\n",
            "          -1.1973e+00, -8.5628e-01, -7.1266e-01, -7.1564e-01,  2.5177e+00,\n",
            "          -7.2179e-02, -5.8028e-01, -8.0205e-01,  3.3228e+00, -6.6253e-01,\n",
            "          -7.6646e-01, -5.0694e-01, -7.4518e-01, -6.4335e-01, -7.1585e-01,\n",
            "          -7.0859e-01, -8.0211e-01, -5.7316e-01, -6.8855e-01, -7.6769e-01,\n",
            "          -8.2403e-01, -8.5916e-01, -7.5674e-01, -5.9399e-01, -6.9154e-01,\n",
            "          -5.8299e-01, -6.3381e-01,  1.5416e+00, -8.7091e-01, -5.2953e-01,\n",
            "          -8.3718e-01, -5.9832e-01, -5.9124e-01,  1.7102e+00, -3.9478e-01,\n",
            "          -7.7806e-01,  6.5149e-01,  1.1556e+00, -7.8370e-01, -8.2286e-01,\n",
            "          -7.9745e-01, -8.5906e-01, -4.9914e-01,  6.6380e-01, -1.9944e-01,\n",
            "          -9.1696e-01, -2.3730e-01, -7.4088e-01, -1.6145e-01,  1.3096e-01,\n",
            "          -4.7346e-01, -5.8223e-01, -8.4562e-01, -9.2666e-01, -2.1724e-01],\n",
            "         [ 1.6582e+00, -9.2093e-02, -7.4569e-01, -4.5929e-01, -7.8475e-01,\n",
            "          -6.7667e-01, -2.7798e-01, -9.7873e-01,  1.6445e-01, -7.8265e-01,\n",
            "          -1.4915e+00, -8.5978e-01, -6.9011e-01, -8.8945e-01,  2.7483e+00,\n",
            "           2.5341e-01, -5.7002e-01, -9.6022e-01,  4.3389e+00, -8.1344e-01,\n",
            "          -7.2176e-01, -5.8654e-01, -9.1163e-01, -5.3658e-01, -7.0376e-01,\n",
            "          -8.6254e-01, -7.9538e-01, -5.6186e-01, -6.8412e-01, -7.9760e-01,\n",
            "          -8.3671e-01, -9.2499e-01, -8.4222e-01, -5.5842e-01, -6.8073e-01,\n",
            "          -6.1872e-01, -6.6571e-01,  9.9955e-01, -9.6054e-01, -7.8591e-01,\n",
            "          -9.4831e-01, -5.2004e-01, -6.9344e-01,  1.6835e+00, -6.5270e-01,\n",
            "          -8.3636e-01,  9.6729e-01,  2.9989e+00, -8.4547e-01, -1.3169e+00,\n",
            "          -5.2718e-01, -1.2518e+00, -1.0819e+00,  4.8386e-01,  6.3818e-02,\n",
            "          -9.7468e-01, -2.7921e-01, -1.6466e+00,  1.6476e-03,  4.0164e-01,\n",
            "          -5.3069e-01, -6.9350e-01, -8.3013e-01, -9.5665e-01,  3.1041e-01],\n",
            "         [-8.0570e-01, -6.4356e-01, -3.2554e-01, -2.3725e-01, -4.4788e-01,\n",
            "          -3.9679e-01, -6.6169e-01, -4.2550e-01, -2.9264e-01, -4.7346e-01,\n",
            "          -8.5238e-01, -4.0962e-01, -2.0993e-01, -3.0632e-01,  9.2624e-01,\n",
            "           1.1671e+00, -2.5064e-01, -5.0177e-01,  2.6566e+00, -4.0339e-01,\n",
            "          -5.2433e-01, -3.4977e-01, -3.6883e-01, -1.9768e-01, -1.4171e-01,\n",
            "          -3.1599e-01, -2.5414e-01, -6.0456e-02, -3.2137e-01, -4.8711e-01,\n",
            "          -3.2844e-01, -4.2452e-01, -2.9485e-01, -2.6453e-01, -4.4980e-01,\n",
            "          -3.6472e-01, -2.2676e-01, -1.2510e+00, -3.8376e-01,  1.4236e-01,\n",
            "          -4.0671e-01, -1.1356e-01, -7.0165e-01,  7.5240e-01,  1.4616e-01,\n",
            "          -4.1133e-01,  1.2289e+00,  4.0960e+00, -4.8627e-01, -1.6193e+00,\n",
            "          -1.5406e-01, -1.1565e+00, -2.8797e-01, -5.3185e-01,  9.4925e-01,\n",
            "          -5.7392e-01,  4.0818e-01, -1.6584e+00,  3.5636e-01,  3.7310e-01,\n",
            "          -1.9950e-01, -4.5251e-01, -3.2563e-01, -5.2722e-01,  7.9790e-01]],\n",
            "\n",
            "        [[ 2.0974e+00,  2.3281e-01, -7.0998e-01, -5.6458e-01, -6.8199e-01,\n",
            "          -6.6940e-01, -2.7039e-01, -8.6000e-01,  1.9116e-01, -8.1428e-01,\n",
            "          -1.1973e+00, -8.5628e-01, -7.1266e-01, -7.1564e-01,  2.5177e+00,\n",
            "          -7.2179e-02, -5.8028e-01, -8.0205e-01,  3.3228e+00, -6.6253e-01,\n",
            "          -7.6646e-01, -5.0694e-01, -7.4518e-01, -6.4335e-01, -7.1585e-01,\n",
            "          -7.0859e-01, -8.0211e-01, -5.7316e-01, -6.8855e-01, -7.6769e-01,\n",
            "          -8.2403e-01, -8.5916e-01, -7.5674e-01, -5.9399e-01, -6.9154e-01,\n",
            "          -5.8299e-01, -6.3381e-01,  1.5416e+00, -8.7091e-01, -5.2953e-01,\n",
            "          -8.3718e-01, -5.9832e-01, -5.9124e-01,  1.7102e+00, -3.9478e-01,\n",
            "          -7.7806e-01,  6.5149e-01,  1.1556e+00, -7.8370e-01, -8.2286e-01,\n",
            "          -7.9745e-01, -8.5906e-01, -4.9914e-01,  6.6380e-01, -1.9944e-01,\n",
            "          -9.1696e-01, -2.3730e-01, -7.4088e-01, -1.6145e-01,  1.3096e-01,\n",
            "          -4.7346e-01, -5.8223e-01, -8.4562e-01, -9.2666e-01, -2.1724e-01],\n",
            "         [-6.6155e-01, -6.7574e-01,  2.1272e-01,  5.0627e-02,  1.1268e-01,\n",
            "          -4.1809e-02, -1.3026e-01,  9.2048e-02, -1.4394e-01, -4.7721e-02,\n",
            "          -1.7226e-01,  8.6193e-02,  1.0971e-01, -4.1820e-03,  3.6697e-01,\n",
            "           8.1316e-01,  1.8181e-01, -9.5886e-02,  2.2288e+00, -1.6342e-01,\n",
            "           4.3706e-02,  7.5028e-02, -3.1436e-02,  1.7258e-01,  1.2138e-01,\n",
            "          -9.4098e-02,  3.7914e-02,  8.3314e-02,  7.7674e-02, -1.3866e-02,\n",
            "           5.6496e-02, -2.0177e-01,  6.9981e-02,  1.3048e-01,  8.8382e-02,\n",
            "           5.0692e-02,  7.7095e-02, -1.5560e+00, -1.2122e-01,  1.4290e-01,\n",
            "           1.9136e-01,  2.3505e-01, -1.6808e-01,  9.4033e-01,  5.3873e-02,\n",
            "           3.6490e-02,  2.1881e-01,  2.7556e+00, -1.1706e-01, -1.1724e+00,\n",
            "           7.9720e-01, -1.4045e+00, -3.7055e-01,  7.2424e-02,  4.3985e-01,\n",
            "          -1.1597e-01, -5.3247e-01, -1.7685e+00,  1.5977e-01,  6.1201e-01,\n",
            "           1.5409e-03, -3.7634e-02,  2.3550e-03, -3.9170e-01,  1.6559e-01],\n",
            "         [-1.3971e+00, -7.3895e-01, -5.4864e+00, -5.6553e+00, -5.5747e+00,\n",
            "          -5.6912e+00, -3.1122e-01, -5.5603e+00, -2.3119e+00, -5.2431e+00,\n",
            "          -1.4840e+00, -5.3680e+00, -5.4166e+00, -5.6024e+00, -2.9074e+00,\n",
            "          -4.0017e+00, -5.5128e+00, -5.6218e+00, -2.5554e+00, -5.8115e+00,\n",
            "          -5.7636e+00, -5.4256e+00, -5.6113e+00, -5.5721e+00, -5.6048e+00,\n",
            "          -5.6684e+00, -5.4771e+00, -5.5399e+00, -5.6401e+00, -5.6060e+00,\n",
            "          -5.8121e+00, -5.5702e+00, -5.5241e+00, -5.3840e+00, -5.8271e+00,\n",
            "          -5.7725e+00, -5.4107e+00, -2.6392e+00, -5.8212e+00,  1.6303e+00,\n",
            "          -5.6961e+00, -5.3442e+00, -2.7983e+00,  6.5030e+00, -1.5294e+00,\n",
            "          -5.5006e+00, -4.3960e+00,  1.1347e+00, -5.8267e+00, -1.8107e+00,\n",
            "          -2.5045e+00, -3.4497e+00, -1.1405e+00, -8.8322e-02, -3.3090e+00,\n",
            "          -5.8760e+00,  3.1930e+00, -2.0904e+00,  3.6231e-01, -4.7241e+00,\n",
            "          -5.5678e+00, -5.5114e+00, -5.4526e+00, -4.7635e+00,  6.2414e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.3860, -1.0741,  0.6211,  ...,  0.7353,  0.0616,  0.0422],\n",
            "         [-0.4553, -0.2620, -0.5896,  ...,  2.3266, -1.6457,  2.3090],\n",
            "         [-0.4367,  1.0121,  0.5903,  ..., -0.3297, -2.8305, -1.8880]],\n",
            "\n",
            "        [[ 1.6350,  1.1331, -1.6991,  ...,  0.3862,  1.2734, -1.1791],\n",
            "         [ 0.2174,  1.6985,  1.6038,  ..., -1.4704,  1.5600,  0.0519],\n",
            "         [-0.4553, -0.2620, -0.5896,  ...,  2.3266, -1.6457,  2.3090]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 2.5220e-01,  5.9564e-02, -2.4028e-01,  ..., -4.9028e-01,\n",
            "           2.2239e-02,  7.1726e-02],\n",
            "         [ 6.7314e-01, -7.0945e-01, -1.8978e-01,  ..., -1.9086e-01,\n",
            "          -7.1178e-01, -2.3231e-04],\n",
            "         [ 1.1593e-01, -6.7785e-01, -2.0678e-02,  ..., -3.0480e-01,\n",
            "           3.5086e-02,  3.1903e-02]],\n",
            "\n",
            "        [[-9.2176e-04, -6.8843e-02,  3.5357e-01,  ..., -1.5069e-03,\n",
            "          -1.9301e-01,  4.1376e-02],\n",
            "         [-1.9974e-03, -2.6998e-04,  7.1214e-02,  ...,  7.6242e-04,\n",
            "          -1.6353e-01,  1.7244e-01],\n",
            "         [ 2.0760e-01, -6.8940e-01,  1.8828e-01,  ...,  7.0130e-05,\n",
            "          -9.1411e-01,  1.5708e-03]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.8309, -0.5676, -4.1787, -4.1054, -4.2383, -4.2081, -0.2491,\n",
            "          -4.0940, -1.4055, -3.9424, -2.1563, -4.1136, -4.1352, -3.8479,\n",
            "          -1.6336, -2.5229, -4.0643, -4.1622, -1.6327, -4.2348, -4.2537,\n",
            "          -4.1201, -4.1724, -4.2868, -4.2326, -4.3089, -4.1285, -4.1964,\n",
            "          -4.2322, -4.1109, -4.4280, -3.8643, -4.2819, -4.1930, -4.4096,\n",
            "          -4.2720, -3.9818, -0.8562, -4.3147,  0.8171, -4.2946, -4.1367,\n",
            "          -2.3502,  5.6250, -1.7650, -4.2272, -2.7410, -0.1860, -4.4008,\n",
            "          -0.7759, -2.2342, -2.0040, -0.7738,  0.7061, -0.9753, -4.4742,\n",
            "           4.0670, -1.6972,  0.2607, -2.9713, -4.1967, -4.0718, -4.1579,\n",
            "          -3.1443, -0.1707],\n",
            "         [-0.9434,  1.7709, -4.3937, -4.3904, -4.5301, -4.6512, -2.3989,\n",
            "          -4.5143, -3.0051, -4.4112, -1.6458, -4.5935, -4.6642, -4.2810,\n",
            "          -0.8362, -1.3607, -4.3832, -4.4673, -1.6601, -4.6302, -4.5943,\n",
            "          -4.6319, -4.4917, -4.6767, -4.9437, -4.7005, -4.3592, -4.7558,\n",
            "          -4.5964, -4.5014, -4.5848, -4.1030, -4.9639, -4.6163, -4.8183,\n",
            "          -4.6575, -4.6683,  0.1473, -4.6068,  2.0493, -4.4910, -4.6087,\n",
            "          -1.9859,  3.0990, -0.6924, -4.5822,  0.1637, -0.5126, -4.8619,\n",
            "          -0.5173, -3.3551, -0.6451, -0.3729,  4.7179,  0.9516, -4.4698,\n",
            "           1.1943,  0.9354,  1.5630, -2.0497, -4.4416, -4.4062, -4.6116,\n",
            "          -3.3971, -0.9144],\n",
            "         [-1.4692, -0.1618, -2.3355, -2.0425, -2.3604, -2.0811, -0.4064,\n",
            "          -2.3930, -0.9612, -2.1884, -1.8156, -2.4062, -2.3395, -2.1883,\n",
            "          -0.0254,  0.1314, -2.2754, -2.0273, -0.8074, -2.2388, -2.3616,\n",
            "          -2.4482, -2.2408, -2.1683, -2.2994, -2.1600, -2.1017, -2.2765,\n",
            "          -2.2879, -2.3769, -2.3499, -1.8418, -2.2906, -2.0216, -2.3563,\n",
            "          -2.2224, -2.0191, -0.6049, -2.2386,  0.8393, -2.2339, -2.3241,\n",
            "          -0.4990,  0.7404, -0.6792, -2.4034,  2.0710, -1.4221, -2.3386,\n",
            "          -1.0503, -1.6082,  0.0916,  0.4907,  1.4070,  2.5123, -2.4266,\n",
            "           3.8199, -0.6316, -0.0363, -0.6387, -2.1441, -2.0257, -2.2917,\n",
            "          -1.2809, -0.2989]],\n",
            "\n",
            "        [[-1.3986,  0.9346, -3.2610, -3.0839, -3.3653, -3.5275, -0.6464,\n",
            "          -3.2723, -0.9629, -3.2763, -0.9195, -3.4265, -3.4382, -2.7763,\n",
            "          -1.2806, -0.9074, -3.2420, -3.3320, -1.5240, -3.3443, -3.3260,\n",
            "          -3.2523, -3.2919, -3.3537, -3.4089, -3.4479, -3.2508, -3.2893,\n",
            "          -3.2322, -3.3635, -3.3862, -3.0346, -3.4042, -3.3024, -3.2676,\n",
            "          -3.2478, -3.3130, -1.4556, -3.3262,  1.0710, -3.2579, -3.1701,\n",
            "          -0.8217,  1.6301, -0.3650, -3.2643, -0.5860,  3.4675, -3.4257,\n",
            "          -0.6714, -1.4649, -0.8602,  0.0719,  0.0943,  0.1843, -3.4201,\n",
            "           0.1604, -0.6381, -0.2997, -1.8853, -3.3162, -3.3121, -3.3019,\n",
            "          -2.3118, -0.3402],\n",
            "         [-1.9889, -1.0560, -4.3161, -4.3474, -4.4905, -4.5689, -0.8701,\n",
            "          -4.5671, -2.2496, -4.3246, -1.5456, -4.2904, -4.3628, -4.6150,\n",
            "          -2.4615, -2.5812, -4.5753, -4.5857, -3.8063, -4.2017, -4.7650,\n",
            "          -4.3888, -4.4270, -4.1246, -4.4463, -4.3507, -4.1799, -4.3772,\n",
            "          -4.2546, -4.2443, -4.5036, -4.5024, -4.3191, -4.2903, -4.4526,\n",
            "          -4.5140, -4.1114, -1.5436, -4.5566, -0.0945, -4.5413, -4.3213,\n",
            "          -1.2900,  1.2257, -2.0244, -4.4355, -1.6542, -0.3041, -4.5141,\n",
            "          -1.7967, -1.9969, -1.8670,  1.0972, -1.9513, -1.5084, -4.4026,\n",
            "           5.6014, -0.3799,  0.7033, -4.6351, -4.0614, -4.2611, -4.4014,\n",
            "          -3.5050,  4.5134],\n",
            "         [-1.4217,  1.7101, -5.2851, -5.2731, -5.4335, -5.6892, -2.0513,\n",
            "          -5.4935, -3.6852, -5.4594, -0.4680, -5.6395, -5.3981, -5.2639,\n",
            "          -2.0475, -0.5418, -5.4708, -5.4229, -3.2740, -5.5232, -5.6220,\n",
            "          -5.4931, -5.2707, -5.4055, -5.5510, -5.4212, -5.1839, -5.3852,\n",
            "          -5.3703, -5.4698, -5.6748, -5.2308, -5.6934, -5.3869, -5.4778,\n",
            "          -5.5620, -5.4055, -0.5657, -5.6130,  0.4626, -5.4394, -5.3274,\n",
            "          -2.0297,  3.7080, -1.5261, -5.5835, -0.0502, -0.0125, -5.3952,\n",
            "          -3.1504, -2.5691, -0.8244, -0.0657,  1.5819, -1.0380, -5.5060,\n",
            "           0.5062,  4.1291,  0.9428, -4.1890, -5.3554, -5.2318, -5.3066,\n",
            "          -4.4106,  0.1535]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.7725, -0.9984,  0.2914,  ...,  1.9401, -1.1098, -0.3683],\n",
            "         [ 0.3562, -0.4373,  0.4058,  ..., -0.1972,  0.4182,  2.1863],\n",
            "         [ 1.0570,  0.4172,  0.2114,  ...,  1.3496,  1.1281,  0.1484]],\n",
            "\n",
            "        [[-0.4609, -0.2657, -0.5909,  ...,  2.3205, -1.6478,  2.3122],\n",
            "         [-0.2624, -1.2852, -0.7028,  ...,  1.5566, -0.7978,  1.0290],\n",
            "         [ 0.7725, -0.9984,  0.2914,  ...,  1.9401, -1.1098, -0.3683]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-1.7867e-03, -7.7580e-04, -1.6855e-03,  ..., -2.3115e-02,\n",
            "          -3.7378e-01,  2.7208e-04],\n",
            "         [ 1.4062e-05, -1.1218e-03, -6.6272e-02,  ..., -5.4035e-02,\n",
            "           9.9971e-02,  2.5533e-01],\n",
            "         [-4.3816e-02, -1.2778e-01, -7.1342e-01,  ..., -3.4687e-04,\n",
            "           1.8856e-01,  8.9776e-04]],\n",
            "\n",
            "        [[ 6.7534e-01, -7.3982e-01,  6.4701e-03,  ..., -2.6760e-02,\n",
            "          -7.2427e-01, -8.7068e-03],\n",
            "         [ 9.7999e-02, -9.4277e-01, -1.4177e-01,  ...,  1.1288e-01,\n",
            "          -2.4032e-02, -2.3956e-01],\n",
            "         [ 6.3680e-01, -3.1548e-01, -1.9679e-01,  ...,  9.6032e-02,\n",
            "          -5.0616e-02, -1.3891e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.6466,  6.6306, -3.6590, -3.8067, -3.6279, -3.6733, -0.0410,\n",
            "          -3.7087, -0.5300, -3.6401, -0.2157, -3.6827, -3.8285, -3.2041,\n",
            "          -1.0163,  0.1803, -4.0532, -3.6286, -1.2379, -3.5499, -3.6195,\n",
            "          -3.7290, -3.7901, -3.8212, -3.8128, -3.7483, -3.6914, -3.6471,\n",
            "          -3.7383, -3.8033, -3.6552, -3.0618, -3.7349, -3.6401, -3.9080,\n",
            "          -3.7838, -3.5813, -0.1517, -3.7450,  0.3246, -3.6978, -3.7642,\n",
            "          -0.2635,  1.1386, -0.3281, -3.7525,  2.6918, -2.1545, -3.8953,\n",
            "          -1.3382, -1.8509, -2.0601, -0.4042, -0.3320, -1.7229, -3.8416,\n",
            "          -0.8343,  0.8869, -2.9904, -3.4648, -3.5894, -3.6654, -3.7413,\n",
            "          -2.1786, -1.5529],\n",
            "         [-2.1560,  2.8290, -3.6145, -3.5316, -3.8039, -3.7574, -0.4040,\n",
            "          -3.4717, -1.0882, -3.5978, -1.0463, -3.5774, -3.5697, -2.8026,\n",
            "          -1.0092,  1.9406, -3.5421, -3.4769, -0.9302, -3.5950, -3.7639,\n",
            "          -3.6924, -3.4880, -3.8683, -3.6245, -3.8042, -3.3544, -3.8112,\n",
            "          -3.6680, -3.5265, -3.5978, -2.9898, -4.0503, -3.8106, -3.5733,\n",
            "          -3.6395, -3.7996, -1.5129, -3.7443,  2.1532, -3.4293, -3.2973,\n",
            "          -0.9942,  3.9083, -0.2410, -3.7158, -1.7433,  0.0885, -3.7679,\n",
            "           0.0485, -1.4371, -2.1633,  0.5296,  0.8467, -0.1873, -3.7179,\n",
            "           0.4726, -0.6215, -1.5903, -1.9692, -3.5522, -3.5456, -3.7864,\n",
            "          -2.0891, -2.2056],\n",
            "         [-1.7369,  4.1792, -5.1437, -5.0406, -4.9705, -5.0859, -1.5343,\n",
            "          -4.9248, -2.7170, -4.9578, -1.2696, -4.6839, -4.9469, -4.6534,\n",
            "          -2.7910, -1.2841, -4.9836, -5.1171, -3.2742, -5.0826, -5.1636,\n",
            "          -4.9421, -4.9299, -4.9565, -5.0421, -4.9588, -5.1432, -5.0873,\n",
            "          -4.8692, -5.0231, -5.1020, -4.7894, -5.0997, -5.0452, -5.0318,\n",
            "          -4.8243, -5.0603, -2.3995, -5.1354,  6.0702, -5.0132, -4.5525,\n",
            "           1.3035,  1.1417,  1.7014, -4.9131, -2.5454, -0.5074, -4.9722,\n",
            "           0.5085, -3.8623, -2.0875,  1.9361,  0.2688, -0.0086, -5.0158,\n",
            "          -0.4266, -0.4015, -1.4751, -3.0238, -5.1129, -5.0817, -5.0780,\n",
            "          -3.4180, -3.0953]],\n",
            "\n",
            "        [[-0.2929,  1.7928, -4.0668, -4.1476, -4.2525, -4.4636, -2.1111,\n",
            "          -4.2799, -3.0752, -4.1677, -0.5402, -4.3144, -4.3076, -3.9319,\n",
            "          -1.1317, -1.3735, -4.1747, -4.2168, -2.0960, -4.2730, -4.3820,\n",
            "          -4.2128, -4.2121, -4.3040, -4.3364, -4.5029, -4.1671, -4.4415,\n",
            "          -4.0954, -4.2551, -4.5404, -3.8685, -4.4650, -4.3289, -4.3403,\n",
            "          -4.2268, -4.2832,  0.0130, -4.3511, -0.2242, -4.3458, -3.7816,\n",
            "          -2.0359,  2.4537, -1.7631, -4.3604,  0.7675, -1.2570, -4.4030,\n",
            "          -1.6002, -2.3588,  0.0295, -0.5778,  3.3029,  0.1094, -4.1789,\n",
            "           0.2460,  2.4548,  3.0179, -2.1893, -4.1252, -4.0447, -4.2847,\n",
            "          -2.9102,  0.1559],\n",
            "         [-1.6108,  0.3560,  0.7988,  0.8031,  0.5307,  0.6469, -2.0210,\n",
            "           0.5203, -1.1879,  0.8283, -0.9988,  0.5772,  0.5774,  0.7310,\n",
            "           1.8352,  3.2309,  0.7533,  0.8108,  1.4996,  0.8319,  0.6884,\n",
            "           0.4635,  0.6085,  0.7205,  0.4558,  0.4652,  0.8160,  0.4695,\n",
            "           0.4732,  0.6203,  0.5367,  0.8888,  0.6127,  0.6511,  0.5378,\n",
            "           0.7515,  0.5759,  0.1896,  0.6895,  0.3729,  0.4524,  1.6448,\n",
            "          -0.3819, -0.0409,  0.0840,  0.6930,  5.5782, -2.7498,  0.6071,\n",
            "          -1.8025, -0.2815,  1.2764,  0.3128,  1.8849,  6.4077,  0.3800,\n",
            "           0.6930, -0.3049,  2.5482,  2.6909,  0.7718,  0.7278,  0.7152,\n",
            "           0.3912,  0.1453],\n",
            "         [-1.5491,  6.3574, -0.4872, -0.2360, -0.4438, -0.2182, -1.0551,\n",
            "          -0.5380,  0.4093, -0.4508, -0.2467, -0.5841, -0.3946, -0.0484,\n",
            "           1.1273,  3.6324, -0.4528, -0.3557,  1.3032, -0.3212, -0.3240,\n",
            "          -0.6923, -0.5631, -0.3376, -0.2949, -0.4515, -0.4535, -0.2882,\n",
            "          -0.4848, -0.2928, -0.4897, -0.0299, -0.3551, -0.4176, -0.6402,\n",
            "          -0.2309, -0.5954,  0.0618, -0.3392,  0.7129, -0.4501,  0.2303,\n",
            "           0.0466,  0.8550,  0.6324, -0.4099,  7.8825, -4.2274, -0.4866,\n",
            "          -1.8824, -0.7024, -0.7795,  0.0216,  0.7593,  2.3552, -0.4178,\n",
            "          -0.6825,  0.5862, -1.7227,  0.3226, -0.3027, -0.1871, -0.4887,\n",
            "           0.0234, -2.0332]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.5554,  1.3622, -0.7288,  ...,  1.7477, -0.4653, -0.9580],\n",
            "         [ 1.0438,  1.4438, -0.5157,  ...,  1.3842,  1.5355,  1.4617],\n",
            "         [ 0.2092,  1.7050,  1.5992,  ..., -1.4748,  1.5607,  0.0450]],\n",
            "\n",
            "        [[-0.4284,  1.0134,  0.5974,  ..., -0.3377, -2.8298, -1.8944],\n",
            "         [-0.4598, -0.2621, -0.5872,  ...,  2.3220, -1.6499,  2.3147],\n",
            "         [ 1.0518,  0.4200,  0.2070,  ...,  1.3439,  1.1242,  0.1438]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-1.6495e-01, -1.5290e-01,  1.0749e-03,  ..., -1.5061e-03,\n",
            "          -6.0896e-03,  2.8474e-02],\n",
            "         [-6.7796e-01, -1.7929e-01,  1.7831e-01,  ..., -2.1987e-03,\n",
            "          -5.9253e-01,  8.3093e-01],\n",
            "         [-5.7108e-05, -1.8900e-04,  1.9144e-01,  ...,  2.3679e-05,\n",
            "          -3.1009e-01,  9.1153e-02]],\n",
            "\n",
            "        [[-3.8586e-01,  5.8694e-02, -4.1362e-02,  ...,  5.4064e-02,\n",
            "          -3.2601e-03,  4.3279e-02],\n",
            "         [ 5.8719e-01, -5.0076e-01, -2.8340e-02,  ...,  1.3437e-02,\n",
            "          -7.5846e-01, -4.8824e-05],\n",
            "         [ 1.8879e-02, -4.1047e-01, -6.7539e-01,  ...,  3.2926e-03,\n",
            "          -6.4373e-01,  5.7114e-06]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.9974,  0.5127, -3.1890, -3.2157, -3.2146, -3.3126, -1.8326,\n",
            "          -3.2698, -2.0457, -3.3094, -1.4104, -3.3251, -3.3343, -2.9771,\n",
            "          -1.9561,  2.4621, -3.4178, -3.1421, -2.2454, -3.3149, -3.3021,\n",
            "          -3.4636, -3.1938, -3.2365, -3.3944, -3.1688, -3.1888, -3.1193,\n",
            "          -3.3853, -3.3607, -3.4984, -3.0829, -3.3522, -3.2061, -3.4191,\n",
            "          -3.2538, -3.2559, -1.5271, -3.3369,  2.7920, -3.1280, -2.5463,\n",
            "          -0.9567, -0.7793,  2.5512, -3.2283,  1.2671,  0.3283, -3.2885,\n",
            "          -0.5715, -2.7723,  1.4009,  1.8283,  0.1733,  3.4392, -3.3115,\n",
            "           0.3773,  1.5691, -0.3064, -0.0092, -3.4243, -3.2090, -3.3131,\n",
            "          -1.6513, -1.2439],\n",
            "         [-1.7362, -1.0912, -4.7041, -4.4558, -4.7537, -4.8670,  0.0782,\n",
            "          -4.8982, -1.0807, -4.5825, -0.8612, -4.8886, -4.8236, -4.4198,\n",
            "          -1.7171, -2.5424, -4.9945, -4.9257, -3.3617, -4.7009, -4.9855,\n",
            "          -4.7706, -4.8016, -4.9664, -4.6818, -4.7436, -4.6682, -4.8119,\n",
            "          -4.6967, -4.6977, -4.8619, -4.3879, -4.9006, -4.4209, -4.7669,\n",
            "          -4.9105, -4.5633, -1.1928, -4.8628, -0.7784, -4.9657, -4.0186,\n",
            "          -1.2481,  2.7015, -1.8764, -4.6989,  2.5550,  5.5630, -4.7398,\n",
            "          -5.3743, -0.5711, -1.5978,  0.3462, -2.4006, -0.5174, -4.7780,\n",
            "           2.7116, -0.6872,  0.1115, -3.4458, -4.5911, -4.7887, -4.8090,\n",
            "          -2.4770, -0.2912],\n",
            "         [-1.5899, -2.2643, -2.8529, -2.9551, -2.9690, -3.2091, -1.5090,\n",
            "          -3.1897, -2.7547, -2.9823, -0.5532, -2.9494, -3.0397, -3.0784,\n",
            "          -1.6375, -1.6338, -3.0348, -3.1453, -2.8852, -2.7242, -3.3599,\n",
            "          -2.9676, -2.9549, -2.6981, -3.1151, -2.8211, -3.0737, -3.0423,\n",
            "          -2.9158, -3.1128, -3.1161, -2.8655, -3.1488, -3.0117, -2.9393,\n",
            "          -3.1997, -2.9890, -0.8035, -3.0446, -0.3672, -2.8152, -2.3989,\n",
            "          -1.6055,  1.5361, -1.8206, -2.8596, -1.1907,  1.7516, -3.0038,\n",
            "          -2.3649, -0.7831, -1.1299,  0.9887, -0.8139, -0.4701, -3.2414,\n",
            "           3.3279,  0.0697,  3.1067, -2.6585, -2.8147, -3.1215, -3.0287,\n",
            "          -2.2156,  2.0789]],\n",
            "\n",
            "        [[-0.2026, -0.7880, -4.6696, -4.5252, -4.6609, -4.5978,  1.3631,\n",
            "          -4.7596, -1.1077, -4.5225, -1.1865, -4.6083, -4.6896, -4.7620,\n",
            "          -2.6889, -3.9836, -4.7126, -4.6328, -3.5515, -4.3348, -4.7308,\n",
            "          -4.5678, -4.6065, -4.5238, -4.5266, -4.5013, -4.3643, -4.5275,\n",
            "          -4.5697, -4.6299, -4.6032, -4.6484, -4.5954, -4.5558, -4.6989,\n",
            "          -4.4562, -4.3033, -1.1035, -4.6428, -0.5121, -4.7217, -3.9707,\n",
            "          -0.1644,  1.1315, -1.5472, -4.5749, -2.7654, -0.6836, -4.5321,\n",
            "          -1.0386, -2.4397, -1.9099,  0.3231, -1.1868, -2.6194, -4.7895,\n",
            "           4.7748, -0.4954, -1.6142, -4.0218, -4.5198, -4.4396, -4.7054,\n",
            "          -2.5942,  0.4198],\n",
            "         [-0.3234,  1.6744, -5.7150, -5.8189, -5.9147, -6.1068, -1.5639,\n",
            "          -5.9650, -3.7631, -5.8236, -0.4328, -5.8782, -5.9054, -5.4204,\n",
            "          -2.3191, -2.9096, -5.9298, -5.8849, -3.2382, -5.8492, -6.0744,\n",
            "          -5.9458, -5.8947, -5.9342, -5.9163, -6.1188, -5.7052, -6.1098,\n",
            "          -5.8026, -5.9750, -6.2061, -5.3747, -6.2659, -5.9090, -5.9034,\n",
            "          -5.8879, -5.8787, -0.8156, -6.1206,  0.1512, -6.1010, -4.9316,\n",
            "          -2.0638,  4.0125, -1.8736, -6.0604, -1.1331, -1.1782, -5.8957,\n",
            "          -2.2998, -2.6286, -1.1872, -0.4508,  2.2790, -1.4218, -5.8965,\n",
            "           0.9671,  2.8101,  1.6222, -3.6646, -5.7004, -5.7918, -5.7996,\n",
            "          -3.7675, -0.8820],\n",
            "         [-1.0334,  3.2265, -4.0456, -3.9500, -3.9574, -4.1825, -1.8672,\n",
            "          -4.0934, -2.8211, -4.0847, -0.7934, -3.7684, -3.9236, -3.8312,\n",
            "          -1.9941, -1.1787, -3.9268, -4.0708, -2.5600, -3.9622, -4.0478,\n",
            "          -3.9854, -3.9470, -4.0630, -4.1631, -4.1255, -4.0878, -4.1418,\n",
            "          -4.1732, -3.9845, -4.1268, -3.7046, -4.0274, -4.0722, -4.0906,\n",
            "          -3.8609, -3.9580, -1.6990, -4.1039,  4.1128, -4.1301, -3.3064,\n",
            "           0.8090,  0.1816,  1.0633, -3.9249, -1.1884, -0.8437, -4.0504,\n",
            "          -0.3846, -3.0804, -1.0454,  1.6899,  0.4481,  0.2116, -3.9963,\n",
            "          -0.2978,  0.2352, -0.3481, -2.0438, -4.0992, -3.9819, -3.9929,\n",
            "          -2.5965, -2.3247]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.1758, -0.5869,  0.4824,  ...,  1.0388,  1.2493, -1.0861],\n",
            "         [ 1.0479,  0.4214,  0.2036,  ...,  1.3395,  1.1203,  0.1401],\n",
            "         [-0.6880,  0.7835, -0.4938,  ..., -0.3975,  0.7760, -0.2305]],\n",
            "\n",
            "        [[-2.2349, -0.2662,  0.9803,  ...,  0.3713,  0.9427,  0.2722],\n",
            "         [-0.5549,  1.3684, -0.7289,  ...,  1.7489, -0.4699, -0.9584],\n",
            "         [-0.2568, -1.2811, -0.6975,  ...,  1.5589, -0.7868,  1.0305]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 3.0784e-01, -3.2590e-01, -2.1937e-01,  ..., -4.4820e-01,\n",
            "          -6.3350e-02,  2.9413e-01],\n",
            "         [ 2.6015e-03, -2.1188e-02, -5.4557e-01,  ..., -1.4480e-04,\n",
            "          -2.4194e-02,  1.6774e-04],\n",
            "         [-2.1714e-03, -9.8829e-05, -1.4010e-05,  ...,  9.3958e-04,\n",
            "           5.2561e-04, -7.3081e-01]],\n",
            "\n",
            "        [[ 7.7118e-02,  1.6290e-02, -2.8563e-02,  ...,  1.0615e-01,\n",
            "          -2.3078e-01, -1.4619e-02],\n",
            "         [-1.6737e-01,  5.1043e-02, -6.9929e-03,  ...,  4.9612e-04,\n",
            "          -9.7725e-03,  6.5587e-02],\n",
            "         [ 1.1248e-02, -5.0632e-01, -7.9804e-02,  ...,  1.9334e-01,\n",
            "          -1.4284e-02, -6.4425e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.6245e+00,  2.8154e+00, -7.5002e+00, -7.5895e+00, -7.4664e+00,\n",
            "          -7.7123e+00, -2.0831e+00, -7.5714e+00, -3.5689e+00, -7.3423e+00,\n",
            "          -1.3384e+00, -7.3226e+00, -7.5683e+00, -5.2582e+00, -4.2430e+00,\n",
            "          -3.4988e+00, -7.5769e+00, -7.4585e+00, -3.6694e+00, -7.6516e+00,\n",
            "          -7.4308e+00, -7.4007e+00, -7.6149e+00, -7.6341e+00, -7.5443e+00,\n",
            "          -7.5247e+00, -7.6709e+00, -7.5341e+00, -7.5439e+00, -7.4167e+00,\n",
            "          -7.6533e+00, -5.3780e+00, -7.8078e+00, -7.7056e+00, -7.5550e+00,\n",
            "          -7.5795e+00, -7.3722e+00, -2.9531e+00, -7.6973e+00,  3.9881e+00,\n",
            "          -7.5214e+00, -6.5872e+00, -2.4216e+00,  8.1828e+00,  1.6641e+00,\n",
            "          -7.4406e+00, -4.1359e+00, -4.2705e-01, -7.5812e+00, -2.5487e+00,\n",
            "          -3.8918e+00, -5.3820e+00,  2.5038e+00,  9.9770e-02, -2.0869e+00,\n",
            "          -7.5636e+00,  7.6433e-01, -1.2900e+00, -9.1932e-01, -4.8832e+00,\n",
            "          -7.2280e+00, -7.4184e+00, -7.4537e+00, -6.3872e+00, -2.2556e+00],\n",
            "         [-1.5065e+00,  2.8439e+00, -5.3224e+00, -5.2639e+00, -5.1103e+00,\n",
            "          -5.3112e+00, -1.6267e+00, -5.1919e+00, -2.9343e+00, -5.5265e+00,\n",
            "          -1.3192e+00, -5.0616e+00, -5.2239e+00, -4.4153e+00, -3.2666e+00,\n",
            "          -2.2054e+00, -5.3827e+00, -5.3743e+00, -3.2303e+00, -5.3292e+00,\n",
            "          -5.3471e+00, -5.1431e+00, -5.2841e+00, -5.3457e+00, -5.3812e+00,\n",
            "          -5.2495e+00, -5.3841e+00, -5.2894e+00, -5.1525e+00, -5.2021e+00,\n",
            "          -5.3665e+00, -4.7177e+00, -5.3746e+00, -5.3366e+00, -5.3382e+00,\n",
            "          -5.2969e+00, -5.3351e+00, -2.4220e+00, -5.3355e+00,  5.5714e+00,\n",
            "          -5.2103e+00, -4.4617e+00,  1.3596e+00,  1.0242e+00,  1.8961e+00,\n",
            "          -5.1703e+00, -3.1220e+00,  1.7771e-01, -5.3563e+00,  3.7424e-01,\n",
            "          -3.5225e+00, -2.4525e+00,  2.1512e+00,  2.2857e-01,  2.2482e-01,\n",
            "          -5.2292e+00,  3.8559e-01, -5.4222e-01, -1.4000e+00, -2.8851e+00,\n",
            "          -5.2129e+00, -5.2683e+00, -5.3940e+00, -3.7007e+00, -2.8540e+00],\n",
            "         [-2.2547e+00,  1.3455e+00, -2.8998e+00, -3.1594e+00, -3.1461e+00,\n",
            "          -3.0883e+00, -2.4399e-01, -3.0036e+00, -1.0658e+00, -3.2382e+00,\n",
            "          -1.9982e+00, -3.2290e+00, -3.1724e+00, -2.5701e+00, -1.0227e+00,\n",
            "          -8.5754e-01, -3.1858e+00, -3.0944e+00, -1.0782e+00, -3.3834e+00,\n",
            "          -3.2227e+00, -3.2149e+00, -3.3978e+00, -3.3465e+00, -3.1805e+00,\n",
            "          -3.2263e+00, -3.2617e+00, -3.1612e+00, -3.2955e+00, -3.3550e+00,\n",
            "          -3.3693e+00, -2.6643e+00, -3.1262e+00, -3.2141e+00, -3.1207e+00,\n",
            "          -3.2253e+00, -3.1727e+00, -1.4343e+00, -3.2483e+00,  4.2585e-01,\n",
            "          -3.0009e+00, -2.0481e+00, -2.9398e-01, -4.8224e-01, -5.0412e-01,\n",
            "          -3.1952e+00, -1.6077e+00,  1.7394e-01, -3.4724e+00,  2.3652e+00,\n",
            "          -2.5781e+00, -7.6268e-01,  6.7896e-01,  4.1062e+00,  3.2968e+00,\n",
            "          -3.3239e+00,  8.6251e-01,  8.2410e-01, -5.9113e-02,  2.3650e+00,\n",
            "          -3.0804e+00, -3.0848e+00, -3.4651e+00,  2.6148e-01, -1.7952e+00]],\n",
            "\n",
            "        [[-2.0518e+00,  1.8643e+00, -2.7016e+00, -2.6641e+00, -2.5926e+00,\n",
            "          -2.6813e+00,  3.0519e-01, -2.6114e+00, -2.4795e-01, -2.4055e+00,\n",
            "          -1.6869e-01, -2.6231e+00, -2.5995e+00, -2.8238e+00, -1.3738e+00,\n",
            "          -1.6765e+00, -2.5220e+00, -2.7181e+00, -2.0991e+00, -2.5593e+00,\n",
            "          -2.7489e+00, -2.6506e+00, -2.7753e+00, -2.7081e+00, -2.5548e+00,\n",
            "          -2.5816e+00, -2.4827e+00, -2.5662e+00, -2.5930e+00, -2.5437e+00,\n",
            "          -2.6259e+00, -3.1334e+00, -2.8956e+00, -2.5796e+00, -2.6576e+00,\n",
            "          -2.5722e+00, -2.6450e+00, -1.7357e+00, -2.7693e+00,  3.4720e-02,\n",
            "          -2.6548e+00, -2.1864e+00, -3.8353e-01,  6.4845e-01, -1.0410e+00,\n",
            "          -2.7764e+00, -1.8809e+00, -8.6293e-01, -2.8703e+00,  8.7600e-01,\n",
            "          -1.2256e+00, -7.6269e-01,  4.3372e-01,  7.0449e-02,  3.0697e-02,\n",
            "          -2.8390e+00,  8.9322e-01, -4.1878e-03,  1.4836e-01, -1.1053e+00,\n",
            "          -2.5947e+00, -2.5720e+00, -2.6343e+00, -6.0167e-01, -8.3087e-01],\n",
            "         [-1.8451e+00,  3.6039e-01, -3.2104e+00, -3.2961e+00, -3.3337e+00,\n",
            "          -3.3099e+00, -1.4480e+00, -3.2532e+00, -1.7477e+00, -3.3949e+00,\n",
            "          -1.3446e+00, -3.4173e+00, -3.3959e+00, -3.1540e+00, -2.3928e+00,\n",
            "           1.9546e+00, -3.4355e+00, -3.2013e+00, -2.4462e+00, -3.2628e+00,\n",
            "          -3.3930e+00, -3.5077e+00, -3.1806e+00, -3.3158e+00, -3.3538e+00,\n",
            "          -3.3199e+00, -3.2383e+00, -3.1308e+00, -3.3527e+00, -3.3461e+00,\n",
            "          -3.5612e+00, -3.2777e+00, -3.4530e+00, -3.2973e+00, -3.5202e+00,\n",
            "          -3.2599e+00, -3.2250e+00, -1.6159e+00, -3.4003e+00,  2.3287e+00,\n",
            "          -3.2002e+00, -2.3995e+00, -8.6124e-01, -7.0484e-01,  2.3537e+00,\n",
            "          -3.2348e+00,  8.0092e-01,  2.2919e-02, -3.4604e+00, -7.7191e-01,\n",
            "          -2.7353e+00,  1.5874e+00,  1.7651e+00, -1.1129e-01,  2.8382e+00,\n",
            "          -3.3938e+00,  3.6354e-01,  1.9533e+00, -9.6834e-02,  5.5020e-02,\n",
            "          -3.4827e+00, -3.2695e+00, -3.3626e+00, -1.4736e+00, -1.2424e+00],\n",
            "         [-1.0346e+00, -1.7119e+00, -9.7789e-01, -9.8052e-01, -1.1331e+00,\n",
            "          -1.2669e+00, -1.8652e-01, -1.2385e+00, -4.1303e-01, -7.6674e-01,\n",
            "          -1.9511e+00, -1.4443e+00, -1.3577e+00, -9.7775e-01, -4.2816e-01,\n",
            "           1.1831e+00, -9.8425e-01, -1.0909e+00, -3.3788e-01, -9.7184e-01,\n",
            "          -1.2705e+00, -1.1081e+00, -1.0351e+00, -1.4701e+00, -1.2266e+00,\n",
            "          -1.2813e+00, -9.0360e-01, -1.0501e+00, -1.1073e+00, -1.3367e+00,\n",
            "          -1.4091e+00, -5.3580e-01, -1.1182e+00, -1.1036e+00, -1.3506e+00,\n",
            "          -1.1550e+00, -9.5671e-01, -6.9499e-01, -1.1171e+00, -1.7748e+00,\n",
            "          -1.4482e+00,  3.6302e-01,  4.2332e-02,  6.8089e-01, -1.7482e+00,\n",
            "          -1.1634e+00,  8.6724e-01, -9.6177e-01, -1.1261e+00, -2.9218e+00,\n",
            "          -1.0819e-01, -2.1409e-01,  6.3241e-02, -4.2361e-01,  6.3632e+00,\n",
            "          -1.4577e+00,  1.8432e+00, -1.6711e+00,  3.1031e+00,  2.6168e+00,\n",
            "          -8.8055e-01, -1.1450e+00, -1.1619e+00,  2.1406e-01,  1.0787e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-1.5309,  0.2066, -0.2155,  ...,  1.6062, -0.1768, -0.9135],\n",
            "         [-0.9496, -0.6756,  0.2789,  ...,  1.2229, -1.5935,  1.7349],\n",
            "         [ 0.5908, -0.8880,  0.3955,  ..., -0.8997, -0.2797,  0.1278]],\n",
            "\n",
            "        [[-0.4238,  1.0210,  0.5888,  ..., -0.3488, -2.8386, -1.9047],\n",
            "         [ 1.3879, -0.1685,  0.3122,  ..., -0.8006,  0.4268,  0.5430],\n",
            "         [ 1.0426,  0.4261,  0.1983,  ...,  1.3364,  1.1146,  0.1344]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-1.5021e-03,  2.1072e-02, -2.1509e-03,  ...,  3.0269e-02,\n",
            "           3.7100e-01,  6.1777e-01],\n",
            "         [ 6.5563e-02,  1.3238e-05, -1.9453e-04,  ...,  3.6783e-02,\n",
            "          -3.4235e-01,  6.5634e-05],\n",
            "         [ 3.7123e-03, -1.5933e-04,  8.4705e-05,  ...,  2.7300e-01,\n",
            "          -5.2862e-02, -6.9473e-06]],\n",
            "\n",
            "        [[-2.6729e-01,  4.1719e-02, -3.1728e-02,  ...,  4.4069e-02,\n",
            "          -3.5847e-02,  1.5980e-02],\n",
            "         [ 5.9294e-01,  9.2373e-04, -5.2951e-02,  ..., -2.3203e-02,\n",
            "           6.6406e-02,  2.9183e-02],\n",
            "         [ 3.5080e-03, -1.8857e-04, -2.5936e-01,  ...,  2.1224e-05,\n",
            "           1.3533e-01,  7.2151e-05]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 1.3596,  0.7061, -4.8522, -4.6009, -4.7169, -4.9284,  4.9186,\n",
            "          -4.8793,  3.5636, -4.6223, -0.2418, -4.6167, -4.9188, -5.0401,\n",
            "          -3.0459, -4.4587, -4.7376, -4.7314, -3.1607, -5.0435, -4.9043,\n",
            "          -4.8516, -4.7013, -4.7274, -4.7850, -4.7432, -4.7479, -4.6019,\n",
            "          -4.8175, -4.6591, -4.8277, -5.0956, -4.7006, -4.7286, -4.7946,\n",
            "          -4.6652, -4.6897, -1.0404, -4.8830, -0.8825, -4.7043, -5.3070,\n",
            "           0.1207,  1.1877, -1.6819, -4.5420, -3.2938, -1.1059, -4.9046,\n",
            "          -1.4351, -1.6094, -2.1925, -1.3039, -1.2814, -4.6770, -4.7803,\n",
            "           0.3233, -0.0588, -2.6318, -4.5377, -4.8169, -4.7706, -4.9106,\n",
            "          -2.1259, -1.9650],\n",
            "         [ 8.8731,  1.7139, -6.0343, -5.7085, -6.1230, -5.9474,  1.9948,\n",
            "          -5.8897, -0.3409, -5.8605,  0.7440, -6.1946, -6.0930, -5.9450,\n",
            "          -3.8878, -5.5038, -6.1700, -6.1256, -3.6110, -5.9527, -6.1726,\n",
            "          -6.0831, -6.1193, -6.1219, -6.2774, -6.1838, -6.3158, -6.1109,\n",
            "          -6.2081, -6.0415, -6.3778, -6.0906, -5.9655, -6.0834, -6.3013,\n",
            "          -5.9137, -5.7618, -0.1109, -5.9681, -0.5117, -6.1053, -6.7084,\n",
            "          -0.8403, -1.1463, -1.6940, -6.2008, -5.0047, -1.9184, -6.2320,\n",
            "          -0.1602, -1.4468, -1.2072, -0.7990, -0.4091, -6.1997, -6.4205,\n",
            "          -0.2811,  0.8373, -1.9282, -5.7815, -6.1817, -6.2758, -6.2126,\n",
            "          -3.2951, -2.0726],\n",
            "         [ 5.4477,  0.8104, -2.6125, -2.5196, -2.6015, -2.7503,  0.3047,\n",
            "          -2.7773, -0.6195, -2.5889, -0.3104, -2.9638, -2.7631, -0.4564,\n",
            "          -0.4969, -1.7799, -2.5529, -2.6260,  0.0988, -2.6735, -2.8458,\n",
            "          -2.5731, -2.6423, -2.8731, -2.8060, -2.6531, -2.7472, -2.8104,\n",
            "          -2.7000, -3.0587, -2.9698, -0.7901, -2.6307, -2.7122, -2.8306,\n",
            "          -2.4956, -2.6082, -0.2997, -2.7886, -0.3941, -2.5369, -2.6935,\n",
            "          -0.2165, -0.1392, -0.6243, -2.7915, -1.7724, -0.9422, -2.9207,\n",
            "           0.0221, -0.8085, -0.5725, -0.0443,  0.0967, -1.4086, -2.8918,\n",
            "          -0.3117, -0.3422, -1.2174, -2.2583, -2.5896, -2.5943, -2.8786,\n",
            "          -1.5713, -1.5982]],\n",
            "\n",
            "        [[-0.3846, -1.0412, -1.2558, -0.9803, -1.2562, -0.9865,  0.8749,\n",
            "          -1.2682, -0.1317, -1.1432, -0.4972, -1.0415, -1.2820, -1.6856,\n",
            "          -1.2323, -1.6029, -1.2369, -1.0489, -1.8896, -0.9443, -1.2954,\n",
            "          -1.1436, -1.0283, -1.1315, -1.0895, -0.9161, -0.8781, -1.0250,\n",
            "          -1.0490, -1.2513, -1.2027, -1.6503, -1.1660, -1.1231, -1.2177,\n",
            "          -1.1135, -0.8665, -0.6964, -1.1276, -0.6335, -1.1829, -0.1210,\n",
            "           0.2897, -0.1560, -0.6240, -1.1357, -0.9526, -0.6148, -1.1191,\n",
            "          -0.3993, -0.8209, -1.1264,  0.3112, -1.2247, -0.9257, -1.2945,\n",
            "           2.4215, -0.9654, -0.7419,  0.5236, -1.0334, -1.0507, -1.2185,\n",
            "          -0.2053,  0.4224],\n",
            "         [-1.3175,  3.3789, -7.8318, -7.8412, -7.9571, -7.6278, -1.6552,\n",
            "          -7.6771, -4.2577, -7.8694,  0.1295, -7.6251, -7.8744, -6.3246,\n",
            "          -5.6446, -5.5446, -7.8782, -7.8970, -4.8208, -7.7346, -7.9146,\n",
            "          -7.9456, -7.8544, -8.0521, -7.6802, -7.6183, -7.6961, -7.6526,\n",
            "          -7.9003, -7.6021, -8.0178, -6.4808, -8.1531, -7.9153, -7.9842,\n",
            "          -7.8332, -7.6329, -2.6525, -7.9364,  4.5939, -7.9127, -7.3599,\n",
            "          -1.9781,  5.7140,  1.9289, -7.8886, -5.5160, -0.7871, -7.6779,\n",
            "          -1.5586, -4.0429, -5.2305,  2.5715, -0.4276, -4.5323, -7.8755,\n",
            "          -0.0854, -0.1731, -1.5488, -5.5267, -7.5682, -7.7222, -7.9528,\n",
            "          -6.0086, -2.6423],\n",
            "         [-1.6357,  2.6772, -6.3554, -6.2319, -6.0331, -6.1532, -1.0081,\n",
            "          -5.9685, -3.2088, -6.3527, -1.0346, -5.9740, -6.1908, -5.3823,\n",
            "          -3.9285, -3.2380, -6.2697, -6.2920, -4.0316, -6.0882, -6.3293,\n",
            "          -6.1634, -6.2648, -6.1904, -6.0938, -6.2003, -6.2124, -6.3205,\n",
            "          -5.9927, -6.1721, -6.3156, -5.4243, -6.2935, -6.0629, -6.2566,\n",
            "          -6.1187, -6.0337, -2.5755, -6.2386,  4.1435, -6.1555, -5.5055,\n",
            "           1.2065,  1.1091,  1.6474, -6.1794, -3.7286,  0.0916, -6.1180,\n",
            "           0.5156, -3.3731, -2.8414,  2.5602, -0.5311, -0.8135, -6.0046,\n",
            "           0.7027,  0.0191, -1.6419, -3.9477, -5.8987, -6.0327, -6.2375,\n",
            "          -4.1350, -2.7520]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.1805,  0.8522,  0.5753,  ...,  0.6781,  0.1845, -0.9315],\n",
            "         [ 1.0389,  0.4296,  0.1955,  ...,  1.3358,  1.1100,  0.1314],\n",
            "         [ 0.8200,  0.3136, -0.4207,  ...,  0.0909,  0.1806,  0.2438]],\n",
            "\n",
            "        [[-0.6428,  1.0091,  0.7937,  ..., -0.3907,  0.6032,  1.9444],\n",
            "         [-0.3949, -1.0678,  0.6214,  ...,  0.7269,  0.0591,  0.0356],\n",
            "         [ 1.0389,  0.4296,  0.1955,  ...,  1.3358,  1.1100,  0.1314]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 9.3669e-04, -1.9818e-01, -6.2816e-01,  ...,  2.5128e-02,\n",
            "           2.8619e-03,  9.0165e-02],\n",
            "         [ 6.6363e-04, -2.1971e-03, -6.1505e-01,  ...,  8.1066e-05,\n",
            "           3.5964e-02,  1.9510e-05],\n",
            "         [ 6.1211e-03,  4.2084e-04, -1.1402e-04,  ...,  5.1241e-02,\n",
            "           3.6147e-05, -4.9717e-01]],\n",
            "\n",
            "        [[ 1.9095e-01,  1.0270e-01, -1.5961e-02,  ...,  5.2574e-02,\n",
            "           9.0051e-02, -1.7288e-01],\n",
            "         [ 7.1701e-01,  3.6531e-01, -5.7395e-01,  ..., -4.0793e-01,\n",
            "           5.7789e-02,  1.5732e-01],\n",
            "         [ 7.5881e-02, -4.8144e-01, -7.3206e-01,  ..., -3.5893e-02,\n",
            "           7.0675e-01,  3.5567e-06]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-7.1786e-01,  2.9269e+00, -6.6568e+00, -7.1077e+00, -6.9298e+00,\n",
            "          -7.1500e+00, -2.0655e-01, -7.1681e+00, -2.0026e+00, -7.0425e+00,\n",
            "          -6.8585e-02, -7.0716e+00, -7.0427e+00, -5.2159e+00, -3.3990e+00,\n",
            "          -3.2113e+00, -6.8897e+00, -7.0020e+00, -2.9806e+00, -7.0665e+00,\n",
            "          -7.0850e+00, -7.1505e+00, -7.1216e+00, -7.0971e+00, -7.1531e+00,\n",
            "          -7.0138e+00, -6.9725e+00, -7.0056e+00, -6.8882e+00, -7.0754e+00,\n",
            "          -7.3309e+00, -4.8891e+00, -7.1942e+00, -7.1175e+00, -7.2144e+00,\n",
            "          -6.9953e+00, -7.0654e+00, -1.9543e+00, -7.2343e+00,  1.4851e+00,\n",
            "          -6.9402e+00, -6.5568e+00, -1.0724e+00,  6.3994e+00, -9.8408e-01,\n",
            "          -7.0824e+00, -3.1866e+00,  7.3971e-02, -6.9312e+00, -2.2055e+00,\n",
            "          -1.7465e+00, -3.6014e+00,  1.2852e+00, -1.5284e-01, -2.6443e+00,\n",
            "          -6.8936e+00, -3.3941e-01,  4.4165e-01, -2.3249e+00, -5.3506e+00,\n",
            "          -6.9107e+00, -7.0671e+00, -6.8736e+00, -4.2662e+00, -3.0234e+00],\n",
            "         [-1.1121e+00,  2.7695e+00, -4.7917e+00, -4.8278e+00, -4.7642e+00,\n",
            "          -4.8153e+00, -8.0433e-01, -4.7296e+00, -2.3228e+00, -4.8395e+00,\n",
            "          -8.6615e-01, -4.6265e+00, -4.7697e+00, -4.0203e+00, -2.9026e+00,\n",
            "          -1.9504e+00, -4.8181e+00, -4.9437e+00, -3.0807e+00, -4.7979e+00,\n",
            "          -4.9045e+00, -4.8439e+00, -4.8527e+00, -4.9102e+00, -4.7693e+00,\n",
            "          -4.7014e+00, -4.8631e+00, -4.9329e+00, -4.6351e+00, -4.7258e+00,\n",
            "          -4.9691e+00, -4.2250e+00, -4.8831e+00, -4.7769e+00, -4.8322e+00,\n",
            "          -4.6786e+00, -4.8005e+00, -2.3955e+00, -4.7940e+00,  2.7223e+00,\n",
            "          -4.8775e+00, -4.0440e+00,  1.3849e+00,  6.8664e-01,  8.3486e-01,\n",
            "          -4.7845e+00, -2.5600e+00,  4.0870e-03, -4.7528e+00, -4.1720e-02,\n",
            "          -2.6383e+00, -2.1089e+00,  2.3441e+00, -4.4642e-01,  9.4566e-02,\n",
            "          -4.7080e+00,  3.9526e-01, -5.8869e-03, -1.2948e+00, -2.4380e+00,\n",
            "          -4.7271e+00, -4.5875e+00, -4.7467e+00, -2.8807e+00, -2.6180e+00],\n",
            "         [-2.2090e+00,  2.4020e+00, -2.6566e+00, -2.6519e+00, -2.7593e+00,\n",
            "          -3.0370e+00,  1.7487e-01, -2.8936e+00, -1.2270e+00, -3.0525e+00,\n",
            "           3.2764e+00, -2.8027e+00, -2.6049e+00, -3.0677e+00, -1.6866e+00,\n",
            "          -3.3471e+00, -3.1117e+00, -2.7873e+00, -2.2211e+00, -2.7270e+00,\n",
            "          -2.9818e+00, -2.7802e+00, -2.7041e+00, -2.9081e+00, -2.9182e+00,\n",
            "          -2.7396e+00, -2.9712e+00, -2.7629e+00, -2.7504e+00, -3.0066e+00,\n",
            "          -2.9368e+00, -3.0374e+00, -2.8401e+00, -2.8276e+00, -2.7236e+00,\n",
            "          -2.6639e+00, -2.8988e+00, -1.6087e+00, -2.8161e+00,  5.0930e-01,\n",
            "          -2.6829e+00, -2.8776e+00, -4.1405e-01, -9.8491e-01, -2.9737e-01,\n",
            "          -2.9467e+00, -2.2591e+00, -4.9435e-01, -2.9640e+00,  1.4243e+00,\n",
            "          -3.2669e-01,  5.4505e-02, -3.4109e-01,  6.0023e-01, -3.0296e+00,\n",
            "          -3.1967e+00, -1.7422e+00,  1.6461e+00, -1.4369e+00, -2.3365e+00,\n",
            "          -2.8382e+00, -2.8747e+00, -2.7345e+00,  2.4225e+00, -2.2393e+00]],\n",
            "\n",
            "        [[-1.2396e+00,  4.0641e-01, -9.9852e-01, -1.1393e+00, -1.1695e+00,\n",
            "          -1.2186e+00, -3.6316e-02, -1.1646e+00, -1.0501e+00, -1.2493e+00,\n",
            "          -2.2648e-01, -1.2385e+00, -1.2402e+00, -8.8427e-01, -1.3179e+00,\n",
            "          -2.5215e-02, -1.0248e+00, -1.2913e+00, -1.2450e+00, -1.0551e+00,\n",
            "          -1.1996e+00, -1.1904e+00, -1.2751e+00, -1.1799e+00, -1.1819e+00,\n",
            "          -1.3699e+00, -1.2474e+00, -1.0740e+00, -1.1835e+00, -1.2317e+00,\n",
            "          -1.1190e+00, -1.2077e+00, -1.1979e+00, -1.0381e+00, -1.1719e+00,\n",
            "          -1.1078e+00, -1.1692e+00, -1.4250e+00, -1.2961e+00, -1.4630e-01,\n",
            "          -1.0750e+00, -2.8012e-01,  7.6041e-01, -1.0039e+00, -3.2558e-01,\n",
            "          -1.1097e+00, -1.0829e+00, -1.1508e+00, -1.1399e+00, -1.2837e-01,\n",
            "           1.4359e-01, -4.7955e-01, -7.2936e-01,  8.0282e-01,  5.0292e+00,\n",
            "          -1.5925e+00, -4.9481e-01, -6.3961e-01,  1.1302e+00,  1.3107e+00,\n",
            "          -1.2750e+00, -1.0434e+00, -1.2783e+00, -1.8443e-01, -5.7539e-01],\n",
            "         [-1.6033e+00, -5.3926e-01, -7.8222e+00, -7.6328e+00, -7.5774e+00,\n",
            "          -7.8848e+00, -6.7145e-01, -7.9036e+00, -2.5726e+00, -7.6555e+00,\n",
            "          -2.4711e+00, -7.8373e+00, -7.9723e+00, -6.3470e+00, -4.8862e+00,\n",
            "          -5.0606e+00, -7.8983e+00, -7.8086e+00, -4.1254e+00, -7.8051e+00,\n",
            "          -7.9493e+00, -7.8505e+00, -7.9721e+00, -7.8421e+00, -7.9543e+00,\n",
            "          -7.9722e+00, -7.6865e+00, -7.7123e+00, -7.8043e+00, -7.7650e+00,\n",
            "          -8.1289e+00, -6.0682e+00, -7.9637e+00, -7.8857e+00, -7.9980e+00,\n",
            "          -7.7854e+00, -7.8022e+00, -2.8110e+00, -7.8799e+00, -1.5801e-01,\n",
            "          -7.9663e+00, -5.1175e+00, -3.0750e+00,  7.6280e+00, -2.9277e+00,\n",
            "          -7.8899e+00, -5.0037e+00, -9.5997e-01, -7.7758e+00, -3.2042e+00,\n",
            "          -4.0196e+00, -4.2601e+00, -3.7193e-01, -3.6519e-01, -1.0746e+00,\n",
            "          -8.2589e+00,  5.8501e+00, -1.8057e+00,  2.7820e-02, -3.2718e+00,\n",
            "          -7.8983e+00, -7.6443e+00, -7.7100e+00, -4.4230e+00, -1.8161e+00],\n",
            "         [-1.4347e+00,  1.9326e+00, -5.2904e+00, -5.1885e+00, -5.0708e+00,\n",
            "          -5.2873e+00, -2.1459e+00, -5.2645e+00, -3.1252e+00, -5.4508e+00,\n",
            "          -1.1208e+00, -5.0395e+00, -5.1821e+00, -4.1192e+00, -3.2206e+00,\n",
            "          -2.1773e+00, -5.1707e+00, -5.2912e+00, -3.0304e+00, -5.3882e+00,\n",
            "          -5.3493e+00, -5.2430e+00, -5.2332e+00, -5.5040e+00, -5.4389e+00,\n",
            "          -5.1144e+00, -5.4527e+00, -5.3204e+00, -5.2375e+00, -5.2332e+00,\n",
            "          -5.3430e+00, -4.5376e+00, -5.4561e+00, -5.3592e+00, -5.2722e+00,\n",
            "          -5.0382e+00, -5.4075e+00, -2.5158e+00, -5.2949e+00,  6.1396e+00,\n",
            "          -5.1993e+00, -3.8496e+00,  5.5947e-01,  1.1627e+00,  2.2539e+00,\n",
            "          -5.1433e+00, -2.8963e+00, -2.4558e-01, -5.3951e+00, -4.0707e-01,\n",
            "          -3.6630e+00, -2.1399e+00,  2.1629e+00,  4.0475e-01,  6.3951e-02,\n",
            "          -5.3691e+00,  1.0948e+00, -4.1254e-01, -9.7106e-01, -2.4643e+00,\n",
            "          -5.4021e+00, -5.0565e+00, -5.5239e+00, -3.8236e+00, -3.0895e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.2525, -1.2793, -0.6917,  ...,  1.5602, -0.7745,  1.0328],\n",
            "         [ 0.7954, -0.9914,  0.3050,  ...,  1.9602, -1.1090, -0.3815],\n",
            "         [-0.5588,  1.3874, -0.7276,  ...,  1.7522, -0.4894, -0.9613]],\n",
            "\n",
            "        [[ 0.1969,  1.7128,  1.5865,  ..., -1.4747,  1.5628,  0.0353],\n",
            "         [ 0.1812,  0.8572,  0.5786,  ...,  0.6746,  0.1860, -0.9297],\n",
            "         [ 1.0361,  0.4322,  0.1932,  ...,  1.3341,  1.1066,  0.1278]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 8.0930e-02, -6.7645e-01, -2.0751e-01,  ...,  5.0904e-01,\n",
            "           5.8121e-02, -5.5910e-01],\n",
            "         [ 6.8934e-01, -2.6813e-01, -7.4067e-01,  ...,  6.3147e-01,\n",
            "          -2.3267e-03, -3.0340e-01],\n",
            "         [-3.2687e-04, -3.8392e-03, -1.1773e-03,  ...,  1.7209e-04,\n",
            "          -1.2105e-04,  3.5193e-02]],\n",
            "\n",
            "        [[-4.8184e-03, -1.0599e-05, -1.4308e-02,  ...,  4.5235e-03,\n",
            "          -6.1710e-02,  1.1336e-01],\n",
            "         [-1.2092e-07, -1.1177e-02, -6.6983e-01,  ...,  2.9105e-04,\n",
            "          -6.8636e-01,  1.1132e-01],\n",
            "         [ 2.0427e-06, -3.6499e-04, -6.0949e-01,  ...,  1.8972e-06,\n",
            "          -8.2826e-01,  2.2264e-05]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.6386,  0.3716, -1.8183, -1.7965, -1.8734, -1.9836, -0.6621,\n",
            "          -1.8568, -1.2892, -1.6759, -1.3975, -2.0720, -1.7707, -0.6143,\n",
            "          -0.6414,  0.3915, -1.6798, -1.9261, -0.3836, -1.7101, -1.8415,\n",
            "          -1.6784, -1.8448, -1.9121, -2.0291, -1.7989, -1.5076, -1.5179,\n",
            "          -1.8359, -1.9152, -1.9818, -0.5097, -1.8321, -1.7933, -1.9046,\n",
            "          -1.7354, -1.6560, -0.9653, -1.7987, -1.8033, -1.9453,  0.0841,\n",
            "           0.3311,  1.5648, -2.1483, -1.8443, -1.4922, -1.7141, -1.7775,\n",
            "          -2.6971, -0.2332, -1.0465, -1.4299,  0.1811,  8.6375, -2.0650,\n",
            "          -0.4819, -2.0678,  5.0236,  2.2899, -1.5005, -1.4005, -1.9159,\n",
            "          -1.5196,  0.3663],\n",
            "         [-0.7675, 11.0084, -6.5892, -6.5002, -6.1833, -6.3205, -0.9859,\n",
            "          -6.2096, -2.1710, -6.3107,  0.0197, -6.5179, -6.7087, -4.3800,\n",
            "          -3.1331, -0.2129, -6.8324, -6.2493, -2.0683, -6.4216, -6.3612,\n",
            "          -6.6132, -6.6616, -6.6869, -6.5380, -6.4914, -6.4729, -6.4271,\n",
            "          -6.4375, -6.4815, -6.4999, -4.0110, -6.4097, -6.4322, -6.8371,\n",
            "          -6.4028, -6.3161, -1.3816, -6.4460,  1.6224, -6.3842, -4.6660,\n",
            "          -0.4162,  2.8520,  0.2134, -6.3505,  0.5892, -4.5834, -6.5919,\n",
            "          -2.3959, -2.2489, -4.2085,  0.3692, -0.5190, -1.0588, -6.8459,\n",
            "          -1.7522,  0.7289, -2.7637, -4.4691, -6.2094, -6.3182, -6.5231,\n",
            "          -3.3615, -4.5747],\n",
            "         [-1.3425,  0.4869, -1.1661, -1.3323, -1.1961, -1.2393, -0.7484,\n",
            "          -1.2634, -0.8978, -1.3221, -0.6001, -1.5964, -1.4149, -0.6769,\n",
            "          -0.6833,  6.9181, -1.2018, -1.2913, -0.2057, -1.4327, -1.2667,\n",
            "          -1.3029, -1.1333, -1.3777, -1.3483, -1.3204, -1.3685, -1.3359,\n",
            "          -1.3825, -1.4171, -1.5080, -1.0293, -1.4309, -1.3621, -1.3888,\n",
            "          -1.3615, -1.4423, -0.8218, -1.3847,  0.0462, -1.1711, -0.3975,\n",
            "          -0.5799, -1.3720,  1.6222, -1.4131,  2.1409, -0.1817, -1.3332,\n",
            "          -0.7398, -0.4452,  1.4854, -0.2019, -0.1669,  2.4118, -1.3274,\n",
            "          -0.9275,  2.2064, -0.4757,  0.7613, -1.3577, -0.1561, -1.2830,\n",
            "          -0.5655, -1.5282]],\n",
            "\n",
            "        [[-2.4578, -1.2253, -4.5903, -4.6802, -4.8052, -4.7779, -1.3815,\n",
            "          -4.9599, -2.8530, -4.5434, -1.5581, -4.5816, -4.7082, -5.1736,\n",
            "          -2.9973, -2.5831, -4.7699, -4.9544, -4.0864, -4.5278, -4.9113,\n",
            "          -4.5828, -4.7834, -4.4935, -4.7296, -4.5961, -4.4817, -4.5197,\n",
            "          -4.5620, -4.4618, -4.9481, -5.0893, -4.6852, -4.4713, -4.7388,\n",
            "          -4.8197, -4.5054, -1.4183, -4.9243, -0.5045, -4.7104, -2.6446,\n",
            "          -1.7397,  1.0849, -2.2320, -4.6183, -1.8740, -1.4613, -4.8122,\n",
            "          -2.1386, -1.6833, -1.9971,  1.5333, -2.4793, -1.6864, -4.6112,\n",
            "           6.1760,  0.2476,  1.0825, -3.6084, -4.4620, -4.5271, -4.7378,\n",
            "          -3.1929,  4.3565],\n",
            "         [-1.7513,  2.8526, -7.9280, -8.5151, -8.3842, -8.7886, -1.2265,\n",
            "          -8.7000, -3.9252, -8.7409, -0.4154, -8.4841, -8.4134, -6.7438,\n",
            "          -4.1475, -1.5540, -8.2776, -8.3285, -4.1481, -8.3224, -8.5205,\n",
            "          -8.5832, -8.5167, -8.2912, -8.6233, -8.3314, -8.2491, -8.3528,\n",
            "          -8.2284, -8.2625, -8.5215, -6.0644, -8.5208, -8.3056, -8.5619,\n",
            "          -8.2774, -8.5448, -1.8751, -8.6643,  1.1289, -8.3390, -7.1681,\n",
            "          -2.0776,  7.0458, -1.6704, -8.6160, -3.0882,  0.5567, -8.2378,\n",
            "          -4.3584, -1.4346, -3.7422,  1.9122, -1.2729, -3.1974, -8.4420,\n",
            "           0.1790,  2.8826, -2.3463, -7.1806, -8.3757, -8.1772, -8.2578,\n",
            "          -5.4451, -2.1344],\n",
            "         [-1.8466,  2.0352, -4.9373, -4.8881, -4.9766, -5.1260, -0.9438,\n",
            "          -4.9478, -2.7085, -5.0601, -0.7578, -4.9063, -4.9762, -4.5451,\n",
            "          -2.9073, -1.6460, -5.0350, -5.0699, -3.5912, -5.0614, -4.9830,\n",
            "          -5.0462, -5.2532, -5.1001, -4.9882, -4.7444, -5.0497, -5.0431,\n",
            "          -4.8606, -4.9188, -5.0880, -4.5258, -4.9999, -5.0246, -5.0483,\n",
            "          -4.7473, -4.9098, -2.1370, -4.9392,  2.1016, -5.0277, -4.2449,\n",
            "           0.5647,  1.0886,  0.9497, -4.9616, -2.2628, -0.1970, -4.9598,\n",
            "          -0.8993, -2.8029, -2.1486,  3.8840, -1.2486, -0.7158, -4.6550,\n",
            "           0.7123,  0.8816, -1.4120, -3.0601, -4.8627, -4.5945, -4.9166,\n",
            "          -3.2072, -1.8126]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.4392, -0.2570, -0.5617,  ...,  2.3392, -1.6646,  2.3263],\n",
            "         [-0.4198,  1.0366,  0.5752,  ..., -0.3669, -2.8536, -1.9228],\n",
            "         [ 1.4014, -0.1820,  0.3246,  ..., -0.8142,  0.4404,  0.5566]],\n",
            "\n",
            "        [[-0.4392, -0.2570, -0.5617,  ...,  2.3392, -1.6646,  2.3263],\n",
            "         [-2.2124, -0.2863,  0.9997,  ...,  0.3903,  0.9637,  0.2938],\n",
            "         [-0.5607,  1.3934, -0.7264,  ...,  1.7527, -0.4959, -0.9632]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 4.8732e-01, -7.3704e-01,  1.8790e-03,  ..., -1.1654e-02,\n",
            "          -7.2569e-01, -1.0317e-02],\n",
            "         [ 2.6963e-01, -5.1847e-02, -9.1259e-02,  ...,  4.5216e-02,\n",
            "          -2.4933e-01,  7.7970e-03],\n",
            "         [ 8.9515e-01, -1.6552e-01, -6.5000e-01,  ..., -1.3579e-01,\n",
            "          -1.9921e-01,  4.3662e-01]],\n",
            "\n",
            "        [[ 4.8732e-01, -7.3704e-01,  1.8790e-03,  ..., -1.1654e-02,\n",
            "          -7.2569e-01, -1.0317e-02],\n",
            "         [ 4.3585e-02, -2.8886e-02, -1.4843e-01,  ...,  1.6887e-01,\n",
            "          -7.2258e-01, -6.4256e-02],\n",
            "         [-3.8083e-03, -3.4554e-02, -2.3220e-02,  ...,  4.7478e-04,\n",
            "          -2.7199e-02,  1.0904e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.1488,  4.1661, -6.4141, -6.5407, -6.5134, -6.8049, -1.7651,\n",
            "          -6.7529, -4.0087, -6.7551, -0.4469, -6.7344, -6.6408, -5.5767,\n",
            "          -2.6272, -3.0985, -6.7015, -6.6743, -3.0508, -6.5851, -6.7601,\n",
            "          -6.5575, -6.6147, -6.6184, -6.6528, -6.8999, -6.4677, -6.6963,\n",
            "          -6.3955, -6.6148, -6.9610, -5.2506, -6.9053, -6.7053, -6.6495,\n",
            "          -6.5996, -6.5827, -1.2390, -6.8034, -1.4721, -6.6994, -4.6622,\n",
            "          -2.2827,  3.3881, -2.5777, -6.8612, -0.9775, -1.5316, -6.6870,\n",
            "          -3.1913, -2.3131, -0.8003, -0.8156,  0.6501, -2.2589, -6.5639,\n",
            "          -0.3641,  3.7683,  3.3552, -3.5648, -6.5082, -5.7213, -6.6814,\n",
            "          -3.6834, -0.4930],\n",
            "         [-0.8502,  1.4059,  2.0921,  2.1001,  2.0579,  2.2890, -0.6053,\n",
            "           2.1828, -0.1409,  1.9961,  0.7344,  2.0540,  1.9498,  1.6312,\n",
            "           1.1350,  0.7690,  2.0957,  2.1922,  0.7754,  2.2513,  2.0163,\n",
            "           2.0187,  2.0734,  2.3227,  2.2068,  2.4082,  2.2588,  2.0974,\n",
            "           2.0555,  2.0067,  1.9313,  1.8000,  1.9961,  2.1959,  1.9532,\n",
            "           2.1806,  2.1139,  0.2670,  2.1668, -0.6757,  2.0326,  4.4505,\n",
            "           0.4191, -0.1228, -0.4576,  2.1039,  1.6269, -2.0430,  2.0096,\n",
            "          -0.8275,  0.7435,  1.0257, -0.4809, -0.2501,  0.5561,  2.1044,\n",
            "          -1.5464, -0.7231,  2.6838,  2.3307,  2.2918,  2.4235,  2.2047,\n",
            "           1.0652,  0.6581],\n",
            "         [-1.8186,  5.4739, -9.3152, -9.5080, -9.5374, -9.0502, -2.9812,\n",
            "          -9.2792, -5.1850, -9.5330,  0.4749, -9.4046, -9.4990, -6.2141,\n",
            "          -7.0320, -5.9246, -9.6522, -9.3949, -5.4495, -9.3898, -9.3642,\n",
            "          -9.4816, -9.6716, -9.4569, -9.3281, -8.9702, -9.3532, -9.4807,\n",
            "          -9.5166, -9.2041, -9.8437, -6.0784, -9.7376, -9.8437, -9.6613,\n",
            "          -9.3922, -9.2300, -3.4677, -9.3264,  4.3784, -9.3153, -7.2786,\n",
            "          -3.4480, 10.9523,  2.0000, -9.5012, -5.9303, -2.1963, -9.0931,\n",
            "          -4.3370, -4.1788, -6.7920,  2.8404, -1.1141, -4.6842, -9.3760,\n",
            "          -1.6439, -0.4017, -0.6746, -6.6012, -9.2244, -9.2441, -9.3753,\n",
            "          -8.2371, -3.0683]],\n",
            "\n",
            "        [[-0.1488,  4.1661, -6.4141, -6.5407, -6.5134, -6.8049, -1.7651,\n",
            "          -6.7529, -4.0087, -6.7551, -0.4469, -6.7344, -6.6408, -5.5767,\n",
            "          -2.6272, -3.0985, -6.7015, -6.6743, -3.0508, -6.5851, -6.7601,\n",
            "          -6.5575, -6.6147, -6.6184, -6.6528, -6.8999, -6.4677, -6.6963,\n",
            "          -6.3955, -6.6148, -6.9610, -5.2506, -6.9053, -6.7053, -6.6495,\n",
            "          -6.5996, -6.5827, -1.2390, -6.8034, -1.4721, -6.6994, -4.6622,\n",
            "          -2.2827,  3.3881, -2.5777, -6.8612, -0.9775, -1.5316, -6.6870,\n",
            "          -3.1913, -2.3131, -0.8003, -0.8156,  0.6501, -2.2589, -6.5639,\n",
            "          -0.3641,  3.7683,  3.3552, -3.5648, -6.5082, -5.7213, -6.6814,\n",
            "          -3.6834, -0.4930],\n",
            "         [-1.4187,  5.5769, -3.9573, -4.1805, -4.1591, -4.1251, -0.9331,\n",
            "          -4.3584, -1.7029, -4.1108,  0.1983, -4.1958, -4.1444, -3.8710,\n",
            "          -1.7426, -2.2405, -3.9624, -4.0991, -2.4135, -4.1181, -4.1568,\n",
            "          -3.9410, -4.2252, -4.1792, -4.1777, -4.3245, -3.8920, -4.2639,\n",
            "          -4.1087, -3.9123, -4.2326, -3.6390, -4.3592, -4.0934, -4.2681,\n",
            "          -4.1116, -4.1642, -1.2790, -4.2932, -0.5183, -4.0501, -2.0471,\n",
            "          -0.3886,  1.7920, -1.8739, -4.2676, -1.1065, -2.4523, -4.3050,\n",
            "          -1.8100, -1.6919, -0.9829, -0.3559,  0.0399, -1.3883, -4.1804,\n",
            "          -0.1959, -0.0267,  2.1900, -2.3075, -3.8829, -3.9575, -4.0577,\n",
            "          -1.7913, -0.4601],\n",
            "         [-1.2153,  0.7218, -1.8748, -1.9867, -1.8412, -2.0006, -1.1446,\n",
            "          -1.9961, -1.6696, -2.1542, -0.3638, -2.0739, -2.1264, -1.9559,\n",
            "          -1.4712,  2.5883, -1.8652, -1.8761, -1.2236, -2.0287, -2.0988,\n",
            "          -2.0418, -1.8794, -1.9276, -2.0750, -1.8463, -1.8702, -1.9033,\n",
            "          -1.9697, -2.0012, -2.1086, -2.2083, -2.0183, -1.8777, -2.0326,\n",
            "          -1.8797, -1.9374, -0.9724, -2.0437,  0.3322, -1.9672, -0.9416,\n",
            "          -0.9156, -1.1006,  1.5750, -1.9596,  1.3206, -0.1898, -2.0190,\n",
            "          -1.0815, -1.2340,  2.6069,  0.2060, -0.0748,  0.9501, -2.0402,\n",
            "          -0.9197,  3.2078,  0.5746,  0.3339, -2.0814, -0.4529, -1.9060,\n",
            "          -0.8455, -1.1954]]], grad_fn=<SliceBackward0>)\n",
            "Epoch 1/1, Loss: 1.91409170627594\n",
            "1\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.5626,  1.3995, -0.7255,  ...,  1.7530, -0.5022, -0.9664],\n",
            "         [ 1.0477,  1.4467, -0.5307,  ...,  1.3882,  1.5236,  1.4879],\n",
            "         [ 0.1895,  1.7179,  1.5789,  ..., -1.4810,  1.5566,  0.0355]],\n",
            "\n",
            "        [[ 0.8018, -0.9895,  0.3088,  ...,  1.9658, -1.1088, -0.3854],\n",
            "         [ 0.1895,  1.7179,  1.5789,  ..., -1.4810,  1.5566,  0.0355],\n",
            "         [ 0.1814,  0.8652,  0.5850,  ...,  0.6662,  0.1892, -0.9259]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-2.9156e-02, -2.5358e-03,  2.3300e-04,  ..., -3.5523e-04,\n",
            "          -1.9718e-03,  2.4272e-02],\n",
            "         [-6.7832e-01, -1.8442e-01,  5.8714e-01,  ..., -1.1481e-02,\n",
            "          -6.1503e-01,  3.3429e-01],\n",
            "         [ 9.3331e-06, -9.5212e-05,  6.9010e-01,  ..., -1.5812e-04,\n",
            "          -2.9196e-01,  8.4825e-03]],\n",
            "\n",
            "        [[-3.5137e-04, -1.2303e-04, -8.7110e-04,  ..., -1.1744e-02,\n",
            "          -2.1974e-01,  8.2945e-05],\n",
            "         [ 7.0545e-05, -1.2016e-08, -1.1873e-03,  ...,  2.7922e-02,\n",
            "          -6.7476e-02,  1.6847e-01],\n",
            "         [ 6.5718e-06, -7.9621e-03, -6.3226e-01,  ...,  2.2025e-02,\n",
            "          -1.0055e-01,  2.7961e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.3207, -0.2853, -1.7682, -1.7886, -1.7142, -1.8652, -0.8463,\n",
            "          -1.8074, -1.2100, -1.8378, -0.7670, -1.9009, -1.9795, -1.7227,\n",
            "          -1.2634,  3.1971, -1.7458, -1.7008, -1.1439, -1.9158, -1.8837,\n",
            "          -1.8521, -1.6917, -1.7927, -1.8248, -1.6621, -1.7219, -1.6907,\n",
            "          -1.8559, -1.8435, -1.8538, -2.2505, -1.8221, -1.7659, -1.7769,\n",
            "          -1.7401, -1.7763, -1.0129, -1.8916, -0.0292, -1.7468, -0.8462,\n",
            "          -0.8311, -1.6550,  1.7935, -1.6965,  2.1223,  0.1776, -1.8225,\n",
            "          -0.6113, -1.2155,  1.8778,  0.3003, -0.1213,  1.0947, -1.8677,\n",
            "          -0.6310,  3.4276, -0.1969,  0.6042, -1.8128,  0.4821, -1.7791,\n",
            "          -0.4505, -1.0802],\n",
            "         [-1.0959, -1.4180, -4.4570, -4.1595, -4.3099, -4.6741,  0.0458,\n",
            "          -4.6827, -0.3416, -4.0785, -0.0132, -4.6640, -4.2070, -2.9163,\n",
            "          -0.6630, -2.5448, -4.7006, -4.8101, -2.6520, -4.3661, -4.6694,\n",
            "          -4.1642, -4.2091, -4.4923, -4.4117, -4.4924, -4.2193, -4.6104,\n",
            "          -4.5661, -4.4493, -4.4859, -2.7650, -4.6059, -4.1297, -4.3520,\n",
            "          -4.6578, -4.3083, -0.6301, -4.6414, -2.2585, -4.7698, -2.7176,\n",
            "          -1.3328,  3.9718, -2.9648, -4.3586,  3.7088,  8.7349, -4.5878,\n",
            "          -6.8346,  3.0269, -1.5776, -2.3063, -2.9871, -1.1820, -4.3893,\n",
            "          -0.6088, -0.6323,  0.6968, -2.7625, -3.9857, -3.7574, -4.4929,\n",
            "          -2.3185, -0.4555],\n",
            "         [-1.2900, -1.7331, -3.0018, -3.2738, -2.9627, -3.3522, -2.1203,\n",
            "          -3.1841, -3.6846, -3.1447, -0.3294, -3.3105, -3.4123, -2.9596,\n",
            "          -1.4735, -1.2024, -3.2293, -3.4931, -2.7001, -2.8525, -3.7485,\n",
            "          -2.9727, -3.2707, -2.7639, -3.4046, -3.0902, -3.2158, -3.1351,\n",
            "          -3.1552, -3.0781, -3.2730, -2.7061, -3.3634, -3.1482, -2.8844,\n",
            "          -3.2549, -3.0575, -0.8461, -3.0581, -0.9789, -2.9589, -1.6704,\n",
            "          -1.9492,  1.3107, -1.8893, -2.9332, -0.8930,  1.9788, -3.3122,\n",
            "          -2.9098,  0.1916, -0.1387, -0.0280, -0.1274,  0.3427, -3.3736,\n",
            "           0.3253,  0.9204,  6.8203, -1.7778, -3.2689, -2.8730, -3.1499,\n",
            "          -2.0882,  0.3584]],\n",
            "\n",
            "        [[-0.8341,  8.6032, -4.6502, -4.8640, -4.6301, -4.7838, -0.0749,\n",
            "          -4.7053, -0.6550, -4.6742, -0.3080, -4.6990, -4.8855, -2.9377,\n",
            "          -1.8896,  0.8403, -5.2071, -4.7091, -1.0908, -4.6229, -4.6673,\n",
            "          -4.9035, -4.8350, -4.9541, -4.8490, -4.7932, -4.8890, -4.8324,\n",
            "          -4.7941, -4.8913, -4.7746, -2.6010, -4.8808, -4.6880, -4.9240,\n",
            "          -4.7877, -4.8464, -0.5701, -4.8503, -0.2778, -4.8112, -3.3589,\n",
            "          -0.2682,  0.9477, -0.7913, -4.8292,  3.1022, -3.9029, -4.9454,\n",
            "          -1.5804, -1.2515, -2.6197, -0.7088, -0.5511, -1.9549, -4.8308,\n",
            "          -0.8041,  0.6479, -3.4202, -3.7903, -4.6177, -4.2780, -4.8589,\n",
            "          -1.8158, -2.2875],\n",
            "         [-1.2692,  1.3973, -1.4779, -1.7770, -1.8769, -1.6596, -0.2712,\n",
            "          -2.0195, -0.7685, -1.8439, -1.2314, -1.8584, -1.7101, -1.5804,\n",
            "          -0.9649,  3.9044, -1.9600, -2.1107, -0.8115, -1.7769, -2.0138,\n",
            "          -1.7639, -1.8041, -1.8181, -2.0705, -1.7244, -1.7829, -1.8198,\n",
            "          -1.7611, -1.8490, -1.9483, -1.3907, -1.9585, -1.7801, -2.0139,\n",
            "          -1.9454, -1.8587, -0.5932, -1.9227, -0.4165, -2.0162, -0.7946,\n",
            "          -0.4189,  0.3027, -0.7301, -1.9476,  0.4962, -2.0195, -1.6876,\n",
            "          -0.2734, -0.5057, -1.2471,  0.4078, -1.0017,  0.2517, -1.9715,\n",
            "           2.3192,  0.0700, -1.3889, -1.4616, -1.7346, -1.1411, -1.6931,\n",
            "          -0.7368,  0.9814],\n",
            "         [-0.9843,  2.8566, -6.5731, -7.0758, -6.9431, -7.2033,  0.0272,\n",
            "          -7.1471, -1.5633, -7.1541, -0.4558, -7.1519, -7.0279, -4.9863,\n",
            "          -3.6238, -0.4651, -6.8951, -6.8503, -2.7197, -7.0030, -7.2651,\n",
            "          -7.2000, -6.9455, -6.9087, -7.0815, -6.9009, -6.9739, -6.9377,\n",
            "          -6.7632, -6.9640, -7.2481, -4.7389, -7.0495, -7.0899, -7.1679,\n",
            "          -6.8981, -6.9907, -1.5842, -7.1195,  0.3401, -6.8020, -6.0600,\n",
            "          -1.0166,  6.2344, -1.3665, -7.0446, -2.6156, -0.1178, -6.8298,\n",
            "          -2.2412, -1.2844, -3.7586,  1.5893, -0.5747, -2.0642, -7.0393,\n",
            "           0.5095,  0.5722, -3.1404, -5.0235, -6.9690, -6.3825, -6.8046,\n",
            "          -4.0033, -3.2784]]], grad_fn=<SliceBackward0>)\n",
            "0\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 1.0319,  0.4320,  0.1920,  ...,  1.3233,  1.1018,  0.1235],\n",
            "         [-0.5643,  1.4055, -0.7244,  ...,  1.7513, -0.5060, -0.9692],\n",
            "         [-0.3959, -1.0665,  0.6244,  ...,  0.7239,  0.0573,  0.0348]],\n",
            "\n",
            "        [[ 0.3823, -0.4736,  0.4382,  ..., -0.1908,  0.4468,  2.2213],\n",
            "         [ 1.0319,  0.4320,  0.1920,  ...,  1.3233,  1.1018,  0.1235],\n",
            "         [-0.4273, -0.2499, -0.5559,  ...,  2.3467, -1.6713,  2.3197]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 7.5413e-03, -1.3228e-01, -7.0495e-01,  ...,  1.6471e-04,\n",
            "          -1.3206e-03,  5.7018e-05],\n",
            "         [-1.1952e-04, -6.1140e-04, -9.1645e-05,  ..., -1.7880e-05,\n",
            "          -2.8459e-06,  3.5636e-01],\n",
            "         [ 1.3083e-01,  2.2958e-01, -4.4726e-01,  ..., -1.8618e-01,\n",
            "           1.7407e-03,  6.2787e-01]],\n",
            "\n",
            "        [[ 1.0652e-03, -2.2802e-01, -3.4927e-01,  ..., -1.8679e-02,\n",
            "           3.3800e-01,  6.8602e-01],\n",
            "         [ 8.6117e-04, -7.0627e-05, -1.7344e-01,  ..., -3.0374e-06,\n",
            "           6.0203e-01,  3.0791e-04],\n",
            "         [ 1.0130e-01, -2.4800e-01, -1.0974e-02,  ..., -1.3910e-03,\n",
            "           1.3819e-02, -1.2191e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-8.3298e-01,  2.9712e+00, -5.6662e+00, -5.7160e+00, -5.7034e+00,\n",
            "          -5.7726e+00, -7.8058e-01, -5.6736e+00, -2.4162e+00, -5.6060e+00,\n",
            "          -1.1143e+00, -5.6077e+00, -5.6941e+00, -4.6153e+00, -3.4544e+00,\n",
            "          -3.0430e+00, -5.7992e+00, -5.8366e+00, -3.4631e+00, -5.6253e+00,\n",
            "          -5.7812e+00, -5.6582e+00, -5.6954e+00, -5.7800e+00, -5.6780e+00,\n",
            "          -5.5079e+00, -5.8560e+00, -5.7571e+00, -5.5883e+00, -5.6229e+00,\n",
            "          -5.8112e+00, -4.9582e+00, -5.6351e+00, -5.7477e+00, -5.7491e+00,\n",
            "          -5.5565e+00, -5.6146e+00, -2.5255e+00, -5.6791e+00,  2.8678e+00,\n",
            "          -5.6923e+00, -4.2915e+00,  1.4607e+00,  8.9632e-01,  1.0599e+00,\n",
            "          -5.6662e+00, -2.9442e+00, -8.7995e-02, -5.7487e+00, -6.2991e-02,\n",
            "          -2.9938e+00, -2.9265e+00,  2.7501e+00, -9.9038e-01, -4.0106e-01,\n",
            "          -5.7546e+00,  1.7031e+00, -3.9203e-01, -1.4797e+00, -3.1283e+00,\n",
            "          -5.6425e+00, -4.7588e+00, -5.7415e+00, -3.4905e+00, -2.8640e+00],\n",
            "         [-1.0630e+00, -6.9811e-01, -1.0956e+00, -1.0943e+00, -1.1568e+00,\n",
            "          -1.0465e+00, -2.5208e-01, -1.1436e+00, -5.7568e-01, -1.1933e+00,\n",
            "          -7.8372e-01, -1.2160e+00, -1.2493e+00, -1.0213e+00, -8.9413e-01,\n",
            "           2.7319e+00, -1.1057e+00, -1.0682e+00, -4.6119e-01, -1.2199e+00,\n",
            "          -1.3104e+00, -1.1994e+00, -1.0037e+00, -1.1542e+00, -1.1220e+00,\n",
            "          -1.0945e+00, -1.1054e+00, -1.1628e+00, -1.1545e+00, -1.1072e+00,\n",
            "          -1.1974e+00, -1.4466e+00, -1.1277e+00, -1.1388e+00, -1.0579e+00,\n",
            "          -1.1244e+00, -1.1468e+00, -7.6250e-01, -1.1461e+00, -3.2005e-01,\n",
            "          -1.2565e+00, -2.6470e-01, -5.4125e-01, -1.4426e+00,  1.2606e+00,\n",
            "          -1.0702e+00,  1.9154e+00,  3.4960e-01, -1.2351e+00, -9.0524e-02,\n",
            "          -8.4754e-01,  9.4248e-01, -2.4113e-01, -2.3567e-01,  1.1662e+00,\n",
            "          -1.0557e+00, -1.9905e-01,  1.9549e+00, -2.9193e-01,  8.7776e-01,\n",
            "          -1.1839e+00,  1.5236e+00, -1.2356e+00, -7.2816e-03, -1.2081e+00],\n",
            "         [-1.8630e+00, -1.4635e+00, -5.3531e+00, -4.9439e+00, -5.1855e+00,\n",
            "          -5.3636e+00, -4.6256e-01, -5.2342e+00, -8.5882e-01, -5.0525e+00,\n",
            "          -2.8675e+00, -5.4003e+00, -5.1977e+00, -3.4681e+00, -2.7043e+00,\n",
            "          -2.8642e+00, -5.3180e+00, -5.3393e+00, -2.1297e+00, -5.2655e+00,\n",
            "          -5.4998e+00, -5.2719e+00, -5.3777e+00, -5.2756e+00, -5.4446e+00,\n",
            "          -5.5021e+00, -5.1755e+00, -5.3108e+00, -5.0539e+00, -5.0239e+00,\n",
            "          -5.4556e+00, -3.0677e+00, -5.2771e+00, -5.3159e+00, -5.2351e+00,\n",
            "          -5.3771e+00, -5.1707e+00, -2.3497e+00, -5.3446e+00, -2.4092e+00,\n",
            "          -5.4142e+00, -9.0915e-01, -2.5399e+00,  7.7031e+00, -3.1038e+00,\n",
            "          -5.4787e+00, -2.4748e+00, -2.6453e-01, -5.1986e+00, -2.7005e+00,\n",
            "          -2.7304e+00, -4.0157e+00, -1.0903e+00, -1.3308e+00, -5.5490e-01,\n",
            "          -5.4562e+00,  5.0429e+00, -2.3651e+00,  8.8051e-01,  1.1871e-01,\n",
            "          -5.1495e+00, -4.1220e+00, -5.1136e+00, -2.7663e+00, -3.8004e-01]],\n",
            "\n",
            "        [[-2.3968e+00,  2.2886e+00, -1.2021e+01, -1.2189e+01, -1.1981e+01,\n",
            "          -1.2212e+01, -1.6592e+00, -1.2160e+01, -3.9281e+00, -1.2077e+01,\n",
            "          -1.1662e+00, -1.2028e+01, -1.2282e+01, -8.1403e+00, -7.1713e+00,\n",
            "          -7.3611e+00, -1.2196e+01, -1.1982e+01, -5.8309e+00, -1.2095e+01,\n",
            "          -1.2046e+01, -1.2256e+01, -1.2189e+01, -1.2357e+01, -1.2325e+01,\n",
            "          -1.2050e+01, -1.1922e+01, -1.2240e+01, -1.2060e+01, -1.2070e+01,\n",
            "          -1.2415e+01, -8.2557e+00, -1.2386e+01, -1.2212e+01, -1.2376e+01,\n",
            "          -1.1979e+01, -1.2193e+01, -4.4680e+00, -1.2399e+01,  2.5899e+00,\n",
            "          -1.1974e+01, -9.2414e+00, -4.3430e+00,  1.1893e+01, -1.4425e+00,\n",
            "          -1.2075e+01, -6.9741e+00,  1.2014e+00, -1.2039e+01, -2.9802e+00,\n",
            "          -4.4071e+00, -7.0284e+00,  1.3570e+00, -5.8663e-01, -4.3038e+00,\n",
            "          -1.2134e+01,  2.9921e+00, -8.3679e-01, -1.4895e+00, -7.8722e+00,\n",
            "          -1.2285e+01, -1.1781e+01, -1.2220e+01, -7.3080e+00, -4.7566e+00],\n",
            "         [-1.5181e+00,  7.3004e-01, -5.4320e+00, -5.3435e+00, -5.1319e+00,\n",
            "          -5.3183e+00, -7.7415e-01, -5.1725e+00, -2.0292e+00, -5.4099e+00,\n",
            "          -1.5068e+00, -5.1697e+00, -5.3393e+00, -4.1459e+00, -3.4044e+00,\n",
            "          -3.4937e+00, -5.2135e+00, -5.5070e+00, -3.1091e+00, -5.3136e+00,\n",
            "          -5.5231e+00, -5.2791e+00, -5.3767e+00, -5.4793e+00, -5.3846e+00,\n",
            "          -5.1762e+00, -5.3962e+00, -5.5680e+00, -5.1027e+00, -5.2144e+00,\n",
            "          -5.4645e+00, -4.5745e+00, -5.4795e+00, -5.3127e+00, -5.4024e+00,\n",
            "          -5.1676e+00, -5.5126e+00, -2.4293e+00, -5.3970e+00,  3.1752e+00,\n",
            "          -5.3599e+00, -3.4300e+00, -9.4002e-02,  1.8283e+00,  1.3621e+00,\n",
            "          -5.3049e+00, -2.9280e+00,  3.4056e-01, -5.3091e+00,  7.5742e-01,\n",
            "          -2.9698e+00, -2.8052e+00,  2.1606e+00, -3.3878e-02, -5.1259e-01,\n",
            "          -5.2468e+00,  2.6226e+00, -7.1719e-01, -1.3040e+00, -2.2785e+00,\n",
            "          -5.1958e+00, -4.5849e+00, -5.3387e+00, -2.8178e+00, -2.8476e+00],\n",
            "         [-3.6057e-01,  2.8929e+00, -5.6409e+00, -5.5663e+00, -5.5341e+00,\n",
            "          -5.6617e+00,  5.3293e-01, -5.5174e+00, -2.6284e+00, -5.7866e+00,\n",
            "          -3.9138e-01, -5.5501e+00, -5.5311e+00, -4.6891e+00, -2.6119e+00,\n",
            "          -3.8732e+00, -5.7005e+00, -5.8173e+00, -2.3650e+00, -5.6696e+00,\n",
            "          -5.8667e+00, -5.5531e+00, -5.6092e+00, -5.6011e+00, -5.6697e+00,\n",
            "          -5.9222e+00, -5.6243e+00, -5.7847e+00, -5.3899e+00, -5.4654e+00,\n",
            "          -5.7663e+00, -4.6524e+00, -5.8923e+00, -5.6859e+00, -5.5361e+00,\n",
            "          -5.5562e+00, -5.6366e+00, -1.3945e+00, -5.7878e+00, -1.3376e+00,\n",
            "          -5.8644e+00, -3.5415e+00, -1.4880e+00,  2.7092e+00, -2.2822e+00,\n",
            "          -5.6931e+00, -2.0822e+00, -3.3846e-01, -5.9070e+00, -9.8742e-01,\n",
            "          -2.0424e+00, -1.0321e+00, -1.2428e+00,  1.2218e+00, -1.4508e+00,\n",
            "          -5.7512e+00,  3.3477e-01,  1.3256e+00,  1.4603e+00, -1.9018e+00,\n",
            "          -5.4623e+00, -4.5909e+00, -5.6329e+00, -1.7748e+00, -1.4844e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.5638,  1.4069, -0.7226,  ...,  1.7513, -0.5060, -0.9718],\n",
            "         [-0.6115, -0.2362, -1.2386,  ...,  0.5062,  1.1249, -1.0549],\n",
            "         [ 0.8367,  0.3370, -0.4074,  ...,  0.1027,  0.1709,  0.2477]],\n",
            "\n",
            "        [[ 0.1819,  0.8699,  0.5914,  ...,  0.6580,  0.1931, -0.9221],\n",
            "         [ 1.0313,  0.4303,  0.1922,  ...,  1.3208,  1.1006,  0.1231],\n",
            "         [ 0.8367,  0.3370, -0.4074,  ...,  0.1027,  0.1709,  0.2477]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-2.2057e-02, -6.4105e-04,  1.5347e-04,  ..., -2.1929e-04,\n",
            "          -1.2967e-03,  1.3233e-01],\n",
            "         [-7.4076e-03,  4.0609e-03, -5.4631e-02,  ..., -6.9955e-03,\n",
            "          -6.0918e-01,  8.5018e-01],\n",
            "         [-1.0654e-04,  8.4148e-03, -3.2451e-05,  ...,  2.4567e-01,\n",
            "          -1.7362e-04, -6.0154e-01]],\n",
            "\n",
            "        [[ 3.1911e-04, -1.6669e-01, -6.3718e-01,  ...,  2.4099e-02,\n",
            "          -7.8608e-03,  9.1048e-02],\n",
            "         [ 3.3110e-04, -6.6316e-05, -5.1333e-01,  ...,  4.8353e-05,\n",
            "          -9.8845e-02,  2.3515e-05],\n",
            "         [ 1.3486e-03,  1.5575e-05, -2.8135e-05,  ...,  2.2850e-02,\n",
            "          -9.3579e-05, -1.4858e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.1912e+00, -6.3547e-01, -1.3319e+00, -1.3198e+00, -1.2888e+00,\n",
            "          -1.4119e+00, -6.5315e-01, -1.3754e+00, -9.7413e-01, -1.3563e+00,\n",
            "          -6.0857e-01, -1.4780e+00, -1.5037e+00, -1.3246e+00, -1.0453e+00,\n",
            "           4.1143e+00, -1.2552e+00, -1.2538e+00, -8.4041e-01, -1.4097e+00,\n",
            "          -1.4355e+00, -1.4041e+00, -1.2569e+00, -1.3280e+00, -1.3729e+00,\n",
            "          -1.2310e+00, -1.2714e+00, -1.2435e+00, -1.3676e+00, -1.4157e+00,\n",
            "          -1.3797e+00, -1.8948e+00, -1.2928e+00, -1.3152e+00, -1.3066e+00,\n",
            "          -1.3320e+00, -1.3545e+00, -9.3140e-01, -1.4465e+00, -2.4869e-01,\n",
            "          -1.3103e+00, -4.5205e-01, -8.1972e-01, -1.8824e+00,  1.9807e+00,\n",
            "          -1.2257e+00,  2.6620e+00,  1.2074e-01, -1.4063e+00, -4.8578e-01,\n",
            "          -8.5433e-01,  1.5585e+00, -4.3157e-02, -2.6050e-01,  8.9298e-01,\n",
            "          -1.3893e+00, -7.8345e-01,  2.8568e+00, -2.5068e-01,  6.5016e-01,\n",
            "          -1.3300e+00,  2.0084e+00, -1.3531e+00, -3.7121e-01, -9.1488e-01],\n",
            "         [-9.6275e-01,  1.7477e-01, -6.6464e+00, -6.4594e+00, -6.7279e+00,\n",
            "          -6.8769e+00, -1.4688e+00, -6.9089e+00, -7.4930e-01, -6.5576e+00,\n",
            "          -3.6395e+00, -7.0642e+00, -6.6840e+00, -5.7137e+00, -2.4990e+00,\n",
            "          -9.1834e-01, -6.7098e+00, -6.9892e+00, -3.1886e+00, -6.7907e+00,\n",
            "          -6.8278e+00, -6.8662e+00, -6.8867e+00, -7.0486e+00, -6.7886e+00,\n",
            "          -6.9663e+00, -6.6559e+00, -6.8215e+00, -6.5815e+00, -6.7625e+00,\n",
            "          -6.7366e+00, -6.2505e+00, -6.6611e+00, -6.6182e+00, -6.8476e+00,\n",
            "          -6.8073e+00, -6.7193e+00, -2.2341e+00, -6.7268e+00, -2.1193e+00,\n",
            "          -6.7134e+00, -3.8469e+00, -1.9273e+00,  5.9544e-01, -6.8292e-01,\n",
            "          -7.0552e+00, -9.4263e-01,  1.7697e-01, -6.9278e+00,  4.2700e+00,\n",
            "          -3.4917e+00, -1.1577e+00,  4.4477e+00, -9.0913e-01, -7.3037e-01,\n",
            "          -6.6520e+00,  3.2327e+00,  6.1772e-01,  5.2149e-02, -8.0136e-01,\n",
            "          -6.7974e+00, -4.1498e+00, -6.6783e+00, -2.2498e+00, -1.7316e-01],\n",
            "         [-1.5937e+00,  8.7056e-01, -4.8755e-01, -5.7047e-01, -5.0629e-01,\n",
            "          -8.2656e-01,  7.3844e-01, -6.2762e-01,  2.7949e+00, -5.2878e-01,\n",
            "           2.0096e+00, -5.6158e-01, -4.4110e-01, -5.5107e-01,  9.9109e-03,\n",
            "          -1.0940e+00, -7.3998e-01, -7.3036e-01, -2.6132e-01, -6.6657e-01,\n",
            "          -6.0017e-01, -3.9762e-01, -4.5248e-01, -5.2242e-01, -8.4675e-01,\n",
            "          -5.3926e-01, -5.7058e-01, -6.4703e-01, -5.9283e-01, -5.7175e-01,\n",
            "          -7.2441e-01, -3.8944e-01, -7.9429e-01, -4.9455e-01, -5.4261e-01,\n",
            "          -4.8721e-01, -4.5562e-01, -9.7457e-01, -6.4956e-01, -1.0579e+00,\n",
            "          -5.7782e-01,  3.1031e-01,  5.4133e-02, -4.3919e-01, -1.4174e+00,\n",
            "          -5.2202e-01,  3.2375e-01, -1.0534e-01, -6.2486e-01, -6.3855e-01,\n",
            "          -1.5532e-01, -7.8391e-01, -5.6553e-01, -1.4385e+00, -5.9572e-01,\n",
            "          -7.9977e-01, -6.7961e-01, -1.3028e+00, -2.3874e-01,  6.7224e-01,\n",
            "          -5.3429e-01, -4.0209e-01, -6.4300e-01,  5.2707e+00, -9.6198e-01]],\n",
            "\n",
            "        [[-1.0134e+00,  2.2958e+00, -7.3337e+00, -7.8032e+00, -7.6247e+00,\n",
            "          -7.8524e+00, -1.3803e-01, -7.8592e+00, -1.6571e+00, -7.7532e+00,\n",
            "          -6.5718e-02, -7.8233e+00, -7.6933e+00, -4.7150e+00, -4.1393e+00,\n",
            "          -3.1787e+00, -7.6291e+00, -7.6745e+00, -3.0065e+00, -7.7565e+00,\n",
            "          -7.7829e+00, -7.8423e+00, -7.8587e+00, -7.7537e+00, -7.8339e+00,\n",
            "          -7.6172e+00, -7.5993e+00, -7.6969e+00, -7.6522e+00, -7.7989e+00,\n",
            "          -8.0543e+00, -4.4628e+00, -7.8980e+00, -7.8263e+00, -7.9076e+00,\n",
            "          -7.6977e+00, -7.6947e+00, -2.1730e+00, -7.8604e+00,  8.3433e-01,\n",
            "          -7.5769e+00, -6.4862e+00, -1.4700e+00,  7.5737e+00, -1.2720e+00,\n",
            "          -7.8011e+00, -3.3549e+00,  3.3550e-01, -7.6069e+00, -2.7974e+00,\n",
            "          -1.8448e+00, -4.1744e+00,  1.6694e+00, -6.4216e-01, -2.5192e+00,\n",
            "          -7.5295e+00,  1.9339e-02,  5.9417e-04, -2.6136e+00, -5.6415e+00,\n",
            "          -7.5728e+00, -7.5485e+00, -7.5681e+00, -4.6236e+00, -3.3342e+00],\n",
            "         [-1.5175e+00,  1.9010e+00, -5.9501e+00, -5.9280e+00, -6.0207e+00,\n",
            "          -5.9633e+00, -5.7828e-01, -5.9643e+00, -2.1650e+00, -5.9310e+00,\n",
            "          -1.1661e+00, -5.8944e+00, -5.9403e+00, -4.7378e+00, -3.6024e+00,\n",
            "          -2.9442e+00, -5.9380e+00, -6.0883e+00, -3.7993e+00, -5.9684e+00,\n",
            "          -5.9695e+00, -6.0099e+00, -6.1964e+00, -6.0846e+00, -5.9097e+00,\n",
            "          -5.7831e+00, -5.9934e+00, -6.0461e+00, -5.8230e+00, -5.8939e+00,\n",
            "          -6.1508e+00, -5.0815e+00, -6.0207e+00, -6.0054e+00, -6.0546e+00,\n",
            "          -5.8105e+00, -6.0061e+00, -2.8176e+00, -5.8886e+00,  1.7226e+00,\n",
            "          -6.0188e+00, -4.3089e+00,  9.2285e-01,  1.6741e+00,  9.6967e-01,\n",
            "          -5.9563e+00, -3.1306e+00,  3.5613e-02, -5.9401e+00,  1.7550e-01,\n",
            "          -2.7625e+00, -3.0071e+00,  3.7638e+00, -1.2593e+00, -4.2377e-01,\n",
            "          -5.7385e+00,  1.7303e+00, -3.5331e-01, -1.9595e+00, -2.8525e+00,\n",
            "          -5.7302e+00, -4.9974e+00, -5.8671e+00, -3.5047e+00, -2.5533e+00],\n",
            "         [-2.1105e+00,  1.3529e+00, -2.6764e+00, -2.6144e+00, -2.8846e+00,\n",
            "          -2.9384e+00,  1.5185e-01, -2.8367e+00, -6.7752e-01, -3.0428e+00,\n",
            "           5.8039e+00, -2.6984e+00, -2.6114e+00, -3.2980e+00, -1.6727e+00,\n",
            "          -3.6283e+00, -3.0001e+00, -2.6585e+00, -2.3721e+00, -2.6393e+00,\n",
            "          -2.9841e+00, -2.7748e+00, -2.7012e+00, -2.9032e+00, -2.9204e+00,\n",
            "          -2.6004e+00, -2.9389e+00, -2.7262e+00, -2.7075e+00, -3.0148e+00,\n",
            "          -2.7785e+00, -3.6529e+00, -2.8518e+00, -2.7031e+00, -2.6756e+00,\n",
            "          -2.5909e+00, -2.8535e+00, -1.7076e+00, -2.7941e+00,  2.4786e-01,\n",
            "          -2.6345e+00, -3.1824e+00, -9.9176e-01, -6.3093e-01, -1.7638e-01,\n",
            "          -2.8157e+00, -2.1803e+00, -4.5740e-01, -2.8283e+00,  2.8821e-01,\n",
            "          -5.7188e-01, -1.4041e-01, -6.1658e-02, -5.2603e-01, -3.6367e+00,\n",
            "          -3.1882e+00, -1.9446e+00,  1.1185e+00, -1.6902e+00, -3.0051e+00,\n",
            "          -2.7399e+00, -2.9986e+00, -2.7459e+00,  3.0412e+00, -2.1383e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.6103, -0.2349, -1.2356,  ...,  0.5077,  1.1217, -1.0543],\n",
            "         [-1.5334,  0.2116, -0.2199,  ...,  1.6216, -0.1737, -0.9124],\n",
            "         [-0.9554, -0.6744,  0.2939,  ...,  1.2250, -1.5991,  1.7408]],\n",
            "\n",
            "        [[ 1.7569, -1.4401, -0.0982,  ...,  0.6282,  0.5647, -0.5731],\n",
            "         [-0.5632,  1.4064, -0.7219,  ...,  1.7525, -0.5068, -0.9745],\n",
            "         [-0.6103, -0.2349, -1.2356,  ...,  0.5077,  1.1217, -1.0543]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-1.8305e-03,  1.9710e-04, -3.3385e-02,  ..., -2.2013e-02,\n",
            "          -4.2220e-01,  7.3972e-01],\n",
            "         [-5.8685e-08,  3.3437e-04, -2.4797e-06,  ...,  1.6133e-02,\n",
            "           2.9767e-01,  7.5869e-01],\n",
            "         [ 5.6754e-01,  1.3560e-04, -1.1242e-04,  ...,  1.8800e-02,\n",
            "          -6.2422e-02,  4.3186e-03]],\n",
            "\n",
            "        [[ 1.0842e-01, -4.3891e-01, -6.4560e-01,  ...,  6.4167e-01,\n",
            "           4.4216e-02, -7.5758e-01],\n",
            "         [-6.9359e-02, -8.8297e-04, -1.4421e-04,  ...,  1.7404e-05,\n",
            "           8.5112e-03,  7.5824e-01],\n",
            "         [-4.0597e-03,  3.0602e-04, -1.1447e-02,  ...,  7.6191e-03,\n",
            "          -1.8885e-01,  9.3592e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ -0.6064,   1.1541,  -7.4265,  -7.3030,  -7.4753,  -7.7036,  -1.0891,\n",
            "           -7.6203,  -1.0630,  -7.3110,  -3.9255,  -7.5824,  -7.3826,  -6.1381,\n",
            "           -2.5373,  -3.4783,  -7.4512,  -7.6498,  -3.2412,  -7.5819,  -7.5921,\n",
            "           -7.4682,  -7.6209,  -7.6591,  -7.4095,  -7.6213,  -7.4851,  -7.6479,\n",
            "           -7.4521,  -7.4846,  -7.5579,  -6.8008,  -7.5238,  -7.4518,  -7.5176,\n",
            "           -7.4845,  -7.4332,  -2.2526,  -7.5650,  -1.5399,  -7.6396,  -5.0456,\n",
            "           -1.4827,   0.2316,  -1.0309,  -7.7291,  -2.3893,  -0.0784,  -7.6891,\n",
            "            7.0297,  -4.0372,  -2.2785,   3.4557,  -0.2174,  -1.3326,  -7.5551,\n",
            "            4.2431,   0.0559,  -0.6291,  -2.0471,  -7.5201,  -5.6602,  -7.3555,\n",
            "           -2.4641,  -0.3126],\n",
            "         [  0.9524,   0.0379,  -1.9308,  -1.8390,  -1.7617,  -2.1227,   2.2537,\n",
            "           -1.9876,   9.2128,  -1.9280,  -1.2559,  -1.8749,  -2.0468,  -1.0620,\n",
            "           -0.6558,  -1.7254,  -1.8853,  -1.8764,  -0.4390,  -2.2805,  -1.9741,\n",
            "           -2.0404,  -1.9258,  -1.8003,  -1.9790,  -1.7752,  -1.8010,  -1.8981,\n",
            "           -2.0698,  -1.8413,  -1.9911,  -1.2166,  -1.9712,  -1.8884,  -1.8000,\n",
            "           -1.9781,  -1.9199,  -0.6321,  -2.0442,  -2.0751,  -2.0284,  -0.9060,\n",
            "            0.5063,  -0.2641,  -2.4288,  -1.9368,  -0.1778,  -0.3602,  -2.0847,\n",
            "           -1.0979,  -1.0412,  -1.9596,  -1.3861,  -1.7627,  -1.0621,  -1.8404,\n",
            "            0.3407,  -1.7030,  -1.3134,  -0.2862,  -1.9359,  -2.0300,  -2.0315,\n",
            "            1.6090,  -1.1133],\n",
            "         [ 12.3427,  -0.5850,  -5.1208,  -4.9801,  -5.3734,  -5.1775,   1.6188,\n",
            "           -5.1430,   4.1344,  -5.2578,   0.5983,  -5.1560,  -5.3353,  -4.8106,\n",
            "           -3.9544,  -5.2616,  -5.1598,  -5.2427,  -2.5871,  -5.2402,  -5.1221,\n",
            "           -5.2078,  -5.1431,  -5.2361,  -5.4275,  -5.1213,  -5.2394,  -5.2186,\n",
            "           -5.3634,  -4.9014,  -5.1934,  -5.2686,  -5.0152,  -5.3617,  -5.4007,\n",
            "           -4.8006,  -4.9829,   0.3868,  -4.8616,  -1.0194,  -5.2743,  -6.4917,\n",
            "           -1.0991,  -2.3088,  -2.7280,  -5.3509,  -4.6964,  -1.8935,  -5.2983,\n",
            "           -0.3905,  -2.0224,  -1.8210,  -0.7937,  -1.3162,  -5.7154,  -5.5198,\n",
            "            0.6229,  -0.7184,  -2.2176,  -5.8861,  -5.2796,  -6.6472,  -5.0889,\n",
            "           -1.7099,  -1.4244]],\n",
            "\n",
            "        [[ -1.5846,  11.5827, -11.3858, -11.4070, -11.5832, -11.4405,   0.6692,\n",
            "          -11.3439,  -2.1713, -11.5774,  -1.0564, -11.1840, -11.5314,  -8.0774,\n",
            "           -6.5156,  -7.4387, -11.4372, -11.3565,  -5.4346, -11.4997, -11.5767,\n",
            "          -11.3179, -11.6444, -11.3242, -11.3113, -11.3683, -11.2819, -11.2269,\n",
            "          -11.4309, -11.6556, -11.5889,  -9.0061, -11.8697, -11.2861, -11.3620,\n",
            "          -11.2279, -11.3346,  -4.7232, -11.4091,   1.6650, -11.2672,  -7.8456,\n",
            "           -1.0232,   3.9888,  -1.2967, -11.6210,  -7.6258,  -1.7691, -11.6654,\n",
            "           -0.6588,  -5.5920,  -6.4358,   1.4438,  -0.5932,  -1.2757, -11.8556,\n",
            "            1.1568,  -1.0829,  -0.5254,  -5.5576, -11.3653, -11.0774, -11.6852,\n",
            "           -4.3377,  -4.9587],\n",
            "         [ -1.6819,  -0.3712,  -3.4809,  -3.6165,  -3.4515,  -3.2980,  -0.6477,\n",
            "           -3.3199,  -1.9870,  -3.7439,  -1.4693,  -3.3978,  -3.6944,  -3.3045,\n",
            "           -2.5639,   0.2454,  -3.5183,  -3.4882,  -2.1662,  -3.4629,  -3.6402,\n",
            "           -3.4892,  -3.3859,  -3.4850,  -3.5431,  -3.4153,  -3.5019,  -3.3538,\n",
            "           -3.5007,  -3.4140,  -3.7358,  -3.7060,  -3.5509,  -3.3563,  -3.5871,\n",
            "           -3.3141,  -3.5745,  -1.5785,  -3.5246,   2.0768,  -3.3674,  -1.7220,\n",
            "           -1.5691,  -0.2546,   2.9827,  -3.3387,   0.5690,  -0.1233,  -3.5609,\n",
            "           -0.1925,  -2.3205,  -0.3092,   0.5464,  -0.8267,   0.4158,  -3.5551,\n",
            "            0.6628,   2.1599,  -0.7385,  -0.9932,  -3.6401,  -0.3207,  -3.6728,\n",
            "           -1.8463,  -1.6553],\n",
            "         [ -1.3039,   0.3513,  -7.8935,  -7.7174,  -7.7295,  -7.9912,  -1.1139,\n",
            "           -7.8639,  -0.9892,  -8.0339,  -4.2789,  -7.8969,  -7.9499,  -6.9426,\n",
            "           -3.5961,  -3.2391,  -7.8366,  -8.2470,  -4.2924,  -8.0049,  -7.9590,\n",
            "           -8.0299,  -8.1632,  -8.1829,  -7.8157,  -8.0068,  -7.8470,  -8.0255,\n",
            "           -7.7404,  -7.8803,  -8.0471,  -7.6960,  -8.0326,  -7.6351,  -8.0563,\n",
            "           -7.7881,  -7.8298,  -2.8998,  -8.0106,  -0.7901,  -7.9335,  -4.7397,\n",
            "           -2.3900,   0.8205,  -0.1697,  -8.1570,  -2.6700,   0.0476,  -8.1300,\n",
            "            5.8003,  -4.5182,  -2.8147,   4.3041,  -1.2605,  -0.9923,  -7.9231,\n",
            "            4.6904,   0.1187,  -0.7672,  -2.2690,  -8.0778,  -5.5803,  -7.9244,\n",
            "           -2.9610,  -0.4181]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.4132, -0.2407, -0.5495,  ...,  2.3556, -1.6827,  2.3086],\n",
            "         [-0.4168,  1.0532,  0.5603,  ..., -0.3851, -2.8680, -1.9424],\n",
            "         [ 1.4168, -0.1980,  0.3387,  ..., -0.8323,  0.4565,  0.5729]],\n",
            "\n",
            "        [[-0.5521, -1.3721,  0.6032,  ...,  0.7402,  1.3846,  0.1633],\n",
            "         [ 0.5821, -0.8577,  0.3889,  ..., -0.8705, -0.2796,  0.1116],\n",
            "         [ 0.1892, -0.5819,  0.5046,  ...,  1.0660,  1.2660, -1.0786]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 4.9715e-01, -6.1620e-01,  1.1225e-03,  ..., -7.3604e-03,\n",
            "          -5.2016e-01, -6.0997e-03],\n",
            "         [ 6.8819e-01, -4.9989e-02, -2.3592e-01,  ...,  3.1093e-02,\n",
            "          -4.0153e-01,  4.0314e-02],\n",
            "         [ 9.1513e-01, -2.0324e-01, -7.7613e-01,  ..., -2.2327e-01,\n",
            "          -2.2444e-01,  7.6296e-01]],\n",
            "\n",
            "        [[ 3.9727e-02,  3.1023e-03, -7.4430e-01,  ..., -4.0010e-01,\n",
            "          -3.9013e-01,  7.5205e-01],\n",
            "         [ 1.7171e-02, -2.7989e-02,  2.4720e-06,  ..., -7.7123e-01,\n",
            "           1.7124e-05, -3.2932e-09],\n",
            "         [ 9.1032e-01, -9.5468e-01,  2.6768e-01,  ..., -2.8531e-01,\n",
            "           1.1285e-02,  1.7948e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-2.1488e-01,  2.7498e+00, -5.3879e+00, -5.4417e+00, -5.4644e+00,\n",
            "          -5.6464e+00,  2.4155e+00, -5.5173e+00, -2.6952e+00, -5.6604e+00,\n",
            "          -2.6160e-01, -5.5608e+00, -5.4524e+00, -4.3026e+00, -2.3578e+00,\n",
            "          -3.3352e+00, -5.6129e+00, -5.5441e+00, -2.1145e+00, -5.3919e+00,\n",
            "          -5.6207e+00, -5.3249e+00, -5.4400e+00, -5.3697e+00, -5.5016e+00,\n",
            "          -5.7707e+00, -5.3327e+00, -5.5537e+00, -5.2058e+00, -5.5011e+00,\n",
            "          -5.6983e+00, -4.0971e+00, -5.7195e+00, -5.6397e+00, -5.4115e+00,\n",
            "          -5.4572e+00, -5.4106e+00, -1.2688e+00, -5.7387e+00, -2.1833e+00,\n",
            "          -5.5621e+00, -2.9115e+00, -1.6216e+00,  2.9938e+00, -3.0026e+00,\n",
            "          -5.7754e+00, -1.1347e+00, -1.0253e+00, -5.5461e+00, -2.6505e+00,\n",
            "          -1.4942e+00, -7.4458e-01, -1.9608e+00,  1.5375e+00, -2.2164e+00,\n",
            "          -5.5413e+00, -7.0327e-01,  1.2173e+00,  2.1675e+00, -2.2148e+00,\n",
            "          -5.3482e+00, -3.9804e+00, -5.5015e+00, -2.2833e+00, -1.0267e+00],\n",
            "         [-1.1650e+00,  2.3931e+00,  1.0244e+00,  9.8499e-01,  9.3051e-01,\n",
            "           1.1047e+00, -2.5921e-01,  8.4735e-01, -2.9516e-01,  9.1679e-01,\n",
            "           4.5924e-01,  8.9975e-01,  1.1705e+00,  1.1368e+00,  8.3571e-01,\n",
            "          -3.6189e-02,  1.0372e+00,  8.4050e-01,  2.6826e-01,  9.0313e-01,\n",
            "           1.0144e+00,  9.8367e-01,  9.4251e-01,  9.6315e-01,  9.0724e-01,\n",
            "           9.6249e-01,  1.2103e+00,  9.6659e-01,  1.0717e+00,  1.1352e+00,\n",
            "           8.3075e-01,  1.3432e+00,  9.9150e-01,  1.0699e+00,  9.8161e-01,\n",
            "           1.0466e+00,  7.5831e-01, -3.7716e-02,  9.9729e-01, -8.5072e-01,\n",
            "           1.1104e+00,  9.5578e+00,  5.8471e-01, -1.7865e-01, -7.4545e-01,\n",
            "           7.7505e-01,  1.1844e+00, -2.0248e+00,  8.6389e-01, -1.1297e+00,\n",
            "           9.9475e-01,  8.3750e-01, -2.8448e-01, -1.3844e-01,  5.2619e-01,\n",
            "           9.1966e-01, -1.9865e+00, -1.8706e+00,  3.0374e+00,  3.1939e+00,\n",
            "           1.0987e+00,  2.0667e+00,  9.1649e-01,  7.8706e-01,  4.0907e-01],\n",
            "         [-2.0398e+00,  5.8215e+00, -1.1842e+01, -1.2143e+01, -1.2095e+01,\n",
            "          -1.1730e+01, -1.8090e+00, -1.2067e+01, -5.3391e+00, -1.1979e+01,\n",
            "           2.0304e-01, -1.2049e+01, -1.1907e+01, -7.1285e+00, -8.7955e+00,\n",
            "          -7.9568e+00, -1.2258e+01, -1.2020e+01, -6.8391e+00, -1.2025e+01,\n",
            "          -1.1978e+01, -1.1970e+01, -1.2337e+01, -1.2153e+01, -1.2085e+01,\n",
            "          -1.1736e+01, -1.1937e+01, -1.2159e+01, -1.2077e+01, -1.1769e+01,\n",
            "          -1.2477e+01, -7.1779e+00, -1.2223e+01, -1.2491e+01, -1.2179e+01,\n",
            "          -1.1916e+01, -1.1851e+01, -4.2272e+00, -1.1933e+01,  2.1504e+00,\n",
            "          -1.1870e+01, -6.0884e+00, -3.9342e+00,  1.3566e+01,  5.1339e-01,\n",
            "          -1.2250e+01, -7.0336e+00, -1.7287e+00, -1.1616e+01, -5.3545e+00,\n",
            "          -4.2438e+00, -7.7828e+00,  2.7285e+00, -1.6808e+00, -5.5910e+00,\n",
            "          -1.1930e+01, -9.9161e-01, -1.6050e+00, -1.1850e+00, -7.5995e+00,\n",
            "          -1.1925e+01, -1.2150e+01, -1.1893e+01, -9.5530e+00, -3.4919e+00]],\n",
            "\n",
            "        [[ 1.3072e+01, -2.4617e-01, -2.9268e+00, -3.0134e+00, -3.0982e+00,\n",
            "          -2.9027e+00,  6.8365e-01, -3.2367e+00,  6.4761e-01, -3.1464e+00,\n",
            "          -3.6056e+00, -3.6623e+00, -3.1562e+00,  7.5812e-01,  8.6080e-02,\n",
            "          -2.8484e+00, -3.1502e+00, -2.9914e+00,  3.5233e+00, -3.2125e+00,\n",
            "          -2.9253e+00, -2.9877e+00, -2.8452e+00, -2.9897e+00, -2.9275e+00,\n",
            "          -3.2644e+00, -3.2014e+00, -3.2641e+00, -3.4014e+00, -3.3149e+00,\n",
            "          -3.4558e+00,  2.1893e+00, -2.9266e+00, -3.1816e+00, -3.2713e+00,\n",
            "          -3.4899e+00, -3.0958e+00,  4.8317e+00, -3.1233e+00, -3.4045e+00,\n",
            "          -3.3961e+00, -3.8060e+00, -5.8745e-01,  3.8434e-01, -3.3381e+00,\n",
            "          -3.1524e+00, -8.7115e-01,  1.2113e+00, -3.4478e+00, -7.6577e-01,\n",
            "          -2.0919e+00,  1.4912e-01, -3.4378e+00,  1.8374e+00, -3.0579e+00,\n",
            "          -3.3204e+00,  2.0904e+00, -8.7957e-01, -2.4201e-01, -2.3434e+00,\n",
            "          -2.5847e+00, -3.3380e+00, -2.6103e+00, -1.7961e+00,  1.1611e-02],\n",
            "         [ 2.0820e+00, -1.4637e+00, -4.0617e-01, -5.7606e-01, -5.7137e-01,\n",
            "          -5.6407e-01, -2.2184e-01, -5.4669e-01, -1.0186e+00, -5.1096e-01,\n",
            "          -2.7349e-01, -7.9004e-01, -5.9445e-01,  4.4581e+00,  1.4320e+00,\n",
            "           1.2843e-01, -5.5229e-01, -7.7740e-01,  2.6147e+00, -4.5912e-01,\n",
            "          -5.0954e-01, -6.6282e-01, -4.2538e-01, -4.6056e-01, -5.6458e-01,\n",
            "          -6.6001e-01, -2.6888e-01, -6.5899e-01, -6.2318e-01, -5.6578e-01,\n",
            "          -7.6066e-01,  8.3953e+00, -4.4611e-01, -5.4263e-01, -5.3413e-01,\n",
            "          -6.8234e-01, -6.1510e-01,  4.4527e-01, -5.8746e-01, -1.2520e+00,\n",
            "          -6.8999e-01,  5.6327e-02,  4.1402e-02,  6.9019e-01, -8.2781e-01,\n",
            "          -5.6186e-01, -3.2649e-01, -7.0505e-01, -4.9840e-01, -1.1896e+00,\n",
            "           2.1430e+00, -2.1992e-01, -1.3785e+00,  3.2084e-01,  1.7074e+00,\n",
            "          -7.3380e-01, -6.3413e-01, -1.2110e+00, -1.9370e-01,  1.0893e+00,\n",
            "          -3.4904e-01, -6.1014e-01, -4.0093e-01, -1.0117e+00, -1.0449e+00],\n",
            "         [-2.5997e+00,  3.5635e+00, -9.9268e+00, -9.9370e+00, -1.0161e+01,\n",
            "          -1.0010e+01, -2.0164e+00, -1.0094e+01, -5.7534e+00, -9.7305e+00,\n",
            "          -3.2403e+00, -1.0062e+01, -9.9738e+00, -6.4083e-01, -6.2194e+00,\n",
            "          -2.6780e+00, -1.0315e+01, -1.0495e+01, -3.1672e+00, -9.7772e+00,\n",
            "          -9.9411e+00, -9.5085e+00, -1.0381e+01, -1.0229e+01, -1.0165e+01,\n",
            "          -1.0163e+01, -9.9738e+00, -1.0067e+01, -1.0054e+01, -9.6335e+00,\n",
            "          -1.0379e+01, -6.4020e-01, -1.0174e+01, -9.9312e+00, -1.0127e+01,\n",
            "          -1.0048e+01, -9.7228e+00, -4.5004e+00, -1.0163e+01, -3.0247e-01,\n",
            "          -1.0035e+01, -5.1419e+00, -1.4317e+00,  1.2224e+01,  3.3899e-01,\n",
            "          -1.0408e+01, -4.1884e+00, -3.3444e+00, -1.0027e+01, -3.3376e+00,\n",
            "          -1.6714e-01, -7.3436e+00,  3.8299e-01,  2.7384e+00,  4.1797e+00,\n",
            "          -1.0096e+01,  1.1484e+00, -2.4594e+00,  4.6559e-01, -5.2693e-01,\n",
            "          -9.7056e+00, -8.3821e+00, -9.8614e+00, -8.6436e+00, -4.7029e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.2358, -1.2639, -0.6825,  ...,  1.5748, -0.7624,  1.0368],\n",
            "         [ 0.8228, -0.9649,  0.2974,  ...,  1.9848, -1.0868, -0.3852],\n",
            "         [-0.5622,  1.4085, -0.7214,  ...,  1.7539, -0.5064, -0.9771]],\n",
            "\n",
            "        [[ 1.6861,  1.1401, -1.7487,  ...,  0.3965,  1.3198, -1.1924],\n",
            "         [ 0.1870,  1.7340,  1.5902,  ..., -1.4991,  1.5632,  0.0512],\n",
            "         [-0.4086, -0.2388, -0.5473,  ...,  2.3584, -1.6838,  2.3055]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 5.7653e-02, -7.1539e-01, -1.4314e-01,  ...,  3.6624e-01,\n",
            "           3.0933e-02, -4.3492e-01],\n",
            "         [ 7.2412e-01, -2.0312e-01, -7.5867e-01,  ...,  6.6609e-01,\n",
            "          -9.0930e-03, -6.0661e-01],\n",
            "         [-1.2079e-03, -1.5795e-04, -6.5864e-04,  ..., -7.2260e-05,\n",
            "          -2.9124e-03,  7.1324e-01]],\n",
            "\n",
            "        [[-1.7924e-05, -3.2570e-03,  6.9287e-01,  ..., -6.5556e-05,\n",
            "          -9.8296e-02,  1.6001e-01],\n",
            "         [-9.7113e-04, -7.8326e-07,  8.7661e-02,  ...,  6.9235e-05,\n",
            "          -1.2436e-01,  3.4007e-01],\n",
            "         [ 1.9530e-03, -2.5211e-01,  6.7186e-01,  ...,  4.3530e-04,\n",
            "          -8.7902e-01,  1.3457e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-3.9576e-01,  1.1785e+00, -3.6156e+00, -3.6209e+00, -3.6769e+00,\n",
            "          -3.7485e+00,  2.5609e-02, -3.7971e+00, -2.2015e+00, -3.4371e+00,\n",
            "          -1.7426e+00, -3.7851e+00, -3.4348e+00, -2.3957e+00, -1.5450e+00,\n",
            "          -1.2104e+00, -3.6201e+00, -3.7025e+00, -1.4378e+00, -3.4964e+00,\n",
            "          -3.7131e+00, -3.3194e+00, -3.6390e+00, -3.6679e+00, -3.7436e+00,\n",
            "          -3.6258e+00, -3.4562e+00, -3.2785e+00, -3.6016e+00, -3.6270e+00,\n",
            "          -3.9376e+00, -1.5255e+00, -3.6768e+00, -3.7014e+00, -3.4525e+00,\n",
            "          -3.6716e+00, -3.4012e+00, -1.1835e+00, -3.5968e+00, -2.6093e+00,\n",
            "          -3.6854e+00,  4.4293e-01, -5.2407e-01,  2.3344e+00, -3.4760e+00,\n",
            "          -3.6935e+00, -3.1087e+00, -2.3661e+00, -3.5707e+00, -3.4774e+00,\n",
            "           3.9849e-01, -1.2275e+00, -2.1351e+00,  1.4589e-02,  4.9171e+00,\n",
            "          -3.7865e+00, -8.2177e-01, -2.5507e+00,  8.0827e+00,  1.0277e+00,\n",
            "          -3.3566e+00, -2.7387e+00, -3.6905e+00, -2.8832e+00,  1.0318e+00],\n",
            "         [-1.2130e+00,  8.7598e+00, -7.3312e+00, -7.3763e+00, -7.0537e+00,\n",
            "          -7.1620e+00, -5.7897e-01, -6.9540e+00, -2.9420e+00, -7.1553e+00,\n",
            "           7.7239e-02, -7.1938e+00, -7.4910e+00, -3.8710e+00, -3.7608e+00,\n",
            "          -5.3502e-01, -7.5943e+00, -7.2377e+00, -2.3378e+00, -7.1206e+00,\n",
            "          -7.1805e+00, -7.3649e+00, -7.3723e+00, -7.4407e+00, -7.3940e+00,\n",
            "          -7.3942e+00, -7.2853e+00, -7.3281e+00, -7.2525e+00, -7.3090e+00,\n",
            "          -7.3341e+00, -3.8776e+00, -7.2975e+00, -7.3305e+00, -7.6312e+00,\n",
            "          -7.1515e+00, -7.1696e+00, -1.6868e+00, -7.2855e+00,  8.1681e-01,\n",
            "          -7.2304e+00, -3.9621e+00, -5.1802e-01,  3.4779e+00, -3.3339e-01,\n",
            "          -7.2811e+00, -6.1239e-01, -1.3342e+00, -7.4490e+00, -2.8285e+00,\n",
            "          -1.7434e+00, -4.7429e+00, -2.3498e-03, -3.9469e-01, -2.0079e+00,\n",
            "          -7.5748e+00, -1.5673e+00, -2.4073e-01, -2.0987e+00, -5.0947e+00,\n",
            "          -6.9293e+00, -6.3652e+00, -7.2983e+00, -4.0399e+00, -4.1953e+00],\n",
            "         [-1.2503e+00, -1.3484e-01, -1.9719e+00, -2.0634e+00, -2.0288e+00,\n",
            "          -1.9460e+00, -6.5750e-01, -1.9938e+00, -1.3279e+00, -2.0878e+00,\n",
            "          -6.3704e-01, -2.2956e+00, -2.2078e+00, -1.1186e+00, -1.2228e+00,\n",
            "           6.4483e+00, -1.9235e+00, -2.1004e+00, -6.0256e-01, -2.1708e+00,\n",
            "          -1.9787e+00, -2.0382e+00, -1.9551e+00, -2.2628e+00, -2.0266e+00,\n",
            "          -2.1314e+00, -1.9893e+00, -2.0699e+00, -2.1922e+00, -2.1880e+00,\n",
            "          -2.2271e+00, -1.6527e+00, -2.0457e+00, -2.1207e+00, -2.0908e+00,\n",
            "          -1.9907e+00, -2.1591e+00, -1.1537e+00, -2.2317e+00,  4.6934e-01,\n",
            "          -2.0768e+00, -4.4068e-01, -1.0276e+00, -1.4198e+00,  1.9175e+00,\n",
            "          -2.2642e+00,  2.9522e+00, -7.1809e-02, -2.1402e+00, -1.4006e+00,\n",
            "          -2.6260e-01,  5.6729e-01, -1.0356e+00, -8.5321e-01,  1.0669e+00,\n",
            "          -2.1155e+00, -9.4290e-01,  2.0314e+00, -5.0596e-01, -6.7217e-01,\n",
            "          -2.0884e+00,  2.4618e+00, -2.0915e+00, -1.6015e+00, -9.6661e-01]],\n",
            "\n",
            "        [[-1.9312e+00, -3.5570e-01, -6.6978e+00, -6.3487e+00, -6.7196e+00,\n",
            "          -6.9167e+00, -1.0705e+00, -6.6559e+00, -1.6924e+00, -6.4396e+00,\n",
            "          -1.1554e+00, -6.8444e+00, -6.7725e+00, -1.5848e+00, -3.5850e+00,\n",
            "          -2.2618e+00, -6.5950e+00, -6.7250e+00, -2.9284e+00, -6.5563e+00,\n",
            "          -6.6885e+00, -6.5865e+00, -6.5813e+00, -6.7375e+00, -6.7713e+00,\n",
            "          -6.9112e+00, -6.4336e+00, -6.8276e+00, -6.4987e+00, -6.7932e+00,\n",
            "          -6.8259e+00, -2.3945e+00, -6.7105e+00, -6.6422e+00, -6.6340e+00,\n",
            "          -6.7015e+00, -6.7037e+00, -3.4091e+00, -6.5221e+00, -7.6013e-01,\n",
            "          -6.7484e+00, -4.8317e+00, -1.5958e+00,  2.5626e+00, -8.5286e-01,\n",
            "          -6.6356e+00, -2.9474e-01,  7.9719e+00, -6.8258e+00, -2.0494e+00,\n",
            "           3.3208e-01, -2.9893e+00, -1.1885e+00, -4.1960e-01,  9.8686e-01,\n",
            "          -6.6544e+00,  5.4576e-01, -1.0794e+00, -6.0888e-01, -1.3128e+00,\n",
            "          -6.5915e+00, -4.6941e+00, -6.5831e+00, -3.5118e+00, -1.9325e+00],\n",
            "         [-2.7461e+00, -1.4377e+00, -6.1336e+00, -6.0875e+00, -6.1909e+00,\n",
            "          -6.3694e+00, -1.2012e+00, -6.4157e+00, -2.2984e+00, -6.0240e+00,\n",
            "          -2.6286e+00, -6.1125e+00, -6.1213e+00, -5.8710e+00, -3.9187e+00,\n",
            "          -4.5119e+00, -6.2955e+00, -6.4472e+00, -4.4511e+00, -5.9223e+00,\n",
            "          -6.5085e+00, -6.0029e+00, -6.2176e+00, -5.8650e+00, -6.0456e+00,\n",
            "          -6.2101e+00, -5.8308e+00, -6.1693e+00, -6.0008e+00, -6.0160e+00,\n",
            "          -6.1374e+00, -5.5648e+00, -6.1218e+00, -5.9830e+00, -5.9986e+00,\n",
            "          -6.0837e+00, -5.8915e+00, -2.2383e+00, -6.3720e+00, -1.7590e+00,\n",
            "          -6.1822e+00, -1.1083e+00, -2.1445e+00,  8.0936e-01, -3.2049e+00,\n",
            "          -6.1075e+00, -1.8958e+00, -8.1183e-01, -6.1865e+00, -1.6623e+00,\n",
            "          -4.9395e-01, -2.1512e+00, -7.9836e-02, -2.7764e+00, -3.2246e-01,\n",
            "          -6.0352e+00,  4.8622e+00, -5.5071e-01,  2.7478e+00, -1.5817e+00,\n",
            "          -5.8117e+00, -5.1201e+00, -6.1407e+00, -2.8598e+00,  2.9918e+00],\n",
            "         [-1.5681e+00,  1.6218e+00, -5.9465e+00, -5.7510e+00, -5.9213e+00,\n",
            "          -6.0557e+00,  1.2598e+00, -5.8983e+00, -3.9245e+00, -6.0480e+00,\n",
            "          -1.4109e-01, -6.1493e+00, -5.9379e+00, -5.2509e+00, -3.0649e+00,\n",
            "          -9.4790e-01, -5.9701e+00, -6.0670e+00, -3.3614e+00, -5.8999e+00,\n",
            "          -5.8991e+00, -6.0143e+00, -5.9140e+00, -5.8030e+00, -5.9045e+00,\n",
            "          -6.0537e+00, -5.6237e+00, -5.9020e+00, -6.0264e+00, -5.9766e+00,\n",
            "          -6.1924e+00, -5.0963e+00, -6.0803e+00, -5.7973e+00, -5.9091e+00,\n",
            "          -5.8228e+00, -5.9653e+00, -1.7265e+00, -6.0742e+00, -1.5603e+00,\n",
            "          -5.9472e+00, -3.6126e+00, -2.2622e+00,  1.7809e+00, -2.5194e+00,\n",
            "          -6.0508e+00,  2.7291e-01,  1.1121e-01, -6.0476e+00, -4.0645e+00,\n",
            "          -1.1881e+00, -2.4442e-01, -1.1956e+00,  4.1368e-01, -2.9696e+00,\n",
            "          -5.9575e+00, -2.0145e+00,  6.0318e+00,  1.5558e+00, -3.3686e+00,\n",
            "          -6.0686e+00, -3.7201e+00, -6.0792e+00, -3.4137e+00, -3.2484e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.2349, -1.2617, -0.6818,  ...,  1.5756, -0.7613,  1.0367],\n",
            "         [-0.4018, -1.0637,  0.6228,  ...,  0.7127,  0.0740,  0.0295],\n",
            "         [ 1.0294,  0.4214,  0.1975,  ...,  1.3097,  1.0970,  0.1233]],\n",
            "\n",
            "        [[-0.6950,  0.7839, -0.5330,  ..., -0.4221,  0.7786, -0.2467],\n",
            "         [-0.4106,  1.0604,  0.5566,  ..., -0.3839, -2.8654, -1.9471],\n",
            "         [-0.4047, -0.2376, -0.5456,  ...,  2.3607, -1.6855,  2.3033]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 0.0539, -0.7174, -0.1289,  ...,  0.3602,  0.0286, -0.4184],\n",
            "         [ 0.9276, -0.9465, -0.9595,  ..., -0.6820,  0.0725, -0.5584],\n",
            "         [ 0.0828, -0.0212, -0.8052,  ..., -0.0022,  0.1533, -0.0072]],\n",
            "\n",
            "        [[ 0.0556, -0.0080,  0.0920,  ..., -0.0517, -0.0087, -0.4277],\n",
            "         [ 0.5947,  0.2889,  0.4453,  ..., -0.0183, -0.0562,  0.0651],\n",
            "         [ 0.7595, -0.4218,  0.4597,  ..., -0.0084, -0.6369,  0.3282]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-3.6260e-01,  1.2733e+00, -3.6884e+00, -3.6821e+00, -3.7315e+00,\n",
            "          -3.8206e+00,  1.7129e-01, -3.8703e+00, -2.2260e+00, -3.4960e+00,\n",
            "          -1.8006e+00, -3.8534e+00, -3.4818e+00, -2.4544e+00, -1.5464e+00,\n",
            "          -1.2473e+00, -3.6910e+00, -3.7682e+00, -1.4559e+00, -3.5608e+00,\n",
            "          -3.7750e+00, -3.3719e+00, -3.6955e+00, -3.7259e+00, -3.8020e+00,\n",
            "          -3.7138e+00, -3.5472e+00, -3.3430e+00, -3.6526e+00, -3.6752e+00,\n",
            "          -4.0070e+00, -1.5115e+00, -3.7483e+00, -3.7918e+00, -3.4894e+00,\n",
            "          -3.7445e+00, -3.4709e+00, -1.1771e+00, -3.6706e+00, -2.7103e+00,\n",
            "          -3.7486e+00,  4.9290e-01, -5.4702e-01,  2.3282e+00, -3.5705e+00,\n",
            "          -3.7730e+00, -3.2220e+00, -2.4519e+00, -3.6344e+00, -3.4781e+00,\n",
            "           5.3388e-01, -1.2305e+00, -2.2286e+00,  9.4860e-02,  5.0685e+00,\n",
            "          -3.8538e+00, -8.1928e-01, -2.6301e+00,  8.1249e+00,  1.0075e+00,\n",
            "          -3.4179e+00, -2.8278e+00, -3.7547e+00, -2.9378e+00,  9.9166e-01],\n",
            "         [-2.2605e+00,  3.2440e+00, -9.3657e+00, -9.8229e+00, -9.5158e+00,\n",
            "          -9.2633e+00, -1.8860e+00, -9.9175e+00, -5.0024e+00, -9.4986e+00,\n",
            "          -1.2598e+00, -9.9656e+00, -9.9282e+00, -4.0916e+00, -6.3422e+00,\n",
            "          -4.5992e+00, -9.8446e+00, -9.4315e+00, -4.0553e+00, -9.6687e+00,\n",
            "          -9.5745e+00, -9.5387e+00, -9.5289e+00, -9.7555e+00, -1.0126e+01,\n",
            "          -9.7459e+00, -9.4802e+00, -9.7415e+00, -9.8567e+00, -9.5347e+00,\n",
            "          -1.0124e+01, -3.8420e+00, -9.8274e+00, -9.8384e+00, -9.9644e+00,\n",
            "          -9.5182e+00, -9.5667e+00, -3.7415e+00, -1.0060e+01,  8.5647e-01,\n",
            "          -9.6069e+00, -4.5067e+00, -3.7048e+00,  1.4425e+01, -1.8214e+00,\n",
            "          -9.8199e+00, -6.8196e+00, -1.2248e+00, -9.5136e+00, -7.1389e+00,\n",
            "          -2.9726e+00, -6.7124e+00, -1.6541e+00, -3.9481e-01, -1.8761e+00,\n",
            "          -1.0118e+01,  6.6700e-01, -2.3162e+00,  1.4510e+00, -5.0658e+00,\n",
            "          -9.8123e+00, -9.4431e+00, -9.7052e+00, -8.5245e+00, -3.7744e+00],\n",
            "         [-1.8139e+00,  1.2731e+00, -7.0781e+00, -7.2634e+00, -7.0682e+00,\n",
            "          -6.9585e+00, -1.8188e+00, -7.2550e+00, -3.8059e+00, -7.3006e+00,\n",
            "          -1.4862e+00, -7.2096e+00, -7.3735e+00, -4.6883e+00, -4.7198e+00,\n",
            "          -3.4150e+00, -7.1650e+00, -7.2898e+00, -4.4283e+00, -7.2820e+00,\n",
            "          -7.2927e+00, -7.3934e+00, -7.1788e+00, -7.4867e+00, -7.3715e+00,\n",
            "          -7.0082e+00, -7.3288e+00, -7.3033e+00, -7.1241e+00, -7.0685e+00,\n",
            "          -7.3810e+00, -5.8306e+00, -7.2904e+00, -7.2634e+00, -7.2682e+00,\n",
            "          -7.0634e+00, -7.4282e+00, -3.0729e+00, -7.2702e+00,  4.4001e+00,\n",
            "          -7.1454e+00, -3.9650e+00,  7.8852e-02,  2.8101e+00,  3.6500e+00,\n",
            "          -7.3376e+00, -3.5736e+00,  2.9467e-01, -7.2004e+00, -7.9117e-01,\n",
            "          -3.3610e+00, -3.8529e+00,  2.6461e+00, -6.0787e-01, -1.1873e+00,\n",
            "          -7.1815e+00,  3.0477e+00, -7.7744e-01, -1.8319e+00, -3.6185e+00,\n",
            "          -7.2157e+00, -5.0015e+00, -7.3830e+00, -5.0703e+00, -3.8429e+00]],\n",
            "\n",
            "        [[-1.8414e+00, -1.5146e+00, -8.0819e-01, -8.0491e-01, -9.3738e-01,\n",
            "          -1.0686e+00, -6.4642e-02, -1.0693e+00, -5.6935e-01, -8.3459e-01,\n",
            "          -5.0602e-01, -1.1659e+00, -9.0271e-01,  2.6743e-01,  1.0605e+00,\n",
            "           5.1267e-01, -8.6021e-01, -9.8333e-01,  9.6467e-01, -1.2413e+00,\n",
            "          -1.1783e+00, -8.4116e-01, -1.1159e+00, -1.2550e+00, -1.0418e+00,\n",
            "          -1.1399e+00, -9.1442e-01, -9.2555e-01, -1.0588e+00, -1.3565e+00,\n",
            "          -1.1433e+00,  4.4812e-01, -1.1921e+00, -9.4537e-01, -1.0071e+00,\n",
            "          -1.2733e+00, -7.4588e-01, -6.9345e-01, -1.0473e+00, -2.3316e+00,\n",
            "          -8.8053e-01,  1.1724e+00, -8.7124e-02, -1.6068e+00, -1.0816e+00,\n",
            "          -8.4522e-01,  2.1272e-01,  1.8857e-01, -1.1085e+00,  4.4508e-01,\n",
            "          -3.9400e-02,  1.0246e+00, -1.2634e+00,  4.2867e+00,  2.9611e+00,\n",
            "          -1.2468e+00, -5.8816e-01, -4.7163e-01,  1.9054e+00,  1.2804e+01,\n",
            "          -9.1585e-01,  1.2194e+00, -1.2219e+00,  2.5480e+00, -1.5519e+00],\n",
            "         [-2.0095e+00, -1.2070e+00,  3.6407e+00,  3.8581e+00,  3.8205e+00,\n",
            "           3.7589e+00, -6.6816e-01,  3.7765e+00,  1.7995e+00,  4.0428e+00,\n",
            "          -3.0418e-01,  3.5770e+00,  3.8859e+00,  3.9591e+00,  2.1847e+00,\n",
            "           2.1637e+00,  3.7500e+00,  3.8362e+00,  8.0425e-01,  3.6243e+00,\n",
            "           3.6743e+00,  4.0821e+00,  3.9542e+00,  3.4645e+00,  3.7265e+00,\n",
            "           3.6720e+00,  3.8108e+00,  4.0008e+00,  3.9170e+00,  3.7844e+00,\n",
            "           3.8390e+00,  3.5794e+00,  3.4523e+00,  3.7773e+00,  3.8038e+00,\n",
            "           3.6232e+00,  4.0337e+00, -1.3794e-02,  3.6960e+00, -2.0869e+00,\n",
            "           3.6578e+00,  1.0445e+01,  1.3952e+00, -9.6636e-01, -1.8237e-01,\n",
            "           3.6802e+00,  2.1620e+00, -7.5893e-01,  3.8208e+00, -6.6477e-01,\n",
            "           3.8589e+00,  4.7052e-01,  1.1741e-02, -3.1548e-01,  4.9252e+00,\n",
            "           3.7964e+00, -1.5794e+00, -2.8542e+00,  2.0354e+00,  1.3597e+01,\n",
            "           4.0871e+00,  5.3135e+00,  3.8153e+00,  2.8872e+00,  7.2510e-01],\n",
            "         [-9.6739e-01,  2.1925e+00, -5.7579e+00, -5.7898e+00, -5.7184e+00,\n",
            "          -5.7406e+00,  1.7849e+00, -5.9993e+00, -3.2426e+00, -5.7255e+00,\n",
            "           2.6461e-02, -6.0567e+00, -6.0259e+00, -4.2920e+00, -4.1377e+00,\n",
            "          -4.0715e+00, -6.3001e+00, -6.0581e+00, -4.0316e+00, -5.8258e+00,\n",
            "          -6.1006e+00, -5.7031e+00, -5.8922e+00, -5.6615e+00, -5.9319e+00,\n",
            "          -6.1306e+00, -5.8626e+00, -6.0126e+00, -5.4067e+00, -5.9955e+00,\n",
            "          -6.1079e+00, -4.2325e+00, -6.2110e+00, -6.1215e+00, -5.8558e+00,\n",
            "          -5.7960e+00, -5.6296e+00, -2.6985e+00, -6.4542e+00, -2.4327e+00,\n",
            "          -5.9279e+00, -9.7305e-01, -2.6914e+00,  9.2137e+00, -3.0443e+00,\n",
            "          -6.1334e+00, -2.6880e+00, -1.7475e+00, -5.9324e+00, -5.7243e+00,\n",
            "          -9.3613e-01, -2.6984e+00, -8.9950e-01, -2.6564e-01, -2.8389e+00,\n",
            "          -5.6793e+00, -1.6274e+00, -7.0780e-01,  2.0817e+00,  5.1160e-01,\n",
            "          -5.7187e+00, -5.2983e+00, -5.5826e+00, -4.2736e+00, -8.9679e-01]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.8287, -0.9525,  0.2895,  ...,  1.9906, -1.0744, -0.3821],\n",
            "         [-0.5625,  1.4101, -0.7221,  ...,  1.7537, -0.5066, -0.9798],\n",
            "         [ 1.0504,  1.4501, -0.5416,  ...,  1.3903,  1.5172,  1.5046]],\n",
            "\n",
            "        [[ 1.0292,  0.4191,  0.1998,  ...,  1.3062,  1.0967,  0.1229],\n",
            "         [ 1.0292,  0.4191,  0.1998,  ...,  1.3062,  1.0967,  0.1229],\n",
            "         [ 1.7529, -1.4370, -0.1072,  ...,  0.6197,  0.5563, -0.5648]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-1.4201e-04, -3.6468e-05, -4.7948e-04,  ..., -1.1302e-02,\n",
            "          -7.7636e-03,  5.9210e-05],\n",
            "         [-2.1788e-03, -1.2135e-06,  4.4447e-05,  ..., -1.5838e-04,\n",
            "          -1.6573e-02,  6.4408e-01],\n",
            "         [-6.8767e-01, -1.2020e-01,  6.6111e-01,  ..., -1.8759e-02,\n",
            "          -7.6062e-01, -2.7232e-01]],\n",
            "\n",
            "        [[ 5.2220e-03, -3.5916e-02, -7.3497e-01,  ...,  2.1084e-04,\n",
            "          -1.5873e-03,  8.2963e-05],\n",
            "         [ 3.1007e-04, -2.0513e-06, -9.1649e-02,  ...,  9.9033e-05,\n",
            "          -8.0260e-04,  1.3983e-02],\n",
            "         [ 1.3562e-02, -5.3184e-03, -8.2795e-02,  ...,  5.7998e-01,\n",
            "           7.1150e-06, -7.6068e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.8731,  3.4990, -3.7061, -3.8473, -3.5845, -3.7374,  0.0906,\n",
            "          -3.5960, -0.8118, -3.8560, -0.2115, -3.6358, -3.9177, -1.9663,\n",
            "          -1.6332,  0.4062, -4.1418, -3.7386, -0.9326, -3.5941, -3.7963,\n",
            "          -3.9793, -3.7476, -3.9192, -3.7688, -3.9550, -3.9998, -3.9482,\n",
            "          -3.7228, -3.9140, -3.8061, -2.0045, -3.9158, -3.6863, -3.8926,\n",
            "          -3.7372, -3.8299, -0.5354, -3.9072, -0.5795, -3.8414, -2.4358,\n",
            "          -0.0575,  0.2258, -0.7172, -3.8931,  3.1143,  1.0804, -3.8534,\n",
            "          -1.0913, -0.6852, -1.9764, -0.8664, -0.2622, -1.7288, -3.6833,\n",
            "          -0.8188,  0.4410, -2.7915, -2.9631, -3.6018, -2.8171, -3.7929,\n",
            "          -1.3485, -1.2477],\n",
            "         [-1.4406, -0.7969, -1.7821, -1.8236, -1.8506, -1.8945, -0.5326,\n",
            "          -1.7957, -1.0012, -1.8521, -0.6143, -2.1283, -1.9356, -1.1688,\n",
            "          -1.0761,  6.5527, -1.6333, -1.8622, -0.5847, -1.8653, -1.8063,\n",
            "          -1.8194, -1.7625, -1.9268, -1.7733, -1.9422, -1.7176, -1.8473,\n",
            "          -1.8319, -1.9303, -1.9291, -1.6482, -1.7183, -1.7983, -1.8266,\n",
            "          -1.7939, -1.8003, -1.0785, -1.9928,  0.6943, -2.0119, -0.7643,\n",
            "          -0.9517, -1.8968,  1.8841, -1.9062,  3.7236, -0.0677, -1.8857,\n",
            "          -1.0279, -0.4243,  0.5123, -0.8076, -0.6686,  1.1059, -1.7839,\n",
            "          -0.9180,  2.2944, -1.0706, -0.7144, -1.8637,  3.2172, -1.8877,\n",
            "          -1.2652, -0.4400],\n",
            "         [-0.8586, -2.4032, -4.1957, -3.9032, -4.0454, -4.4713,  0.3464,\n",
            "          -4.6445,  0.3785, -3.8146,  0.0639, -4.5649, -3.8062, -1.7150,\n",
            "          -0.2940, -0.8897, -4.3658, -4.4949, -2.0753, -3.9961, -4.2620,\n",
            "          -3.8075, -3.9764, -4.1933, -4.1940, -4.4287, -3.8764, -4.3858,\n",
            "          -4.2867, -4.3097, -4.3824, -1.3681, -4.1920, -3.9703, -4.1297,\n",
            "          -4.5244, -4.0501, -0.6785, -4.5044, -2.7301, -4.6486, -2.5354,\n",
            "          -1.0909,  3.2687, -3.3262, -4.1961,  4.1426, 10.9134, -4.4169,\n",
            "          -6.7020,  4.9391, -1.8630, -2.8616, -3.0688,  0.2881, -4.1557,\n",
            "          -1.4625, -1.0981, -0.4919, -2.4073, -3.7315, -2.4548, -4.1514,\n",
            "          -2.0138, -0.5810]],\n",
            "\n",
            "        [[-1.0466,  2.7467, -6.6493, -6.7244, -6.7360, -6.7501, -0.5769,\n",
            "          -6.7137, -2.5798, -6.6178, -1.4267, -6.6557, -6.7052, -5.2042,\n",
            "          -4.1895, -3.9816, -6.8081, -6.8377, -3.9929, -6.6104, -6.7725,\n",
            "          -6.6257, -6.7452, -6.7637, -6.6392, -6.5033, -6.9112, -6.7335,\n",
            "          -6.5774, -6.6655, -6.8073, -5.7918, -6.6032, -6.8029, -6.7302,\n",
            "          -6.5539, -6.6366, -3.0343, -6.6710,  3.5506, -6.7060, -4.4360,\n",
            "           2.5150,  0.8824,  1.2282, -6.7057, -3.4031, -0.1051, -6.7584,\n",
            "          -0.4141, -3.2509, -3.9799,  2.5785, -1.4194, -0.8409, -6.8055,\n",
            "           2.9041, -1.1772, -1.8716, -3.7470, -6.6305, -5.1628, -6.7771,\n",
            "          -4.3577, -3.2835],\n",
            "         [-1.0123,  1.4303, -4.7497, -4.7887, -4.6983, -4.6618,  0.4180,\n",
            "          -4.7284, -1.6146, -4.6983, -0.8490, -4.7555, -4.8562, -3.6978,\n",
            "          -3.1218, -3.2515, -4.7053, -4.8556, -2.8763, -4.7881, -5.0070,\n",
            "          -4.6402, -4.8411, -4.7725, -4.6742, -4.7396, -4.8947, -4.8566,\n",
            "          -4.6589, -4.6482, -4.8591, -4.1399, -4.7557, -4.8427, -4.7179,\n",
            "          -4.7978, -4.7204, -2.5509, -4.8575,  1.6663, -4.8651, -2.9017,\n",
            "           1.8685,  0.4741,  0.6026, -4.8724, -2.6954,  0.0496, -4.6985,\n",
            "           0.6961, -1.9339, -2.9520,  1.3142, -0.9373, -0.2347, -4.7434,\n",
            "           2.7343, -1.0388, -2.0746, -2.2000, -4.6378, -3.4242, -4.9430,\n",
            "          -2.0807, -2.5735],\n",
            "         [-1.5092,  9.2828, -8.9049, -8.8344, -8.9343, -8.8204,  3.0382,\n",
            "          -8.6460, -0.3732, -8.9114, -0.0139, -8.8291, -8.8417, -6.9852,\n",
            "          -4.9996, -7.2236, -8.8863, -8.7840, -4.1936, -8.7842, -9.2726,\n",
            "          -8.7635, -8.9657, -8.7046, -8.5670, -9.1372, -8.8855, -8.7266,\n",
            "          -8.7924, -9.1324, -8.9274, -7.8050, -9.1906, -8.5729, -8.6933,\n",
            "          -8.9094, -8.9800, -4.3666, -8.8838, -0.7120, -8.8858, -4.8865,\n",
            "          -0.2438,  1.6661, -3.8146, -9.2783, -6.1809, -0.6266, -9.0556,\n",
            "           0.0646, -4.3140, -5.8489, -0.7952,  0.0838, -0.7244, -9.1862,\n",
            "           2.2784, -2.3209, -0.6090, -3.5707, -8.7248, -8.1285, -9.1307,\n",
            "          -0.5413, -3.8316]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 1.0507,  1.4504, -0.5427,  ...,  1.3905,  1.5167,  1.5062],\n",
            "         [ 0.1871,  1.7393,  1.5925,  ..., -1.5009,  1.5669,  0.0631],\n",
            "         [ 0.8322, -0.9476,  0.2865,  ...,  1.9929, -1.0688, -0.3803]],\n",
            "\n",
            "        [[ 1.0507,  1.4504, -0.5427,  ...,  1.3905,  1.5167,  1.5062],\n",
            "         [ 0.1871,  1.7393,  1.5925,  ..., -1.5009,  1.5669,  0.0631],\n",
            "         [ 0.8322, -0.9476,  0.2865,  ...,  1.9929, -1.0688, -0.3803]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-7.3611e-01, -3.7949e-01,  6.1955e-01,  ..., -1.8740e-02,\n",
            "          -6.6784e-01,  7.0206e-01],\n",
            "         [-6.6473e-06, -6.2600e-06,  6.7405e-01,  ..., -2.0945e-05,\n",
            "          -3.3822e-01,  1.7473e-03],\n",
            "         [-1.8543e-03, -9.0599e-03,  7.1134e-01,  ..., -1.9006e-03,\n",
            "          -3.1936e-02,  4.8583e-02]],\n",
            "\n",
            "        [[-7.3611e-01, -3.7949e-01,  6.1955e-01,  ..., -1.8740e-02,\n",
            "          -6.6784e-01,  7.0206e-01],\n",
            "         [-6.6473e-06, -6.2600e-06,  6.7405e-01,  ..., -2.0945e-05,\n",
            "          -3.3822e-01,  1.7473e-03],\n",
            "         [-1.8543e-03, -9.0599e-03,  7.1134e-01,  ..., -1.9006e-03,\n",
            "          -3.1936e-02,  4.8583e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.6782, -1.7683, -5.9751, -5.8527, -5.5763, -6.1133, -0.0390,\n",
            "          -6.0732, -0.9516, -5.4769,  0.1180, -6.2072, -5.6965, -2.9251,\n",
            "          -1.2312, -3.8194, -6.1789, -6.2056, -3.4522, -5.6393, -6.0383,\n",
            "          -5.4932, -5.6406, -5.8089, -5.9901, -6.2400, -5.6447, -6.1079,\n",
            "          -6.0423, -6.0084, -6.1277, -2.3269, -6.0668, -5.5405, -5.9237,\n",
            "          -5.9624, -5.7489, -0.5000, -6.1362, -1.7578, -6.2666, -4.2800,\n",
            "          -2.2691,  4.8340, -3.5300, -5.8937,  3.6644, 11.5053, -6.0351,\n",
            "          -6.9550,  4.6156, -2.3340, -2.7248, -3.1567, -1.4427, -5.6162,\n",
            "          -1.3497, -0.4336, -0.3812, -4.9109, -5.5177, -4.9453, -5.7415,\n",
            "          -3.0685, -0.9308],\n",
            "         [-1.0039, -1.5965, -3.5796, -3.8543, -3.5515, -4.0190, -1.1736,\n",
            "          -3.7925, -4.5140, -3.7323, -0.6755, -3.8793, -4.0109, -3.4946,\n",
            "          -1.9028, -2.7992, -3.7557, -4.1716, -3.1124, -3.4451, -4.3373,\n",
            "          -3.6229, -3.8058, -3.4390, -3.9240, -3.7291, -3.6857, -3.6922,\n",
            "          -3.7351, -3.6733, -3.8203, -2.9630, -3.9003, -3.5802, -3.4627,\n",
            "          -3.7173, -3.6630, -1.3197, -3.6226, -1.3448, -3.5288, -1.5523,\n",
            "          -2.4900,  1.1817, -2.5595, -3.6308, -1.6094,  1.0677, -3.8690,\n",
            "          -2.8483,  0.7040, -0.4114, -0.0970,  0.2752,  0.9647, -3.8437,\n",
            "          -1.0657,  0.5604,  8.8329, -1.8252, -4.0074, -3.4517, -3.6566,\n",
            "          -2.1419, -0.0411],\n",
            "         [-1.0296,  0.7717, -4.1879, -4.4498, -4.1323, -4.3980, -0.7914,\n",
            "          -3.9890, -2.4614, -4.4059, -0.2582, -4.6060, -4.6461, -2.0945,\n",
            "          -1.6706,  0.1570, -4.6117, -4.5909, -1.9689, -4.0931, -4.4129,\n",
            "          -4.4994, -4.1358, -4.3396, -4.6237, -4.5024, -4.3484, -4.7713,\n",
            "          -4.2819, -4.3846, -4.6045, -2.1897, -4.3026, -3.9621, -4.3244,\n",
            "          -4.3850, -4.2201, -1.0260, -4.3340, -0.5650, -4.5087, -3.5486,\n",
            "          -0.6967,  0.7461, -1.7137, -4.4711,  2.5087,  5.6813, -4.5099,\n",
            "          -2.3255,  0.5217, -2.3906,  0.0540, -1.3781, -1.1003, -4.3150,\n",
            "          -1.0997,  0.4728, -0.5440, -4.0139, -4.4108, -3.4439, -4.2076,\n",
            "          -2.3435, -0.8717]],\n",
            "\n",
            "        [[-0.6782, -1.7683, -5.9751, -5.8527, -5.5763, -6.1133, -0.0390,\n",
            "          -6.0732, -0.9516, -5.4769,  0.1180, -6.2072, -5.6965, -2.9251,\n",
            "          -1.2312, -3.8194, -6.1789, -6.2056, -3.4522, -5.6393, -6.0383,\n",
            "          -5.4932, -5.6406, -5.8089, -5.9901, -6.2400, -5.6447, -6.1079,\n",
            "          -6.0423, -6.0084, -6.1277, -2.3269, -6.0668, -5.5405, -5.9237,\n",
            "          -5.9624, -5.7489, -0.5000, -6.1362, -1.7578, -6.2666, -4.2800,\n",
            "          -2.2691,  4.8340, -3.5300, -5.8937,  3.6644, 11.5053, -6.0351,\n",
            "          -6.9550,  4.6156, -2.3340, -2.7248, -3.1567, -1.4427, -5.6162,\n",
            "          -1.3497, -0.4336, -0.3812, -4.9109, -5.5177, -4.9453, -5.7415,\n",
            "          -3.0685, -0.9308],\n",
            "         [-1.0039, -1.5965, -3.5796, -3.8543, -3.5515, -4.0190, -1.1736,\n",
            "          -3.7925, -4.5140, -3.7323, -0.6755, -3.8793, -4.0109, -3.4946,\n",
            "          -1.9028, -2.7992, -3.7557, -4.1716, -3.1124, -3.4451, -4.3373,\n",
            "          -3.6229, -3.8058, -3.4390, -3.9240, -3.7291, -3.6857, -3.6922,\n",
            "          -3.7351, -3.6733, -3.8203, -2.9630, -3.9003, -3.5802, -3.4627,\n",
            "          -3.7173, -3.6630, -1.3197, -3.6226, -1.3448, -3.5288, -1.5523,\n",
            "          -2.4900,  1.1817, -2.5595, -3.6308, -1.6094,  1.0677, -3.8690,\n",
            "          -2.8483,  0.7040, -0.4114, -0.0970,  0.2752,  0.9647, -3.8437,\n",
            "          -1.0657,  0.5604,  8.8329, -1.8252, -4.0074, -3.4517, -3.6566,\n",
            "          -2.1419, -0.0411],\n",
            "         [-1.0296,  0.7717, -4.1879, -4.4498, -4.1323, -4.3980, -0.7914,\n",
            "          -3.9890, -2.4614, -4.4059, -0.2582, -4.6060, -4.6461, -2.0945,\n",
            "          -1.6706,  0.1570, -4.6117, -4.5909, -1.9689, -4.0931, -4.4129,\n",
            "          -4.4994, -4.1358, -4.3396, -4.6237, -4.5024, -4.3484, -4.7713,\n",
            "          -4.2819, -4.3846, -4.6045, -2.1897, -4.3026, -3.9621, -4.3244,\n",
            "          -4.3850, -4.2201, -1.0260, -4.3340, -0.5650, -4.5087, -3.5486,\n",
            "          -0.6967,  0.7461, -1.7137, -4.4711,  2.5087,  5.6813, -4.5099,\n",
            "          -2.3255,  0.5217, -2.3906,  0.0540, -1.3781, -1.1003, -4.3150,\n",
            "          -1.0997,  0.4728, -0.5440, -4.0139, -4.4108, -3.4439, -4.2076,\n",
            "          -2.3435, -0.8717]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.5818, -0.8397,  0.3814,  ..., -0.8608, -0.2839,  0.1012],\n",
            "         [-0.6786,  0.9604,  0.7469,  ..., -0.4296,  0.6167,  1.9035],\n",
            "         [-0.4052, -1.0635,  0.6201,  ...,  0.7032,  0.0805,  0.0278]],\n",
            "\n",
            "        [[-0.5632,  1.4113, -0.7233,  ...,  1.7530, -0.5071, -0.9831],\n",
            "         [ 2.5847, -0.6852,  0.5444,  ...,  1.3427,  0.5272,  0.6380],\n",
            "         [ 1.0302,  0.4143,  0.2066,  ...,  1.3034,  1.0939,  0.1237]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 1.7379e-02, -1.7468e-03,  3.2202e-04,  ..., -4.2774e-02,\n",
            "           4.1207e-05,  2.1997e-03],\n",
            "         [ 6.9170e-01, -8.1209e-01,  2.5167e-01,  ...,  4.3997e-03,\n",
            "           4.5248e-02, -1.1111e-01],\n",
            "         [ 9.6899e-01, -9.7509e-01, -6.2772e-03,  ..., -4.4828e-01,\n",
            "           4.5525e-04, -6.8627e-02]],\n",
            "\n",
            "        [[-3.1017e-02, -2.4779e-05, -4.3529e-05,  ..., -6.1944e-05,\n",
            "          -1.1677e-03,  4.7440e-01],\n",
            "         [-3.6735e-02,  1.2028e-01,  1.5213e-01,  ..., -4.2234e-01,\n",
            "          -3.5201e-01, -6.4109e-01],\n",
            "         [-3.8685e-04, -8.2028e-04, -7.3308e-01,  ..., -2.0816e-03,\n",
            "          -7.7575e-01, -3.5556e-05]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 1.5337e+00, -2.5971e-01, -1.4363e+00, -1.4199e+00, -1.5269e+00,\n",
            "          -1.4918e+00, -3.2792e-01, -1.2926e+00, -8.0972e-01, -1.3853e+00,\n",
            "          -1.2677e-01, -1.4190e+00, -1.4144e+00,  3.4383e+00,  1.3515e+00,\n",
            "          -1.1863e+00, -1.4378e+00, -1.4966e+00,  1.4894e+00, -1.3122e+00,\n",
            "          -1.3756e+00, -1.2667e+00, -1.4139e+00, -1.5382e+00, -1.4897e+00,\n",
            "          -1.3971e+00, -1.4623e+00, -1.4714e+00, -1.4706e+00, -1.4849e+00,\n",
            "          -1.4245e+00,  1.8275e+00, -1.4876e+00, -1.5081e+00, -1.5556e+00,\n",
            "          -1.4749e+00, -1.4041e+00,  1.2774e+00, -1.3205e+00, -2.8307e-01,\n",
            "          -1.4623e+00, -9.6254e-01, -5.9281e-01, -6.7878e-02, -2.9823e-01,\n",
            "          -1.3368e+00, -6.2515e-01, -8.6427e-01, -1.6431e+00,  2.3307e-01,\n",
            "           5.9744e-01, -1.3611e-01, -2.0138e-01,  7.6194e-01, -2.4460e-01,\n",
            "          -1.4423e+00,  7.2570e-02, -5.0023e-01, -1.0284e+00, -1.5359e-01,\n",
            "          -1.4533e+00, -1.4288e+00, -1.3283e+00, -1.8053e-02, -1.1637e+00],\n",
            "         [-1.6313e+00, -1.5780e-01, -2.5288e+00, -2.6263e+00, -2.4499e+00,\n",
            "          -3.0195e+00,  7.7767e-02, -2.6539e+00, -3.4690e+00, -1.9804e+00,\n",
            "          -3.7157e+00, -3.0511e+00, -2.5699e+00,  4.8108e+00,  3.6616e-01,\n",
            "           2.4846e+00, -2.7494e+00, -2.8653e+00,  1.7573e+00, -2.6621e+00,\n",
            "          -3.0347e+00, -2.3532e+00, -2.7208e+00, -2.8201e+00, -2.7824e+00,\n",
            "          -3.3028e+00, -2.4588e+00, -2.5461e+00, -2.3730e+00, -2.5678e+00,\n",
            "          -2.7550e+00,  3.5185e+00, -2.8007e+00, -2.3578e+00, -2.6589e+00,\n",
            "          -2.8207e+00, -2.2951e+00, -1.5579e+00, -2.6078e+00, -3.2385e+00,\n",
            "          -2.5171e+00,  1.9736e+00,  1.8027e+00, -2.2869e-01, -9.9240e-01,\n",
            "          -2.9024e+00,  3.2070e-01, -1.8808e+00, -2.4331e+00, -1.7154e+00,\n",
            "           5.4300e+00, -7.8413e-01, -2.7829e+00,  6.0563e+00,  1.5686e+01,\n",
            "          -2.8303e+00, -2.8382e-01, -2.1758e+00,  4.5590e+00,  7.9302e+00,\n",
            "          -2.4198e+00,  2.2877e+00, -2.7576e+00, -2.1671e+00, -1.8813e+00],\n",
            "         [-3.0969e+00,  1.3066e+00, -2.3156e+00, -2.0088e+00, -1.9061e+00,\n",
            "          -2.1404e+00, -1.8950e+00, -2.0741e+00, -3.3345e+00, -1.6749e+00,\n",
            "          -7.3848e-01, -2.2790e+00, -2.1446e+00,  3.3943e+00, -2.4656e+00,\n",
            "          -6.3242e-01, -2.4709e+00, -2.1072e+00, -7.8226e-01, -2.1160e+00,\n",
            "          -2.2006e+00, -1.6382e+00, -1.8658e+00, -2.5672e+00, -2.5974e+00,\n",
            "          -2.3552e+00, -2.1430e+00, -2.1160e+00, -2.1372e+00, -2.0405e+00,\n",
            "          -2.2766e+00,  2.2894e+00, -2.4541e+00, -2.1217e+00, -2.4745e+00,\n",
            "          -1.9631e+00, -1.8183e+00, -2.8673e+00, -2.4334e+00, -7.2695e-01,\n",
            "          -1.8807e+00,  1.6837e+00, -1.0585e+00,  9.8928e+00, -3.4583e-01,\n",
            "          -2.1074e+00, -2.9438e+00, -2.1139e+00, -1.8296e+00, -5.2016e+00,\n",
            "           2.8135e+00, -4.4971e+00, -1.8003e+00,  1.2552e+00,  6.0555e+00,\n",
            "          -2.5429e+00, -1.2119e+00, -3.5365e+00,  2.0952e+00,  5.0206e+00,\n",
            "          -2.2173e+00, -6.2484e-01, -2.0881e+00, -5.0509e+00, -2.6728e+00]],\n",
            "\n",
            "        [[-1.3392e+00, -1.2897e+00, -2.6958e+00, -2.6808e+00, -2.6680e+00,\n",
            "          -2.6908e+00, -6.9375e-01, -2.7295e+00, -1.5757e+00, -2.7481e+00,\n",
            "          -7.2093e-01, -2.7679e+00, -2.8118e+00, -2.1303e+00, -1.7120e+00,\n",
            "           2.5258e+00, -2.6036e+00, -2.7202e+00, -1.4285e+00, -2.7340e+00,\n",
            "          -2.7622e+00, -2.7033e+00, -2.7137e+00, -2.7523e+00, -2.7327e+00,\n",
            "          -2.6578e+00, -2.5626e+00, -2.6128e+00, -2.7415e+00, -2.7157e+00,\n",
            "          -2.7878e+00, -2.6615e+00, -2.6571e+00, -2.6726e+00, -2.6923e+00,\n",
            "          -2.6379e+00, -2.6228e+00, -1.3512e+00, -2.8196e+00,  1.8237e+00,\n",
            "          -2.7706e+00, -8.4746e-01, -1.3307e+00, -1.9137e+00,  2.2127e+00,\n",
            "          -2.6477e+00,  3.2374e+00,  9.4176e-04, -2.7873e+00, -1.1284e+00,\n",
            "          -1.3994e+00,  1.3994e-01, -3.3104e-01, -5.3930e-01,  8.8158e-01,\n",
            "          -2.8140e+00, -6.7377e-01,  2.5227e+00, -6.6427e-01, -5.6287e-01,\n",
            "          -2.5842e+00,  3.3633e+00, -2.6994e+00, -1.4335e+00, -1.0598e+00],\n",
            "         [-2.7018e+00,  2.9386e-01, -7.8448e+00, -7.7810e+00, -7.8527e+00,\n",
            "          -8.0805e+00, -2.4030e-01, -8.1155e+00, -1.3214e+00, -7.9271e+00,\n",
            "          -9.6865e-01, -7.9833e+00, -7.9162e+00, -5.2481e+00, -4.1017e+00,\n",
            "          -2.7158e+00, -8.2252e+00, -8.0014e+00, -3.7425e+00, -7.8759e+00,\n",
            "          -8.0222e+00, -7.8849e+00, -8.0208e+00, -8.2063e+00, -8.1237e+00,\n",
            "          -8.1022e+00, -7.8935e+00, -7.9388e+00, -7.9130e+00, -8.2004e+00,\n",
            "          -7.8838e+00, -4.8810e+00, -8.0906e+00, -7.7335e+00, -7.9912e+00,\n",
            "          -8.1603e+00, -7.9297e+00, -3.4366e+00, -8.1257e+00, -1.8825e+00,\n",
            "          -8.0874e+00, -3.8084e+00, -2.3903e+00,  7.6214e+00, -2.8640e+00,\n",
            "          -8.0728e+00, -6.0941e-01,  3.9823e+00, -7.9593e+00, -5.2101e+00,\n",
            "          -1.4007e+00, -3.9241e+00, -3.6024e-01, -2.2711e+00, -1.8576e+00,\n",
            "          -8.0001e+00,  1.5636e+00, -1.7112e+00, -8.4151e-01, -3.3443e+00,\n",
            "          -7.6395e+00, -6.2752e+00, -8.0448e+00, -3.6984e+00, -1.9422e+00],\n",
            "         [-1.3504e+00,  5.4910e+00, -8.2990e+00, -8.2482e+00, -8.3044e+00,\n",
            "          -8.3911e+00, -4.9505e-01, -8.4260e+00, -3.7857e+00, -8.1709e+00,\n",
            "          -1.1467e+00, -8.2614e+00, -8.3656e+00, -5.5748e+00, -4.6188e+00,\n",
            "          -3.7490e+00, -8.2950e+00, -8.4287e+00, -3.9957e+00, -8.0960e+00,\n",
            "          -8.4770e+00, -8.2889e+00, -8.3094e+00, -8.4537e+00, -8.4165e+00,\n",
            "          -8.3672e+00, -8.4201e+00, -8.2944e+00, -8.3956e+00, -8.3608e+00,\n",
            "          -8.4108e+00, -6.2640e+00, -8.2280e+00, -8.3252e+00, -8.3940e+00,\n",
            "          -8.2022e+00, -8.3812e+00, -3.5477e+00, -8.2125e+00,  2.6857e+00,\n",
            "          -8.3066e+00, -4.9456e+00,  1.8137e+00,  1.8226e+00,  3.5887e-01,\n",
            "          -8.3219e+00, -1.5743e+00,  7.3725e-01, -8.3109e+00, -2.2491e+00,\n",
            "          -2.9680e+00, -4.1103e+00,  2.0958e+00, -5.2455e-01, -1.5751e+00,\n",
            "          -8.2075e+00,  1.1983e+00, -1.6519e+00, -1.3601e+00, -4.4210e+00,\n",
            "          -8.0700e+00, -5.4089e+00, -8.2860e+00, -4.7346e+00, -4.2883e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.5646,  1.4118, -0.7240,  ...,  1.7523, -0.5071, -0.9851],\n",
            "         [ 0.3889, -0.4833,  0.4467,  ..., -0.1899,  0.4495,  2.2303],\n",
            "         [ 1.0304,  0.4121,  0.2099,  ...,  1.3012,  1.0915,  0.1246]],\n",
            "\n",
            "        [[ 1.0304,  0.4121,  0.2099,  ...,  1.3012,  1.0915,  0.1246],\n",
            "         [-0.6913,  0.7674, -0.5428,  ..., -0.4049,  0.7698, -0.2500],\n",
            "         [-0.3910,  1.0760,  0.5556,  ..., -0.3744, -2.8556, -1.9577]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-2.8499e-02, -1.8241e-05, -5.3182e-05,  ..., -5.7557e-05,\n",
            "          -1.3719e-03,  4.4220e-01],\n",
            "         [-1.8844e-04, -1.9163e-01, -4.1626e-01,  ..., -2.0649e-02,\n",
            "           5.1932e-01,  7.1197e-01],\n",
            "         [-3.2994e-03, -1.5163e-05, -6.4549e-01,  ..., -6.8611e-05,\n",
            "           7.1667e-01,  1.9725e-03]],\n",
            "\n",
            "        [[ 3.4330e-03, -3.1043e-02, -7.4033e-01,  ...,  2.5763e-04,\n",
            "          -1.8602e-03,  7.1752e-05],\n",
            "         [-4.8196e-05, -2.9566e-08, -9.5324e-06,  ...,  2.3732e-03,\n",
            "          -1.8975e-07, -6.2793e-01],\n",
            "         [ 1.3745e-02,  9.9455e-02, -1.8601e-01,  ...,  5.2362e-02,\n",
            "          -2.7298e-01,  8.7193e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ -1.3876,  -1.3851,  -2.8087,  -2.7913,  -2.7862,  -2.8079,  -0.7153,\n",
            "           -2.8547,  -1.6648,  -2.8614,  -0.7559,  -2.8752,  -2.9183,  -2.1883,\n",
            "           -1.7656,   2.4096,  -2.7272,  -2.8444,  -1.4970,  -2.8293,  -2.8676,\n",
            "           -2.8141,  -2.8259,  -2.8671,  -2.8519,  -2.7796,  -2.6726,  -2.7288,\n",
            "           -2.8524,  -2.8316,  -2.9047,  -2.7117,  -2.7739,  -2.7821,  -2.8094,\n",
            "           -2.7539,  -2.7243,  -1.3981,  -2.9359,   1.9449,  -2.8880,  -0.8822,\n",
            "           -1.3754,  -1.9666,   2.0926,  -2.7672,   3.2461,  -0.0225,  -2.9029,\n",
            "           -1.2085,  -1.4326,   0.3366,  -0.3835,  -0.5400,   0.9977,  -2.9377,\n",
            "           -0.6792,   2.5361,  -0.6727,  -0.6223,  -2.6823,   3.6565,  -2.8032,\n",
            "           -1.5417,  -1.0707],\n",
            "         [ -2.7123,   0.5536,  -9.8338,  -9.8943,  -9.9419,  -9.8650,  -0.7497,\n",
            "           -9.8065,  -1.8126,  -9.9582,  -1.3890,  -9.7115,  -9.8839,  -5.8240,\n",
            "           -6.3627,  -5.9124, -10.1344,  -9.9189,  -4.7745, -10.2047,  -9.8957,\n",
            "          -10.2477,  -9.9736, -10.1693, -10.0378,  -9.8000,  -9.9176, -10.0667,\n",
            "           -9.8778,  -9.8768, -10.0104,  -6.5736, -10.0936, -10.1249,  -9.9993,\n",
            "           -9.8632, -10.0138,  -4.0614, -10.1607,   0.9454, -10.0557,  -5.4199,\n",
            "           -4.1196,   9.9934,  -1.6742, -10.0265,  -5.0658,   2.2613,  -9.8848,\n",
            "           -3.3923,  -4.3187,  -6.3452,   1.5141,  -1.3246,  -3.6090,  -9.8363,\n",
            "            3.6932,  -1.7777,  -1.4577,  -4.4485,  -9.9512,  -9.1581, -10.0075,\n",
            "           -5.4667,  -3.9545],\n",
            "         [ -1.9489,   0.8741,  -7.8114,  -7.7450,  -7.7103,  -7.8891,  -0.9976,\n",
            "           -7.7189,  -3.0794,  -7.7969,  -2.3524,  -7.7659,  -7.7854,  -5.5293,\n",
            "           -5.2612,  -4.8919,  -7.7394,  -8.0767,  -4.5625,  -7.8861,  -8.0501,\n",
            "           -7.8601,  -7.8180,  -7.9465,  -7.8839,  -7.5830,  -8.0464,  -8.0579,\n",
            "           -7.6880,  -7.7498,  -8.0196,  -6.3865,  -7.8183,  -7.8230,  -7.7879,\n",
            "           -7.6205,  -8.0593,  -3.5437,  -7.9837,   4.8507,  -7.9450,  -3.8777,\n",
            "            0.7135,   2.3698,   1.7278,  -7.7965,  -3.2716,   0.3111,  -7.7996,\n",
            "           -0.6817,  -4.2427,  -4.3373,   2.4908,  -1.1662,  -1.2231,  -7.7362,\n",
            "            4.2926,  -1.3031,  -1.8279,  -2.9799,  -7.6153,  -5.4991,  -7.9160,\n",
            "           -4.3052,  -3.4791]],\n",
            "\n",
            "        [[ -1.4334,   2.9818,  -7.2064,  -7.3533,  -7.3659,  -7.2959,  -0.5909,\n",
            "           -7.3659,  -2.7999,  -7.2491,  -1.4073,  -7.2979,  -7.3313,  -5.3777,\n",
            "           -4.5719,  -4.3388,  -7.3721,  -7.3803,  -4.2080,  -7.1890,  -7.4128,\n",
            "           -7.2330,  -7.3763,  -7.3478,  -7.2639,  -7.1635,  -7.4554,  -7.3185,\n",
            "           -7.1823,  -7.2647,  -7.4014,  -5.8906,  -7.2391,  -7.4273,  -7.3997,\n",
            "           -7.1609,  -7.2637,  -3.4012,  -7.2802,   3.8357,  -7.2565,  -4.5357,\n",
            "            2.9505,   1.7814,   0.7947,  -7.3440,  -3.2401,  -0.1334,  -7.3577,\n",
            "           -1.2568,  -3.1554,  -4.3842,   2.0109,  -1.3559,  -1.0847,  -7.3901,\n",
            "            2.1381,  -1.4057,  -1.8619,  -3.8971,  -7.1810,  -5.5863,  -7.4102,\n",
            "           -5.0213,  -3.5249],\n",
            "         [ -0.9486,  -0.1819,  -2.4823,  -2.5555,  -2.6218,  -2.7177,   1.3096,\n",
            "           -2.6010,  -0.0440,  -2.7034,   0.5356,  -2.7079,  -2.6448,  -1.5034,\n",
            "           -0.6544,  -1.9531,  -2.5676,  -2.5063,  -0.5111,  -2.8887,  -2.8126,\n",
            "           -2.6537,  -2.8495,  -2.8390,  -2.5973,  -2.8775,  -2.8022,  -2.6249,\n",
            "           -2.9246,  -3.0575,  -2.7773,  -1.8902,  -2.6469,  -2.6633,  -2.6292,\n",
            "           -3.0281,  -2.5920,  -1.3401,  -2.7967,  -1.5635,  -2.6192,  -1.1549,\n",
            "            0.0145,  -1.0323,  -1.7397,  -2.6205,  -1.6359,   0.0107,  -2.8840,\n",
            "            0.4864,  -0.8689,  -1.3944,  -1.7833,   4.2292,   0.5763,  -2.8883,\n",
            "            0.6455,  -0.6329,  -0.1782,   4.1549,  -2.7395,  -0.7696,  -2.9462,\n",
            "            1.5890,  -1.4973],\n",
            "         [ -0.5918,   0.5532,  -0.7805,  -0.5157,  -0.7670,  -0.8027,  -0.3216,\n",
            "           -0.7297,   0.3955,  -0.5669,  -0.2081,  -0.6645,  -0.6536,  -0.2831,\n",
            "           -0.3233,  -1.5635,  -0.6965,  -0.8504,  -1.0144,  -0.8925,  -0.7294,\n",
            "           -0.6408,  -0.6511,  -0.8989,  -0.7827,  -0.8170,  -0.6281,  -0.6502,\n",
            "           -0.7078,  -0.7437,  -0.8362,  -0.5600,  -0.8620,  -0.7038,  -0.5252,\n",
            "           -0.7183,  -0.6483,  -0.8125,  -0.6919,  -1.3402,  -0.8774,   4.9725,\n",
            "            1.0172,  -0.6745,  -1.6016,  -0.9792,  -0.4090,  -0.2972,  -0.7907,\n",
            "           -0.0111,   1.2284,  -0.6629,   0.1069,  -0.6160,   0.9500,  -0.9025,\n",
            "            2.6838,  -2.3807,   0.4785,   2.8656,  -0.4093,   0.5825,  -0.8425,\n",
            "            1.4429,  -0.0578]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-1.5360,  0.2126, -0.2225,  ...,  1.6294, -0.1717, -0.9119],\n",
            "         [-0.9586, -0.6738,  0.3024,  ...,  1.2262, -1.6023,  1.7442],\n",
            "         [ 0.5818, -0.8344,  0.3792,  ..., -0.8584, -0.2843,  0.0982]],\n",
            "\n",
            "        [[ 0.1940, -0.5826,  0.5150,  ...,  1.0742,  1.2702, -1.0793],\n",
            "         [ 1.0304,  0.4094,  0.2128,  ...,  1.2991,  1.0877,  0.1253],\n",
            "         [-0.6911,  0.7628, -0.5444,  ..., -0.4021,  0.7697, -0.2502]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-2.4533e-04,  1.5005e-02, -5.6637e-04,  ...,  5.5830e-02,\n",
            "           6.7008e-01,  7.1533e-01],\n",
            "         [ 4.8325e-01,  3.1797e-06, -1.8606e-05,  ...,  5.5060e-03,\n",
            "          -4.2821e-01,  4.7191e-05],\n",
            "         [ 3.6358e-02, -1.9732e-07,  4.8867e-08,  ...,  2.7166e-01,\n",
            "          -6.5115e-04, -2.9005e-07]],\n",
            "\n",
            "        [[ 4.4749e-01, -5.8846e-01, -3.6817e-01,  ..., -6.0692e-01,\n",
            "          -3.3440e-01,  4.9393e-01],\n",
            "         [ 1.2247e-04, -4.9391e-07, -1.0787e-03,  ..., -2.2870e-05,\n",
            "          -4.9051e-01,  7.2650e-02],\n",
            "         [ 6.1494e-06, -3.3626e-08, -4.9425e-06,  ...,  2.9097e-03,\n",
            "          -6.3469e-04, -2.1715e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 1.8201e+00, -4.7198e-01, -2.6145e+00, -2.5163e+00, -2.5116e+00,\n",
            "          -2.6582e+00,  2.7042e+00, -2.6669e+00,  1.3653e+01, -2.6534e+00,\n",
            "          -1.2978e-01, -2.4139e+00, -2.8375e+00, -3.0310e+00, -2.3165e+00,\n",
            "          -3.0572e+00, -2.4915e+00, -2.5093e+00, -1.5014e+00, -2.7706e+00,\n",
            "          -2.5806e+00, -2.7446e+00, -2.4426e+00, -2.5486e+00, -2.5289e+00,\n",
            "          -2.4145e+00, -2.4675e+00, -2.4014e+00, -2.6418e+00, -2.5045e+00,\n",
            "          -2.5656e+00, -3.3333e+00, -2.5560e+00, -2.4404e+00, -2.6760e+00,\n",
            "          -2.4273e+00, -2.6752e+00, -5.2817e-01, -2.6166e+00, -1.2156e+00,\n",
            "          -2.4971e+00, -2.3201e+00,  9.3390e-02, -2.3408e-01, -1.9096e+00,\n",
            "          -2.2834e+00, -1.0781e+00, -3.5044e-01, -2.7400e+00, -1.7736e+00,\n",
            "          -1.6622e+00, -2.6090e+00, -1.3357e+00, -2.3931e+00, -2.7081e+00,\n",
            "          -2.4568e+00, -5.8361e-01, -1.2037e+00, -3.1339e+00, -1.7621e+00,\n",
            "          -2.5009e+00, -3.9726e+00, -2.6580e+00,  1.0229e+00, -1.9537e+00],\n",
            "         [ 1.3532e+01,  3.3048e-01, -6.0884e+00, -5.9135e+00, -6.4095e+00,\n",
            "          -6.1214e+00,  1.0908e+00, -5.9758e+00,  5.4202e+00, -6.3084e+00,\n",
            "           1.6524e+00, -6.2413e+00, -6.4397e+00, -4.5037e+00, -4.6112e+00,\n",
            "          -6.0050e+00, -6.2871e+00, -6.1757e+00, -2.8088e+00, -6.0985e+00,\n",
            "          -6.2916e+00, -6.4461e+00, -6.2309e+00, -6.2403e+00, -6.4185e+00,\n",
            "          -6.1465e+00, -6.5563e+00, -6.2032e+00, -6.4669e+00, -6.1771e+00,\n",
            "          -6.4183e+00, -6.0774e+00, -6.1219e+00, -6.4809e+00, -6.4415e+00,\n",
            "          -5.9206e+00, -6.0552e+00,  1.9702e-01, -5.9998e+00, -3.6899e-01,\n",
            "          -6.2510e+00, -6.2449e+00, -1.5159e+00, -2.1190e+00, -2.3828e+00,\n",
            "          -6.3214e+00, -4.7985e+00, -2.1800e+00, -6.4453e+00, -6.4537e-01,\n",
            "          -2.7107e+00, -2.1643e+00, -4.1895e-01, -2.1967e+00, -6.6687e+00,\n",
            "          -6.5004e+00, -1.0944e+00, -3.9463e-01, -3.0815e+00, -6.1657e+00,\n",
            "          -6.4466e+00, -8.5248e+00, -6.2370e+00, -1.8157e+00, -2.4658e+00],\n",
            "         [ 6.2666e+00,  1.5575e-01, -2.9004e+00, -3.0276e+00, -3.1100e+00,\n",
            "          -3.0796e+00, -1.0165e-01, -3.0595e+00,  6.6218e-01, -2.8835e+00,\n",
            "           1.8755e-01, -3.2482e+00, -3.0526e+00,  3.0356e+00, -5.5692e-02,\n",
            "          -2.4777e+00, -3.0446e+00, -3.0355e+00,  7.8925e-01, -3.0611e+00,\n",
            "          -3.1115e+00, -2.9988e+00, -3.0849e+00, -3.3003e+00, -3.1856e+00,\n",
            "          -3.0397e+00, -3.0066e+00, -3.2208e+00, -3.1479e+00, -3.3201e+00,\n",
            "          -3.2784e+00,  8.1532e-01, -3.0192e+00, -3.1156e+00, -3.3069e+00,\n",
            "          -3.1204e+00, -2.8461e+00,  5.1961e-01, -2.9616e+00, -9.5986e-01,\n",
            "          -2.9493e+00, -2.4692e+00, -3.2091e-01, -1.0442e+00, -7.9667e-01,\n",
            "          -3.2337e+00, -1.7351e+00, -1.8014e+00, -3.1370e+00,  1.2376e-01,\n",
            "          -3.4453e-01, -7.8418e-01, -6.8654e-01, -4.2409e-01, -1.2168e+00,\n",
            "          -3.1972e+00, -3.6154e-01, -1.2800e+00, -1.6767e+00, -1.6796e+00,\n",
            "          -3.0023e+00, -3.6411e+00, -3.0606e+00, -6.1501e-01, -1.5130e+00]],\n",
            "\n",
            "        [[-2.4036e+00,  3.7985e+00, -1.2767e+01, -1.3122e+01, -1.2819e+01,\n",
            "          -1.2816e+01, -1.8990e+00, -1.3113e+01, -5.5089e+00, -1.2768e+01,\n",
            "          -3.0095e+00, -1.2770e+01, -1.2967e+01, -5.9478e+00, -8.2606e+00,\n",
            "          -5.6226e+00, -1.3073e+01, -1.2870e+01, -6.6103e+00, -1.2958e+01,\n",
            "          -1.2815e+01, -1.2960e+01, -1.3083e+01, -1.3178e+01, -1.3011e+01,\n",
            "          -1.2698e+01, -1.2988e+01, -1.3058e+01, -1.2833e+01, -1.2700e+01,\n",
            "          -1.3129e+01, -6.1799e+00, -1.3138e+01, -1.3047e+01, -1.2914e+01,\n",
            "          -1.3000e+01, -1.2763e+01, -5.1665e+00, -1.2827e+01,  1.5587e+00,\n",
            "          -1.2888e+01, -8.1500e+00, -3.2308e+00,  1.1758e+01,  1.9821e+00,\n",
            "          -1.2949e+01, -4.8699e+00, -7.3065e-01, -1.2954e+01, -3.3511e+00,\n",
            "          -5.3041e+00, -8.4019e+00,  2.6750e+00, -1.7325e-01, -3.2054e+00,\n",
            "          -1.2784e+01,  1.4719e+00, -1.6173e+00, -2.0162e+00, -6.0476e+00,\n",
            "          -1.2570e+01, -1.2439e+01, -1.2820e+01, -1.0764e+01, -3.7065e+00],\n",
            "         [-1.5015e+00,  7.8534e-01, -5.1942e+00, -5.2000e+00, -5.1126e+00,\n",
            "          -5.2478e+00, -1.2766e+00, -5.1603e+00, -3.2675e+00, -5.3231e+00,\n",
            "          -1.9237e+00, -5.2483e+00, -5.2358e+00, -3.4813e+00, -3.0736e+00,\n",
            "          -2.7132e+00, -5.2672e+00, -5.3557e+00, -3.0723e+00, -5.1893e+00,\n",
            "          -5.2739e+00, -5.1229e+00, -5.2183e+00, -5.2743e+00, -5.4140e+00,\n",
            "          -5.1027e+00, -5.3830e+00, -5.2184e+00, -5.2429e+00, -5.0577e+00,\n",
            "          -5.1985e+00, -4.3248e+00, -5.0831e+00, -5.2392e+00, -5.2877e+00,\n",
            "          -5.3637e+00, -5.2308e+00, -1.7875e+00, -5.1152e+00,  3.3525e+00,\n",
            "          -5.1957e+00, -3.8417e+00,  1.5757e+00,  1.8742e+00,  4.3326e+00,\n",
            "          -5.1673e+00, -2.2766e+00,  3.6910e-01, -5.2206e+00, -1.6384e-01,\n",
            "          -2.4636e+00, -3.2844e+00,  2.1664e+00, -1.3588e-01, -6.9996e-01,\n",
            "          -5.0508e+00,  1.6465e+00, -8.1376e-01, -2.2648e+00, -2.2648e+00,\n",
            "          -4.9450e+00, -3.0546e+00, -5.2047e+00, -4.3326e+00, -2.5943e+00],\n",
            "         [-1.6486e+00, -5.4527e-01, -3.8246e+00, -3.8221e+00, -3.8160e+00,\n",
            "          -3.9245e+00,  9.2685e-01, -3.9419e+00, -1.7285e+00, -3.9004e+00,\n",
            "          -2.7085e-02, -4.1011e+00, -3.9893e+00, -2.3105e+00, -9.8759e-01,\n",
            "          -2.3876e+00, -3.8129e+00, -3.8212e+00, -7.5352e-01, -4.2387e+00,\n",
            "          -4.0459e+00, -3.9342e+00, -4.2199e+00, -4.0828e+00, -4.0279e+00,\n",
            "          -4.1453e+00, -4.0898e+00, -3.8932e+00, -4.2012e+00, -4.3615e+00,\n",
            "          -4.0619e+00, -2.5773e+00, -3.8791e+00, -3.9732e+00, -3.9419e+00,\n",
            "          -4.2802e+00, -3.8213e+00, -1.5221e+00, -4.0400e+00, -1.4292e+00,\n",
            "          -3.8337e+00, -2.0657e+00, -6.1341e-01,  5.2740e-02, -9.8298e-01,\n",
            "          -3.9027e+00, -2.1940e+00, -1.7774e-01, -4.2019e+00,  3.5002e-01,\n",
            "          -1.8657e+00, -1.3268e+00, -1.4999e+00,  6.4076e+00,  4.2679e-01,\n",
            "          -4.2534e+00,  4.6783e-01, -2.0042e-01,  2.9352e-03,  4.5826e+00,\n",
            "          -4.0372e+00, -1.5220e+00, -4.0998e+00,  8.1658e-01, -2.1077e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.4024, -1.0602,  0.6248,  ...,  0.7032,  0.0790,  0.0312],\n",
            "         [-0.3869, -0.2244, -0.5394,  ...,  2.3713, -1.6849,  2.2891],\n",
            "         [-0.3834,  1.0802,  0.5557,  ..., -0.3715, -2.8503, -1.9614]],\n",
            "\n",
            "        [[ 0.5817, -0.8321,  0.3782,  ..., -0.8569, -0.2842,  0.0968],\n",
            "         [ 0.5817, -0.8321,  0.3782,  ..., -0.8569, -0.2842,  0.0968],\n",
            "         [ 1.7000,  1.1321, -1.7572,  ...,  0.3959,  1.3320, -1.1936]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 4.1425e-01,  4.9920e-01, -3.7399e-01,  ..., -3.9927e-01,\n",
            "           1.3296e-03,  7.1386e-02],\n",
            "         [ 6.9800e-01, -5.7216e-01, -6.3134e-01,  ..., -5.4998e-01,\n",
            "          -7.5616e-01, -1.0143e-03],\n",
            "         [ 8.8431e-01, -2.4223e-02, -3.8049e-01,  ...,  3.1559e-02,\n",
            "          -3.7281e-01,  1.1415e-01]],\n",
            "\n",
            "        [[ 1.7973e-02, -1.2334e-03,  2.1838e-04,  ..., -4.6204e-02,\n",
            "           2.0997e-05,  1.9342e-03],\n",
            "         [ 3.0196e-01, -9.0668e-03,  9.7731e-05,  ..., -6.8735e-02,\n",
            "           6.3377e-06,  2.7995e-05],\n",
            "         [ 1.2338e-02, -5.7346e-02,  6.7666e-01,  ..., -4.5545e-05,\n",
            "           7.1772e-03,  1.0788e-02]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.2224, -1.7016, -4.4956, -4.2435, -4.2316, -4.2568, -0.2527,\n",
            "          -4.3285, -0.8209, -4.2389, -2.4822, -4.5694, -4.4067, -2.4202,\n",
            "          -2.7247, -2.9618, -4.3290, -4.3256, -1.5139, -4.5203, -4.4425,\n",
            "          -4.4193, -4.2269, -4.2010, -4.5355, -4.6395, -4.3747, -4.5828,\n",
            "          -4.2594, -4.3814, -4.5884, -1.8508, -4.3393, -4.3695, -4.3734,\n",
            "          -4.5318, -4.3977, -2.2309, -4.2919, -3.4272, -4.3608,  1.0030,\n",
            "          -3.0930,  7.1919, -3.9549, -4.4298, -3.9345, -0.9753, -4.3733,\n",
            "          -1.6180, -2.0849, -3.6708, -2.0195,  0.0946, -0.7803, -4.6067,\n",
            "           6.4905, -2.7733,  1.1087,  0.5207, -4.2728, -4.2649, -4.1296,\n",
            "          -2.0769, -0.9490],\n",
            "         [-0.2138,  1.2462, -5.4373, -5.4046, -5.5764, -5.2942,  1.7442,\n",
            "          -5.7160, -5.0188, -5.6571, -1.7966, -5.7538, -5.5328, -3.6399,\n",
            "          -1.9785, -2.6264, -5.5419, -5.5066, -1.3235, -5.6161, -5.7410,\n",
            "          -5.4376, -5.5095, -5.7625, -5.7650, -5.9067, -5.6662, -5.7865,\n",
            "          -5.3721, -5.7092, -5.4730, -3.2562, -5.7973, -5.8926, -5.3896,\n",
            "          -5.4460, -6.0369, -1.2802, -5.6313, -0.7335, -5.4986, -1.0105,\n",
            "          -1.8154,  2.1565, -1.9965, -5.8057, -1.2033, -0.9115, -5.6587,\n",
            "          -2.4604, -2.1126, -2.0784, -2.7240,  8.0132, -2.4532, -5.5271,\n",
            "          -0.3462,  0.7103,  2.0099, -0.5286, -5.7461, -3.0630, -5.6825,\n",
            "          -3.5064, -2.0242],\n",
            "         [-0.7658,  1.7853,  1.1167,  1.1872,  1.0757,  1.1711, -0.5615,\n",
            "           1.0237, -0.8280,  1.2333,  0.5712,  1.0458,  1.3890,  1.5268,\n",
            "           1.3585, -0.0968,  1.1591,  1.0425,  0.5390,  0.9594,  1.1781,\n",
            "           1.3155,  1.2035,  1.0581,  0.8937,  0.9599,  1.1146,  1.1027,\n",
            "           1.2748,  1.2881,  1.0291,  1.7991,  0.9720,  1.2019,  1.1910,\n",
            "           1.2133,  0.9376,  0.2994,  1.0506, -0.7350,  1.1807,  8.1279,\n",
            "           0.3463, -0.0944, -0.9157,  1.0045,  1.1109, -1.7665,  1.0146,\n",
            "          -0.6177,  1.1935,  1.0057, -0.7672,  1.1123,  0.3332,  1.0915,\n",
            "           0.7201, -2.2747,  1.9660,  1.5793,  1.0205,  2.4923,  1.0087,\n",
            "           0.6558, -0.2143]],\n",
            "\n",
            "        [[ 1.2023, -0.2800, -1.4553, -1.4584, -1.5712, -1.5262, -0.4160,\n",
            "          -1.3511, -0.8705, -1.3846, -0.1019, -1.4438, -1.4466,  3.4937,\n",
            "           1.8615, -1.2274, -1.4877, -1.5222,  1.5212, -1.3481, -1.4318,\n",
            "          -1.2981, -1.4277, -1.5611, -1.5212, -1.4725, -1.4728, -1.4783,\n",
            "          -1.4811, -1.5263, -1.4848,  2.3938, -1.5144, -1.5011, -1.5904,\n",
            "          -1.4928, -1.4111,  1.8692, -1.3362, -0.3843, -1.5221, -0.8883,\n",
            "          -0.6058, -0.1859, -0.3867, -1.3772, -0.5719, -0.9158, -1.6526,\n",
            "           0.1039,  0.6345, -0.2071, -0.3484,  0.7200, -0.3232, -1.4811,\n",
            "           0.0098, -0.5781, -0.9911, -0.1698, -1.4750, -1.5197, -1.3665,\n",
            "          -0.1022, -0.9972],\n",
            "         [ 1.6771, -0.3195, -1.6330, -1.7345, -1.8329, -1.7629, -0.4560,\n",
            "          -1.5410, -1.0468, -1.5790, -0.1012, -1.7447, -1.7500,  4.7347,\n",
            "           2.5528, -1.4791, -1.8031, -1.7732,  2.4889, -1.5949, -1.6275,\n",
            "          -1.4981, -1.6483, -1.8479, -1.7652, -1.7664, -1.6930, -1.7214,\n",
            "          -1.7342, -1.8045, -1.7330,  3.4335, -1.7755, -1.7283, -1.8090,\n",
            "          -1.7662, -1.6538,  2.3082, -1.5555, -0.6195, -1.8347, -0.9449,\n",
            "          -0.4875, -0.3584, -0.6109, -1.5900, -0.7258, -1.1603, -1.8726,\n",
            "          -0.0271,  1.5146, -0.1702, -0.6801,  1.2547,  0.0780, -1.7191,\n",
            "          -0.1867, -0.8450, -0.9215,  0.0809, -1.7085, -1.8628, -1.6589,\n",
            "          -0.0765, -1.3578],\n",
            "         [-2.0637, -1.6109, -7.3333, -6.9654, -7.5983, -7.4902, -1.1079,\n",
            "          -7.4745, -2.8153, -7.1617, -1.2820, -7.5077, -7.3886,  0.3696,\n",
            "          -3.0993, -2.9632, -7.4054, -7.4408, -2.3172, -7.1945, -7.4862,\n",
            "          -7.1619, -7.2297, -7.5502, -7.3982, -7.7166, -7.1925, -7.6378,\n",
            "          -7.1886, -7.5865, -7.4856, -0.9724, -7.3627, -7.4302, -7.4384,\n",
            "          -7.4840, -7.4620, -2.8821, -7.2473, -1.5364, -7.5346, -5.4418,\n",
            "          -1.0905,  2.1005, -1.6566, -7.4278, -1.6068,  9.5609, -7.5930,\n",
            "          -2.2383,  2.3088, -3.0418, -2.4637,  1.4006,  1.7112, -7.5268,\n",
            "           0.5556, -1.8772, -0.6124,  0.0826, -7.3022, -4.7994, -7.1874,\n",
            "          -3.9363, -2.9423]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 1.0304,  0.4059,  0.2173,  ...,  1.2960,  1.0794,  0.1257],\n",
            "         [-0.3844, -0.2224, -0.5385,  ...,  2.3728, -1.6841,  2.2865],\n",
            "         [-2.1733, -0.3220,  1.0319,  ...,  0.4303,  0.9941,  0.3281]],\n",
            "\n",
            "        [[-0.9598, -0.6735,  0.3034,  ...,  1.2259, -1.6029,  1.7447],\n",
            "         [ 0.5793, -0.8299,  0.3769,  ..., -0.8552, -0.2838,  0.0956],\n",
            "         [ 0.5793, -0.8299,  0.3769,  ..., -0.8552, -0.2838,  0.0956]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 2.6719e-03, -3.0858e-02, -7.4403e-01,  ...,  3.1633e-04,\n",
            "          -2.1245e-03,  6.6811e-05],\n",
            "         [ 1.2347e-03, -9.8032e-06, -1.6806e-03,  ...,  2.6755e-03,\n",
            "          -1.8639e-03, -1.3708e-01],\n",
            "         [ 4.2940e-01, -9.7340e-03, -8.2592e-01,  ...,  6.4943e-01,\n",
            "          -7.3179e-01, -4.5038e-01]],\n",
            "\n",
            "        [[-3.6608e-01,  1.1521e-03, -1.8329e-04,  ...,  5.7271e-02,\n",
            "           5.6741e-02,  6.8440e-03],\n",
            "         [-1.9392e-03, -4.1880e-08,  2.0021e-08,  ...,  2.4177e-01,\n",
            "          -9.4486e-04, -1.3954e-07],\n",
            "         [ 1.6509e-01, -1.0481e-03,  7.6140e-06,  ...,  9.8102e-02,\n",
            "          -1.4632e-06, -4.3871e-05]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.5663e+00,  3.6386e+00, -7.4860e+00, -7.6807e+00, -7.6897e+00,\n",
            "          -7.5662e+00, -7.8259e-01, -7.6545e+00, -3.1292e+00, -7.5317e+00,\n",
            "          -1.2506e+00, -7.5759e+00, -7.6097e+00, -5.4639e+00, -4.7636e+00,\n",
            "          -4.3966e+00, -7.6827e+00, -7.6334e+00, -4.3712e+00, -7.4333e+00,\n",
            "          -7.7215e+00, -7.5612e+00, -7.6688e+00, -7.6651e+00, -7.5822e+00,\n",
            "          -7.4307e+00, -7.7090e+00, -7.6294e+00, -7.4933e+00, -7.5228e+00,\n",
            "          -7.6613e+00, -5.9571e+00, -7.5397e+00, -7.6693e+00, -7.6918e+00,\n",
            "          -7.4257e+00, -7.5585e+00, -3.6293e+00, -7.5600e+00,  3.6830e+00,\n",
            "          -7.5521e+00, -4.6730e+00,  2.5366e+00,  1.9050e+00,  1.7024e+00,\n",
            "          -7.6456e+00, -3.0417e+00, -2.9123e-01, -7.6385e+00, -1.7879e+00,\n",
            "          -3.3004e+00, -4.4533e+00,  1.9548e+00, -1.2898e+00, -1.3008e+00,\n",
            "          -7.6833e+00,  1.2377e+00, -1.4865e+00, -1.7680e+00, -3.9389e+00,\n",
            "          -7.4317e+00, -5.6179e+00, -7.6453e+00, -5.2358e+00, -3.8325e+00],\n",
            "         [-5.6741e-01,  1.4449e+00, -3.0786e+00, -3.0415e+00, -2.9175e+00,\n",
            "          -3.0169e+00,  3.5965e+00, -3.0229e+00, -2.1698e-01, -3.1247e+00,\n",
            "           5.7609e-01, -3.0359e+00, -2.8905e+00, -2.4370e+00, -1.6735e+00,\n",
            "          -2.8581e+00, -3.0433e+00, -3.0612e+00, -1.1573e+00, -2.9300e+00,\n",
            "          -3.2470e+00, -3.0366e+00, -3.2454e+00, -2.9092e+00, -2.9409e+00,\n",
            "          -3.2401e+00, -3.0063e+00, -3.0046e+00, -2.9275e+00, -2.9645e+00,\n",
            "          -3.0651e+00, -2.8271e+00, -3.0302e+00, -3.0684e+00, -2.9174e+00,\n",
            "          -2.9905e+00, -2.9815e+00, -1.0113e+00, -3.2388e+00, -1.9154e+00,\n",
            "          -3.0845e+00, -1.6452e+00,  6.6500e-02,  5.0239e-01, -2.4645e+00,\n",
            "          -3.1831e+00, -1.3831e+00, -5.1382e-01, -3.0077e+00, -6.2449e-01,\n",
            "          -9.6141e-01, -1.0259e+00, -2.4142e+00,  1.5852e+00, -5.9164e-01,\n",
            "          -3.1435e+00,  8.0836e-02, -4.1319e-04, -2.1017e-01, -9.0434e-01,\n",
            "          -2.8350e+00, -1.4895e+00, -3.1956e+00,  3.7771e-01, -1.4470e+00],\n",
            "         [-1.9870e+00,  1.2336e+01, -1.1469e+01, -1.1628e+01, -1.1522e+01,\n",
            "          -1.1428e+01,  2.5967e+00, -1.1659e+01, -1.8111e+00, -1.1568e+01,\n",
            "           6.9777e-01, -1.1317e+01, -1.1323e+01, -9.4434e+00, -5.8712e+00,\n",
            "          -7.3373e+00, -1.1344e+01, -1.1636e+01, -5.3390e+00, -1.1452e+01,\n",
            "          -1.1423e+01, -1.1339e+01, -1.1850e+01, -1.1629e+01, -1.1597e+01,\n",
            "          -1.1840e+01, -1.1325e+01, -1.1370e+01, -1.1333e+01, -1.1347e+01,\n",
            "          -1.1376e+01, -1.0309e+01, -1.1713e+01, -1.1289e+01, -1.1287e+01,\n",
            "          -1.1435e+01, -1.1576e+01, -4.8622e+00, -1.1764e+01, -1.5075e+00,\n",
            "          -1.1446e+01, -4.6435e+00, -7.1403e-01,  1.8677e+00, -3.8526e+00,\n",
            "          -1.1622e+01, -2.8735e+00, -2.2439e+00, -1.1547e+01, -2.9347e+00,\n",
            "          -4.9476e+00, -5.4509e+00, -7.7857e-01,  1.1001e-01, -3.8202e+00,\n",
            "          -1.1830e+01,  5.8112e-01, -2.5133e+00, -7.0248e-01, -5.3662e+00,\n",
            "          -1.1370e+01, -1.0023e+01, -1.1752e+01, -2.9351e+00, -3.9280e+00]],\n",
            "\n",
            "        [[ 1.1349e+01, -1.0944e+00, -6.2644e+00, -6.3631e+00, -6.5792e+00,\n",
            "          -6.1696e+00,  9.4347e-01, -6.2901e+00,  3.5204e+00, -6.7548e+00,\n",
            "           9.8962e-01, -6.1424e+00, -6.4618e+00, -5.4792e+00, -5.2403e+00,\n",
            "          -5.8351e+00, -6.5347e+00, -6.5235e+00, -3.4260e+00, -6.3217e+00,\n",
            "          -6.2973e+00, -6.5651e+00, -6.2686e+00, -6.4681e+00, -6.4972e+00,\n",
            "          -6.1120e+00, -6.3944e+00, -6.3509e+00, -6.5693e+00, -6.1784e+00,\n",
            "          -6.3439e+00, -7.1478e+00, -6.2641e+00, -6.2870e+00, -6.5915e+00,\n",
            "          -5.9930e+00, -6.1019e+00, -4.1987e-02, -5.8992e+00, -3.5597e-01,\n",
            "          -6.4726e+00, -6.6401e+00, -1.5421e+00, -2.4129e+00, -1.8533e+00,\n",
            "          -6.2174e+00, -5.2549e+00, -1.7941e+00, -6.6201e+00,  3.0246e-01,\n",
            "          -2.7558e+00, -2.1543e+00, -2.3097e-01, -2.2048e+00, -6.6080e+00,\n",
            "          -6.6810e+00, -8.6886e-01,  1.1771e-01, -3.3010e+00, -5.9004e+00,\n",
            "          -6.4927e+00, -8.4281e+00, -6.0639e+00, -2.0762e+00, -1.1166e+00],\n",
            "         [ 6.3358e+00, -1.6902e-01, -3.6642e+00, -3.7031e+00, -3.7554e+00,\n",
            "          -3.7352e+00, -1.5806e-01, -3.7857e+00, -2.6885e-01, -3.5857e+00,\n",
            "          -1.8562e-01, -3.9114e+00, -3.7934e+00,  2.3163e+00, -1.7993e-01,\n",
            "          -2.5481e+00, -3.6767e+00, -3.7528e+00,  6.2269e-01, -3.7466e+00,\n",
            "          -3.8999e+00, -3.6178e+00, -3.6340e+00, -3.9460e+00, -3.8952e+00,\n",
            "          -3.6273e+00, -3.7443e+00, -3.8618e+00, -3.7792e+00, -3.9817e+00,\n",
            "          -3.9808e+00,  3.8551e-01, -3.6574e+00, -3.7771e+00, -3.8620e+00,\n",
            "          -3.7751e+00, -3.4391e+00,  3.6330e-01, -3.6149e+00, -7.2999e-01,\n",
            "          -3.5517e+00, -2.9956e+00, -4.0925e-01, -7.7535e-01, -9.3636e-01,\n",
            "          -3.8464e+00, -1.7971e+00, -1.9567e+00, -3.8474e+00, -3.4663e-01,\n",
            "          -3.8590e-01, -9.8698e-01, -8.9791e-01, -4.8464e-01, -1.0368e+00,\n",
            "          -3.9352e+00, -4.8668e-01, -9.6915e-01, -1.4185e+00, -1.8406e+00,\n",
            "          -3.6671e+00, -4.0868e+00, -3.6728e+00, -1.4199e+00, -1.5246e+00],\n",
            "         [ 2.8341e+00, -3.6379e-01, -1.6160e+00, -1.7713e+00, -1.8347e+00,\n",
            "          -1.7830e+00, -4.1172e-01, -1.5384e+00, -3.6275e-01, -1.6027e+00,\n",
            "           5.8343e-02, -1.8107e+00, -1.7556e+00,  4.2650e+00,  1.8186e+00,\n",
            "          -1.4873e+00, -1.6995e+00, -1.7485e+00,  2.1311e+00, -1.5779e+00,\n",
            "          -1.7522e+00, -1.5012e+00, -1.6337e+00, -1.8708e+00, -1.7374e+00,\n",
            "          -1.7389e+00, -1.7028e+00, -1.7069e+00, -1.6455e+00, -1.8366e+00,\n",
            "          -1.8265e+00,  2.6526e+00, -1.8239e+00, -1.6548e+00, -1.7957e+00,\n",
            "          -1.7131e+00, -1.5956e+00,  1.9517e+00, -1.5178e+00, -4.3223e-01,\n",
            "          -1.6563e+00, -1.3016e+00, -3.9672e-01, -8.7099e-01, -4.4921e-01,\n",
            "          -1.5799e+00, -9.8058e-01, -1.4883e+00, -1.8408e+00,  3.3821e-01,\n",
            "           1.1722e+00, -3.1368e-01, -6.9011e-01,  6.2879e-01, -2.6824e-01,\n",
            "          -1.7682e+00, -1.4905e-01, -9.2107e-01, -1.2044e+00, -1.7911e-01,\n",
            "          -1.6048e+00, -1.9840e+00, -1.6046e+00,  3.0057e-02, -1.0926e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.1888,  1.7466,  1.5960,  ..., -1.5042,  1.5729,  0.0789],\n",
            "         [ 0.1800,  0.8983,  0.6151,  ...,  0.6200,  0.2072, -0.9083],\n",
            "         [ 1.0296,  0.4065,  0.2179,  ...,  1.2951,  1.0752,  0.1247]],\n",
            "\n",
            "        [[-0.5685,  1.4144, -0.7249,  ...,  1.7453, -0.5045, -0.9947],\n",
            "         [-0.6914,  0.7507, -0.5484,  ..., -0.3949,  0.7703, -0.2500],\n",
            "         [-1.2245, -0.0412,  0.0396,  ...,  1.4759,  1.2736,  0.5017]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-1.0016e-03, -9.8651e-07, -1.8187e-03,  ...,  1.2125e-03,\n",
            "          -6.3171e-02,  1.8205e-01],\n",
            "         [-1.8359e-09, -5.8080e-03, -7.1311e-01,  ...,  2.4270e-04,\n",
            "          -1.9278e-02,  4.4017e-01],\n",
            "         [ 4.3536e-06, -1.0105e-04, -7.9563e-01,  ...,  1.9345e-06,\n",
            "          -8.3442e-01,  2.9160e-04]],\n",
            "\n",
            "        [[-2.4647e-02, -8.5203e-06, -7.1973e-05,  ..., -3.8638e-05,\n",
            "          -1.0984e-03,  1.3622e-01],\n",
            "         [ 4.6310e-04,  2.2096e-03,  5.3760e-04,  ...,  1.5177e-03,\n",
            "          -1.5309e-01, -4.2526e-01],\n",
            "         [-9.6201e-02,  5.9653e-01, -5.9304e-01,  ...,  1.3795e-02,\n",
            "          -5.7563e-01,  5.6091e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-1.9068, -1.1363, -5.3719, -5.4952, -5.6949, -5.4923, -0.8269,\n",
            "          -5.6816, -4.0935, -5.3584, -2.1029, -5.3380, -5.5254, -5.7652,\n",
            "          -4.3763, -5.0518, -5.3375, -5.7215, -4.5474, -5.1582, -5.5229,\n",
            "          -5.3174, -5.4828, -5.2927, -5.4493, -5.4706, -5.2839, -5.2924,\n",
            "          -5.3127, -5.0662, -5.6385, -5.2420, -5.4449, -5.2143, -5.4937,\n",
            "          -5.4363, -5.2597, -1.9690, -5.7051, -0.9011, -5.3595, -1.3856,\n",
            "          -2.9427,  1.0822, -2.8049, -5.2713, -2.7441, -0.6587, -5.5528,\n",
            "          -2.0684, -1.3471, -1.6760,  0.3062, -2.3730, -0.5098, -5.2213,\n",
            "           1.8527,  0.0692,  7.2858, -2.3737, -5.3580, -5.1300, -5.4470,\n",
            "          -2.8540,  1.7503],\n",
            "         [-3.3915, -1.5733, -7.1481, -7.7781, -7.5565, -7.9915, -0.3560,\n",
            "          -7.9541, -4.0385, -8.0081, -0.3874, -7.6158, -7.7088, -4.4189,\n",
            "          -4.3823, -1.8030, -7.6737, -7.7010, -3.8021, -7.3431, -7.5503,\n",
            "          -7.8414, -7.7085, -7.5950, -7.7586, -7.4910, -7.5679, -7.5581,\n",
            "          -7.4868, -7.6500, -7.8931, -4.1910, -7.7204, -7.5913, -7.7275,\n",
            "          -7.3762, -7.7646, -2.0316, -7.9073,  0.7564, -7.6732, -4.5958,\n",
            "          -2.9046,  7.7489, -2.9432, -7.7147, -2.5589,  5.0760, -7.3237,\n",
            "          -4.8610, -0.3698, -3.8115,  1.8519, -2.3712, -1.8473, -7.4200,\n",
            "          -1.8136,  2.2085, -1.2553, -5.6648, -7.5752, -6.6149, -7.4766,\n",
            "          -5.0841, -2.5821],\n",
            "         [-3.3078,  1.7461, -8.2595, -8.3224, -8.2737, -8.4493, -1.7084,\n",
            "          -8.4793, -3.9822, -8.4551, -1.4755, -8.5068, -8.3184, -6.7837,\n",
            "          -5.5366, -3.9355, -8.3254, -8.3940, -5.7559, -8.2785, -8.1839,\n",
            "          -8.5380, -8.5861, -8.5921, -8.3196, -7.8584, -8.4985, -8.2942,\n",
            "          -8.1219, -8.3071, -8.4715, -6.8888, -8.1105, -8.2405, -8.3752,\n",
            "          -7.9213, -8.3690, -3.6150, -8.0833,  3.3888, -8.4259, -5.0513,\n",
            "           0.0868,  2.3566,  0.9674, -8.3050, -2.5810,  1.3411, -8.2656,\n",
            "          -2.3184, -3.9364, -3.7890,  6.3533, -3.2107, -1.9390, -7.9442,\n",
            "           0.3781,  0.1812, -1.4387, -4.5343, -8.1695, -6.2884, -8.1226,\n",
            "          -5.5384, -2.9397]],\n",
            "\n",
            "        [[-1.5930, -1.5447, -3.4827, -3.4458, -3.4754, -3.5085, -0.8257,\n",
            "          -3.5770, -2.0465, -3.5259, -0.8769, -3.5288, -3.5504, -2.6287,\n",
            "          -2.1755,  1.8997, -3.4463, -3.5642, -1.9395, -3.4843, -3.5171,\n",
            "          -3.4649, -3.4937, -3.5500, -3.5485, -3.4783, -3.3401, -3.4292,\n",
            "          -3.5357, -3.5232, -3.5776, -3.1248, -3.5153, -3.4485, -3.4991,\n",
            "          -3.4693, -3.3565, -1.7707, -3.6201,  2.1100, -3.5671, -1.2401,\n",
            "          -1.6179, -2.0419,  1.5061, -3.4870,  3.2312, -0.2238, -3.5939,\n",
            "          -1.4094, -1.5865,  1.3757, -0.5307, -0.4868,  1.2340, -3.6439,\n",
            "          -0.8566,  3.1039, -0.6819, -0.9713, -3.3391,  3.9678, -3.4741,\n",
            "          -2.0841, -1.1084],\n",
            "         [-1.6788, -1.7019, -2.4732, -2.1798, -2.4632, -2.7106,  0.1178,\n",
            "          -2.7005, -0.9391, -2.3219, -0.8625, -2.8796, -2.5632, -1.5653,\n",
            "           0.4174,  0.6877, -2.3132, -2.4642,  0.0361, -2.7872, -2.8219,\n",
            "          -2.4895, -2.5933, -2.8581, -2.5034, -2.6994, -2.4382, -2.3643,\n",
            "          -2.6401, -3.0139, -2.7110, -1.1718, -2.5485, -2.7340, -2.6596,\n",
            "          -2.9386, -2.2030, -0.8630, -2.6950, -2.0457, -2.5632, -0.9827,\n",
            "          -0.7925, -1.6423, -1.0609, -2.7039, -0.0528, -0.2096, -2.5074,\n",
            "          -0.7621, -0.9868,  0.4029, -1.0423,  6.7608,  3.0976, -2.7716,\n",
            "           0.2370,  0.2383,  0.5739,  7.2593, -2.4143,  1.6424, -2.7464,\n",
            "           1.1451, -1.5976],\n",
            "         [-0.1093, -0.5208, -5.3987, -5.2877, -5.3739, -5.6212,  0.2937,\n",
            "          -5.6476,  1.2014, -5.2190, -1.6156, -5.7090, -5.4802, -3.9029,\n",
            "          -2.9243, -4.4832, -5.7134, -5.5797, -3.2035, -5.7877, -5.5053,\n",
            "          -5.3315, -5.3144, -5.5987, -5.4492, -5.7300, -5.3162, -5.4202,\n",
            "          -5.2737, -5.4260, -5.6841, -3.6482, -5.8838, -5.7069, -5.4851,\n",
            "          -5.5613, -5.1624, -2.1216, -5.7512, -3.3423, -5.9205,  0.0817,\n",
            "          -0.7090,  1.3058, -3.7373, -5.6485, -1.2906,  0.1727, -5.6472,\n",
            "          -2.1145, -1.0170, -2.6700, -1.0861, -2.3294, -0.9402, -5.6428,\n",
            "           5.3061, -2.4658, -0.4375, -0.5944, -5.1828, -4.4502, -5.4236,\n",
            "          -1.6051, -0.1376]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[ 0.5759, -0.8257,  0.3730,  ..., -0.8519, -0.2828,  0.0931],\n",
            "         [ 0.5759, -0.8257,  0.3730,  ..., -0.8519, -0.2828,  0.0931],\n",
            "         [ 1.3409, -0.1370, -0.1563,  ...,  0.1716,  0.6776,  0.2737]],\n",
            "\n",
            "        [[ 1.0291,  0.4066,  0.2183,  ...,  1.2945,  1.0714,  0.1226],\n",
            "         [-0.5818, -0.2171, -1.1900,  ...,  0.5362,  1.1178, -1.0393],\n",
            "         [-1.5226,  0.2204, -0.2146,  ...,  1.6482, -0.1750, -0.9127]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 5.7275e-02, -9.5978e-04,  1.6134e-04,  ..., -4.2939e-02,\n",
            "           1.2436e-05,  1.7160e-03],\n",
            "         [ 5.8154e-01, -7.0387e-03,  6.1388e-05,  ..., -5.7686e-02,\n",
            "           3.1627e-06, -1.2332e-05],\n",
            "         [ 9.7629e-01, -1.3060e-02, -6.5744e-02,  ...,  8.1475e-04,\n",
            "           1.3316e-01, -7.3661e-02]],\n",
            "\n",
            "        [[ 2.2637e-03, -2.7147e-02, -7.4550e-01,  ...,  3.3237e-04,\n",
            "          -2.0197e-03,  6.3770e-05],\n",
            "         [ 2.3088e-03, -1.5251e-09, -1.2439e-04,  ...,  5.8700e-04,\n",
            "          -1.4833e-03,  7.6161e-01],\n",
            "         [ 6.6548e-05,  3.3194e-03, -8.2187e-05,  ...,  1.2020e-02,\n",
            "           3.6285e-02,  7.6156e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ 1.8320e+00, -2.7961e-01, -1.8900e+00, -1.8903e+00, -2.0303e+00,\n",
            "          -1.9595e+00, -3.9562e-01, -1.8682e+00, -8.9657e-01, -1.8456e+00,\n",
            "          -1.9278e-01, -1.9132e+00, -1.9136e+00,  2.5824e+00,  1.9540e+00,\n",
            "          -1.5058e+00, -1.9498e+00, -1.9774e+00,  2.4074e+00, -1.8319e+00,\n",
            "          -1.9243e+00, -1.7737e+00, -1.8670e+00, -1.9864e+00, -1.9648e+00,\n",
            "          -1.9518e+00, -1.9127e+00, -1.9043e+00, -1.8943e+00, -2.0031e+00,\n",
            "          -1.9585e+00,  2.2217e+00, -1.9753e+00, -1.9370e+00, -2.0335e+00,\n",
            "          -1.9354e+00, -1.8553e+00,  2.2113e+00, -1.8385e+00, -5.7538e-01,\n",
            "          -1.9589e+00, -1.1239e+00, -6.6484e-01, -2.4896e-01, -6.0771e-01,\n",
            "          -1.8716e+00, -6.6733e-01, -9.5035e-01, -2.0969e+00, -1.5548e-01,\n",
            "           3.1794e-01, -4.6495e-01, -5.3588e-01,  5.9097e-01, -5.1251e-01,\n",
            "          -1.9672e+00, -1.2321e-01, -6.2593e-01, -9.9252e-01, -4.0049e-01,\n",
            "          -1.8850e+00, -1.9623e+00, -1.8624e+00, -4.0307e-01, -9.5766e-01],\n",
            "         [ 2.1565e+00, -3.5232e-01, -2.0863e+00, -2.1897e+00, -2.3139e+00,\n",
            "          -2.2478e+00, -4.1222e-01, -2.1076e+00, -9.6530e-01, -2.0875e+00,\n",
            "          -2.1858e-01, -2.2676e+00, -2.2203e+00,  3.2634e+00,  2.5632e+00,\n",
            "          -1.7442e+00, -2.2809e+00, -2.2582e+00,  3.6149e+00, -2.1421e+00,\n",
            "          -2.1738e+00, -2.0454e+00, -2.1181e+00, -2.2894e+00, -2.2370e+00,\n",
            "          -2.2812e+00, -2.2035e+00, -2.1933e+00, -2.2005e+00, -2.3606e+00,\n",
            "          -2.2672e+00,  2.8656e+00, -2.2775e+00, -2.2196e+00, -2.2823e+00,\n",
            "          -2.2265e+00, -2.1189e+00,  2.5873e+00, -2.0882e+00, -7.4422e-01,\n",
            "          -2.2891e+00, -1.1986e+00, -5.6634e-01, -4.7557e-01, -8.5349e-01,\n",
            "          -2.1154e+00, -7.9273e-01, -1.0863e+00, -2.3365e+00, -3.3918e-01,\n",
            "           1.0605e+00, -5.9059e-01, -8.5260e-01,  1.1316e+00, -1.9902e-01,\n",
            "          -2.2260e+00, -3.1069e-01, -8.6816e-01, -9.8331e-01, -2.4678e-01,\n",
            "          -2.1435e+00, -2.1900e+00, -2.1897e+00, -4.1953e-01, -1.3294e+00],\n",
            "         [ 8.1015e-01, -1.9665e+00, -1.2723e+00, -1.3434e+00, -1.4565e+00,\n",
            "          -1.3725e+00,  2.7644e-01, -1.4416e+00, -2.7942e+00, -9.6862e-01,\n",
            "           7.2732e-01, -1.4855e+00, -1.2414e+00,  4.5693e+00,  1.4713e+00,\n",
            "          -4.4961e-01, -1.6072e+00, -1.6296e+00,  3.1500e+00, -1.2241e+00,\n",
            "          -1.6644e+00, -1.0045e+00, -1.2362e+00, -1.3627e+00, -1.4891e+00,\n",
            "          -1.5830e+00, -1.4140e+00, -1.5742e+00, -1.4000e+00, -1.3884e+00,\n",
            "          -1.6589e+00,  3.4532e+00, -1.7788e+00, -1.4688e+00, -1.4934e+00,\n",
            "          -1.4380e+00, -1.0037e+00,  1.1178e+00, -1.3616e+00, -3.3497e+00,\n",
            "          -1.9589e+00, -9.4582e-01,  1.0831e+00,  1.2257e-01, -3.2657e+00,\n",
            "          -1.5032e+00, -1.8522e-01, -1.1789e+00, -1.1498e+00, -1.7996e+00,\n",
            "           1.7726e+01, -3.4244e-01, -3.7797e+00,  3.0270e+00,  5.5249e+00,\n",
            "          -1.1197e+00, -8.6099e-01, -1.7166e+00,  7.1661e-01,  1.6997e+00,\n",
            "          -1.5093e+00, -1.0567e+00, -1.2206e+00, -1.7898e+00, -4.8121e-01]],\n",
            "\n",
            "        [[-1.7081e+00,  3.2838e+00, -7.4442e+00, -7.6575e+00, -7.6459e+00,\n",
            "          -7.5322e+00, -9.0665e-01, -7.6328e+00, -3.2656e+00, -7.4973e+00,\n",
            "          -1.2836e+00, -7.5626e+00, -7.5486e+00, -5.4183e+00, -4.8265e+00,\n",
            "          -4.4338e+00, -7.6318e+00, -7.5928e+00, -4.4981e+00, -7.4222e+00,\n",
            "          -7.6530e+00, -7.5503e+00, -7.6665e+00, -7.6348e+00, -7.5530e+00,\n",
            "          -7.3593e+00, -7.6802e+00, -7.6209e+00, -7.4713e+00, -7.4918e+00,\n",
            "          -7.6464e+00, -5.9650e+00, -7.4986e+00, -7.6168e+00, -7.6456e+00,\n",
            "          -7.3871e+00, -7.5376e+00, -3.6901e+00, -7.5181e+00,  3.7507e+00,\n",
            "          -7.5170e+00, -4.5739e+00,  2.0848e+00,  1.8771e+00,  2.2143e+00,\n",
            "          -7.6241e+00, -3.0745e+00, -4.2972e-01, -7.6075e+00, -1.7050e+00,\n",
            "          -3.4259e+00, -4.3970e+00,  2.5619e+00, -1.4268e+00, -1.4042e+00,\n",
            "          -7.6430e+00,  1.2736e+00, -1.5759e+00, -1.8066e+00, -3.8716e+00,\n",
            "          -7.4171e+00, -5.5448e+00, -7.5684e+00, -5.2157e+00, -3.9116e+00],\n",
            "         [-8.9687e-01, -6.2635e-02, -3.6592e+00, -3.6475e+00, -3.5000e+00,\n",
            "          -3.5932e+00,  2.3559e-02, -3.7286e+00, -7.5242e-01, -3.6388e+00,\n",
            "          -1.7001e+00, -3.7075e+00, -3.7061e+00, -3.2320e+00, -2.0219e+00,\n",
            "          -2.6438e+00, -3.8426e+00, -3.9282e+00, -2.1708e+00, -3.7463e+00,\n",
            "          -3.7299e+00, -3.7833e+00, -3.7912e+00, -3.7723e+00, -3.4380e+00,\n",
            "          -3.6294e+00, -3.7046e+00, -3.6718e+00, -3.5528e+00, -3.5952e+00,\n",
            "          -3.8818e+00, -3.7155e+00, -3.5616e+00, -3.6622e+00, -3.5410e+00,\n",
            "          -3.4983e+00, -3.6009e+00, -1.8504e+00, -3.9531e+00, -1.7366e+00,\n",
            "          -3.5767e+00, -2.0481e+00,  1.9309e-01, -9.4357e-01, -2.0809e+00,\n",
            "          -4.0477e+00, -2.0985e+00, -2.8683e-01, -3.7089e+00,  4.9635e+00,\n",
            "          -2.4038e+00, -2.2754e+00,  5.0297e-01, -1.3494e-01, -7.3985e-01,\n",
            "          -3.7522e+00,  4.1254e+00, -1.2913e+00, -5.8580e-01, -1.0879e+00,\n",
            "          -3.6969e+00, -2.0382e+00, -3.7594e+00, -2.3218e-01, -5.3055e-01],\n",
            "         [ 6.5609e-01,  1.2326e+00, -3.3477e+00, -3.6240e+00, -3.2591e+00,\n",
            "          -3.4881e+00,  3.6345e+00, -3.4985e+00,  6.6421e+00, -3.4793e+00,\n",
            "          -2.0987e-01, -3.4105e+00, -3.5117e+00, -1.9817e+00, -1.8501e+00,\n",
            "          -1.8469e+00, -3.4008e+00, -3.2729e+00, -9.7684e-01, -3.7002e+00,\n",
            "          -3.5056e+00, -3.5921e+00, -3.3442e+00, -3.3792e+00, -3.4577e+00,\n",
            "          -3.5355e+00, -3.3409e+00, -3.3144e+00, -3.4285e+00, -3.3682e+00,\n",
            "          -3.4093e+00, -2.0170e+00, -3.3686e+00, -3.4660e+00, -3.3100e+00,\n",
            "          -3.2731e+00, -3.4695e+00, -1.3176e+00, -3.6226e+00, -1.6088e+00,\n",
            "          -3.6015e+00, -2.2437e+00,  1.5704e-02, -6.5893e-01, -3.0736e+00,\n",
            "          -3.4724e+00, -1.3055e-01, -2.0484e-01, -3.4621e+00, -2.0601e+00,\n",
            "          -1.1532e+00, -2.1638e+00, -2.0479e+00, -1.3385e+00, -1.8837e+00,\n",
            "          -3.5338e+00,  1.3815e-01, -1.9241e+00, -1.6856e+00, -1.2085e+00,\n",
            "          -3.5560e+00, -3.3958e+00, -3.6665e+00,  1.0574e+00, -1.9687e+00]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-1.2265, -0.0405,  0.0400,  ...,  1.4737,  1.2745,  0.5010],\n",
            "         [-0.3786, -0.2182, -0.5362,  ...,  2.3755, -1.6825,  2.2815],\n",
            "         [ 0.8442, -0.9181,  0.2698,  ...,  2.0005, -1.0392, -0.3774]],\n",
            "\n",
            "        [[ 0.8666,  0.3682, -0.3904,  ...,  0.1050,  0.1581,  0.2533],\n",
            "         [-0.5581, -1.3559,  0.6024,  ...,  0.7710,  1.3872,  0.1554],\n",
            "         [ 0.5739, -0.8243,  0.3717,  ..., -0.8509, -0.2818,  0.0922]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[-2.4413e-02, -1.0868e-02, -2.9538e-02,  ...,  3.3492e-03,\n",
            "          -4.2349e-01,  6.6455e-02],\n",
            "         [ 1.2451e-02, -2.1864e-02, -2.4011e-02,  ...,  1.3093e-02,\n",
            "          -6.6875e-01,  3.0393e-04],\n",
            "         [ 2.1675e-01, -2.9946e-04, -6.0209e-02,  ..., -5.0646e-01,\n",
            "          -3.7891e-03,  2.1384e-03]],\n",
            "\n",
            "        [[-1.4282e-03,  1.8188e-02,  3.1934e-04,  ...,  6.3536e-02,\n",
            "          -3.4094e-05, -1.0233e-02],\n",
            "         [ 2.6252e-03,  1.2864e-02, -7.5374e-01,  ..., -1.6290e-01,\n",
            "          -6.2178e-01,  3.2507e-01],\n",
            "         [ 2.6357e-02, -4.4476e-04,  3.9852e-08,  ..., -8.4320e-01,\n",
            "          -1.6017e-06, -3.8276e-06]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.0991, -1.2561, -6.1264, -6.0249, -6.1020, -6.2974,  0.9826,\n",
            "          -6.2014, -0.1914, -6.1934, -1.3232, -6.2245, -5.9995, -5.4038,\n",
            "          -3.5724, -4.1557, -6.1405, -6.1249, -3.8677, -6.1389, -6.2047,\n",
            "          -6.0258, -6.2337, -6.2399, -6.1161, -6.2253, -6.2212, -6.0085,\n",
            "          -6.0925, -6.1110, -6.2822, -5.2336, -6.3404, -6.1656, -6.1319,\n",
            "          -6.1573, -6.0779, -2.5182, -6.4715, -2.1185, -6.2389, -0.9880,\n",
            "          -1.7536,  0.7781, -2.4388, -6.1987, -2.4073, -0.2083, -6.3059,\n",
            "          -0.6244, -2.0974, -1.9671,  0.4712, -2.2690, -1.9149, -6.2397,\n",
            "           3.7654, -0.5481, -0.0741, -1.4426, -6.1043, -5.2534, -6.1086,\n",
            "          -1.1990, -1.1786],\n",
            "         [-0.8409,  0.6958, -6.1137, -5.9178, -6.1206, -6.2572,  2.6372,\n",
            "          -6.2556, -3.6615, -6.2453, -0.8398, -6.1117, -6.1121, -4.7492,\n",
            "          -3.5762, -3.2585, -6.2396, -6.1966, -3.3063, -6.0626, -6.2368,\n",
            "          -6.0327, -5.9894, -6.1170, -6.1297, -6.4748, -5.8685, -6.1059,\n",
            "          -5.9789, -6.1443, -6.2851, -4.6025, -6.2482, -6.2270, -6.0833,\n",
            "          -5.9332, -6.0937, -2.3232, -6.2754, -2.7922, -6.3703, -2.2414,\n",
            "          -2.5048,  2.3609, -3.5108, -6.2794, -0.9811, -0.4860, -6.3521,\n",
            "          -3.5080, -1.4865, -0.6525, -2.3302,  2.1304, -2.1649, -6.3071,\n",
            "          -0.9156,  2.3408,  2.0833, -2.5337, -5.9965, -4.1748, -6.0930,\n",
            "          -2.8616, -1.1023],\n",
            "         [-1.2404,  2.2357, -2.9390, -3.0677, -3.0041, -2.8498, -0.3271,\n",
            "          -3.0780, -1.8344, -3.0345, -0.2854, -2.9051, -3.3533, -1.3994,\n",
            "          -1.1228,  0.4371, -3.2276, -3.1685, -0.5187, -2.8772, -2.9316,\n",
            "          -3.1546, -2.9976, -3.1449, -3.0296, -3.2237, -3.2079, -3.1091,\n",
            "          -3.0869, -3.1343, -3.0430, -1.4433, -3.1411, -3.0799, -3.1897,\n",
            "          -2.9424, -3.1317, -0.6134, -3.2874, -0.5271, -3.0760, -0.1496,\n",
            "          -0.9135, -0.4092, -1.1103, -3.1569,  5.5461,  1.3514, -3.2105,\n",
            "          -1.8358, -0.8728,  0.3762, -0.7877,  0.6668, -1.8669, -3.0945,\n",
            "          -1.1485,  0.5749, -1.1996, -1.4035, -2.9633, -1.0767, -3.1713,\n",
            "          -1.7007, -1.0950]],\n",
            "\n",
            "        [[-2.2669, -0.3477, -0.8362, -0.8129, -0.9916, -0.8922,  0.0747,\n",
            "          -0.8636, -0.5118, -1.0991,  7.0794, -0.7796, -0.9335, -1.4838,\n",
            "          -0.7444, -1.9182, -0.9582, -0.8230, -0.8412, -0.8201, -1.1166,\n",
            "          -0.8436, -0.9035, -1.0304, -1.0399, -0.8340, -1.0347, -0.7248,\n",
            "          -0.8958, -0.9921, -0.9063, -1.8987, -0.9826, -0.8763, -0.8494,\n",
            "          -0.8738, -0.8963, -0.9371, -1.1106, -0.6403, -0.7503, -1.1116,\n",
            "          -1.4197, -1.2149, -0.6589, -0.7769, -0.1086,  0.2968, -0.8886,\n",
            "          -0.2336,  0.7469,  0.0726, -1.2236, -0.5876, -2.8984, -1.1714,\n",
            "          -2.4513,  0.0813, -1.2846, -0.7008, -0.8636, -0.8394, -1.0057,\n",
            "           6.6176, -1.5284],\n",
            "         [12.9218,  1.4555, -6.3591, -6.2391, -6.6568, -6.3709,  0.7764,\n",
            "          -6.7611, -0.3848, -6.7497, -1.6950, -6.9269, -6.5877, -0.6099,\n",
            "          -1.5242, -5.2915, -6.5308, -6.2507,  1.9092, -6.6834, -6.5720,\n",
            "          -6.2565, -6.2394, -6.3319, -6.4096, -6.7122, -6.6982, -6.6076,\n",
            "          -7.1153, -6.7786, -6.8952, -0.2878, -6.3522, -6.6172, -6.6330,\n",
            "          -6.9107, -6.6834,  3.4102, -6.5719, -3.4982, -6.5666, -4.8248,\n",
            "          -1.4086, -1.0067, -3.3512, -6.6834, -2.6514,  1.7426, -6.7903,\n",
            "          -0.6034, -3.2992, -1.5449, -3.5418,  1.2207, -5.2856, -6.9771,\n",
            "           0.0683, -1.1149, -1.6055, -3.2059, -6.1101, -7.1170, -6.1432,\n",
            "          -0.0244, -1.7602],\n",
            "         [ 1.6229, -0.5733, -2.0924, -2.3131, -2.3624, -2.4900, -0.7986,\n",
            "          -2.3617, -0.8142, -2.3445, -0.8785, -2.4330, -2.3469,  1.6475,\n",
            "           3.4601, -0.7602, -2.4510, -2.3746,  2.2357, -2.1902, -2.4126,\n",
            "          -2.2908, -2.2557, -2.2921, -2.3107, -2.3628, -2.1578, -2.2721,\n",
            "          -2.2917, -2.3722, -2.4583,  3.4372, -2.2459, -2.3034, -2.2348,\n",
            "          -2.2956, -2.3341,  2.9539, -2.3685, -1.2214, -2.3283, -0.8521,\n",
            "          -0.2428, -0.2318, -0.5236, -2.4101, -0.6647, -0.9986, -2.1283,\n",
            "          -1.0960,  0.4758, -0.8638, -0.9020, -0.4104,  0.1624, -2.4948,\n",
            "          -0.0493, -0.9557, -0.7471, -0.1509, -2.2281, -1.9297, -2.2986,\n",
            "          -1.2435, -1.0469]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-0.2389, -1.2329, -0.6818,  ...,  1.5741, -0.7492,  1.0305],\n",
            "         [-0.4041, -1.0529,  0.6220,  ...,  0.6999,  0.0788,  0.0296],\n",
            "         [ 1.0269,  0.4035,  0.2209,  ...,  1.2957,  1.0663,  0.1206]],\n",
            "\n",
            "        [[-0.5700,  1.4179, -0.7217,  ...,  1.7440, -0.5025, -1.0010],\n",
            "         [ 0.1137, -0.5280, -0.8154,  ...,  0.6835,  0.9515,  0.2014],\n",
            "         [ 1.0269,  0.4035,  0.2209,  ...,  1.2957,  1.0663,  0.1206]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 6.2209e-02, -7.1194e-01, -1.4789e-02,  ...,  3.1257e-01,\n",
            "           9.0424e-02, -3.1642e-01],\n",
            "         [ 9.6030e-01, -9.5913e-01, -9.6208e-01,  ..., -6.7339e-01,\n",
            "           9.0736e-02, -7.3041e-01],\n",
            "         [ 1.8542e-02, -9.7073e-01, -9.9454e-01,  ..., -3.2822e-01,\n",
            "           4.1477e-01, -1.4312e-01]],\n",
            "\n",
            "        [[-2.7605e-02, -5.5791e-06, -8.2347e-05,  ..., -2.9161e-05,\n",
            "          -7.3519e-04,  3.5349e-02],\n",
            "         [-6.0194e-04,  1.9014e-02, -1.4122e-01,  ...,  1.1657e-02,\n",
            "          -3.4724e-01,  3.4599e-01],\n",
            "         [ 1.3674e-04, -1.0987e-03, -7.7405e-01,  ...,  2.0040e-03,\n",
            "          -7.8430e-01,  1.0453e-05]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[-0.5318,  0.7353, -3.6944, -3.6566, -3.4892, -3.9931,  0.6674,\n",
            "          -3.9146, -1.6658, -3.2375, -2.4239, -4.0462, -3.4986, -1.5722,\n",
            "          -1.0358, -0.4957, -3.6575, -3.7794, -1.1217, -3.5357, -3.7036,\n",
            "          -3.1344, -3.4966, -3.6362, -3.7879, -3.7915, -3.4706, -3.3219,\n",
            "          -3.5022, -3.7048, -3.9931, -0.7350, -3.7211, -3.9596, -3.5166,\n",
            "          -3.6949, -3.4080, -1.3637, -3.6929, -3.5545, -3.7808,  0.2663,\n",
            "           0.1631,  1.5230, -3.7788, -3.8422, -3.1785, -2.6976, -3.5767,\n",
            "          -3.2761,  1.8802, -1.8175, -3.2161,  1.4022, 11.7667, -3.8317,\n",
            "          -0.0869, -2.9612,  4.6821,  1.1818, -3.3552, -2.0126, -3.6699,\n",
            "          -2.6267,  0.9079],\n",
            "         [-2.6441,  2.3489, -5.9514, -6.3106, -6.0336, -5.7762, -1.7062,\n",
            "          -6.5191, -3.7716, -5.8914, -1.8005, -6.7310, -6.3798, -1.3711,\n",
            "          -4.3200, -2.7296, -6.6452, -5.8093, -2.3170, -6.1928, -6.2015,\n",
            "          -5.8242, -5.8789, -6.1176, -6.5537, -6.4487, -6.0564, -6.3573,\n",
            "          -6.4937, -6.2934, -6.6941, -0.5974, -6.1013, -6.3146, -6.5966,\n",
            "          -6.1540, -6.1810, -2.8884, -6.4928, -0.7503, -6.1390, -1.0486,\n",
            "          -3.3016, 15.3759, -4.6300, -6.3457, -5.6988, -0.8836, -5.8812,\n",
            "          -7.5413, -0.2829, -6.1788, -4.7499,  0.9576,  2.3395, -6.6278,\n",
            "           0.0707, -3.7202,  1.4770, -2.1093, -6.3327, -6.6124, -6.1724,\n",
            "          -6.6628, -2.5130],\n",
            "         [-2.6048,  2.8047, -8.9630, -9.1585, -9.0567, -8.6620, -3.5071,\n",
            "          -9.4836, -6.1660, -9.3709, -1.8362, -9.2785, -9.4261, -5.1495,\n",
            "          -6.3134, -4.2022, -9.3178, -9.1555, -5.5298, -9.2603, -9.1924,\n",
            "          -9.5734, -8.9861, -9.4248, -9.5949, -8.9003, -9.2599, -9.2375,\n",
            "          -9.4248, -8.8235, -9.4869, -6.5697, -9.2339, -9.2546, -9.3216,\n",
            "          -8.7738, -9.3709, -4.4763, -9.1641, 11.3676, -9.0403, -4.1547,\n",
            "          -1.1221,  3.4761,  3.6440, -9.3583, -3.5674, -0.6560, -9.2107,\n",
            "          -3.1357, -3.8992, -4.4890,  1.7003, -0.6822, -1.6526, -9.2856,\n",
            "           0.4200, -0.8763, -0.9935, -5.0822, -9.4409, -5.6858, -9.5594,\n",
            "          -7.6254, -4.5849]],\n",
            "\n",
            "        [[-1.5774, -1.4041, -4.0031, -3.9417, -4.0792, -4.0505, -0.9064,\n",
            "          -4.1018, -2.1590, -3.9802, -0.9367, -4.0158, -3.9950, -3.0026,\n",
            "          -2.5263,  1.5060, -3.9832, -4.1187, -2.3612, -3.9972, -4.0256,\n",
            "          -3.9811, -4.0341, -4.0498, -4.1225, -4.0235, -3.8339, -4.0352,\n",
            "          -4.0661, -4.0601, -4.0517, -3.4220, -4.1019, -3.9223, -4.0642,\n",
            "          -4.0953, -3.8713, -2.0472, -4.1286,  1.9735, -4.0724, -1.5556,\n",
            "          -1.7329, -1.7731,  1.9765, -4.0277,  2.6390, -0.5378, -4.0957,\n",
            "          -1.4568, -1.8305,  2.0595, -0.3302, -0.4910,  1.2847, -4.1777,\n",
            "          -1.0567,  3.6511, -0.6509, -0.8931, -3.8775,  3.0963, -4.0422,\n",
            "          -2.4222, -1.2168],\n",
            "         [-0.5035, -0.1520, -7.8158, -7.6415, -7.9534, -7.7235,  1.4106,\n",
            "          -7.7189,  1.4852, -7.6434, -0.9056, -7.8169, -7.8621, -4.6338,\n",
            "          -4.5470, -3.6577, -7.9676, -7.9443, -2.8215, -7.6900, -7.7582,\n",
            "          -8.0182, -8.1254, -8.0714, -7.8791, -8.1105, -7.9315, -7.9890,\n",
            "          -7.9135, -7.6541, -7.9046, -4.4784, -7.9923, -7.7652, -7.7082,\n",
            "          -7.6511, -7.9671, -2.8978, -7.9117, -2.6889, -8.0338, -3.7668,\n",
            "          -2.8280,  8.0065, -3.9627, -7.8723, -3.0466,  2.3684, -7.9468,\n",
            "          -4.1474, -0.7388, -5.2196, -0.4720, -3.2654, -1.8809, -8.0560,\n",
            "           2.1518, -2.5950, -2.0000, -2.9649, -7.8371, -7.5739, -7.8985,\n",
            "          -4.0128, -2.8262],\n",
            "         [-1.0049,  5.1097, -7.4292, -7.6736, -7.7630, -7.7972, -0.2474,\n",
            "          -7.7263, -3.9650, -7.6398, -0.7484, -7.6208, -7.6222, -5.0323,\n",
            "          -5.2215, -3.6976, -7.6752, -7.7590, -4.1374, -7.3903, -7.8532,\n",
            "          -7.6239, -7.7229, -7.7652, -7.7974, -7.6605, -7.7096, -7.6528,\n",
            "          -7.7736, -7.7932, -7.9074, -5.7721, -7.5777, -7.6693, -7.7091,\n",
            "          -7.5659, -7.6217, -3.7545, -7.6763,  2.5289, -7.8236, -4.6903,\n",
            "           0.6083,  1.8829,  0.8697, -7.6991, -1.5879,  0.2737, -7.7241,\n",
            "          -2.9116, -2.5431, -3.6317,  2.0671, -0.7109, -1.1613, -7.6243,\n",
            "           0.2982, -1.5699, -1.4893, -4.1810, -7.4656, -4.4452, -7.6833,\n",
            "          -4.6951, -3.8844]]], grad_fn=<SliceBackward0>)\n",
            "torch.Size([5, 3, 256])\n",
            "tensor([[[-2.1685, -0.3257,  1.0346,  ...,  0.4367,  0.9973,  0.3317],\n",
            "         [-0.5712,  1.4193, -0.7203,  ...,  1.7427, -0.5023, -1.0037],\n",
            "         [-0.2387, -1.2313, -0.6817,  ...,  1.5741, -0.7484,  1.0302]],\n",
            "\n",
            "        [[ 0.1777,  0.8916,  0.6213,  ...,  0.6106,  0.2120, -0.9034],\n",
            "         [ 1.0255,  0.4014,  0.2220,  ...,  1.2965,  1.0638,  0.1205],\n",
            "         [ 0.8726,  0.3752, -0.3899,  ...,  0.1121,  0.1561,  0.2553]]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            " x, _ = self.rnn(x)   x, _ = self.rnn(x)   x, _ = self.rnn(x) \n",
            "torch.Size([5, 3, 1024])\n",
            "tensor([[[ 1.0635e-01,  2.0088e-03, -4.0309e-01,  ...,  3.0763e-01,\n",
            "          -7.3508e-01, -5.0412e-02],\n",
            "         [-6.1774e-02,  2.9838e-07, -3.9365e-04,  ...,  7.0999e-05,\n",
            "          -4.5855e-05,  7.5115e-01],\n",
            "         [ 9.4053e-03, -6.5695e-01, -1.7035e-02,  ...,  4.4951e-01,\n",
            "           3.4332e-01, -7.4156e-01]],\n",
            "\n",
            "        [[ 7.0501e-05, -1.2492e-01, -6.0990e-01,  ...,  1.9946e-02,\n",
            "          -7.0957e-03,  8.3471e-02],\n",
            "         [ 4.3968e-05, -2.6039e-05, -7.6963e-01,  ...,  8.3917e-05,\n",
            "          -3.3340e-01,  6.4982e-05],\n",
            "         [ 9.5394e-05,  5.0178e-10, -1.1718e-05,  ...,  1.8120e-04,\n",
            "          -2.9468e-05, -2.0780e-01]]], grad_fn=<SliceBackward0>)\n",
            "x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)  x = self.fc(x)\n",
            "torch.Size([5, 3, 65])\n",
            "tensor([[[ -2.7497,  11.0687, -11.3131, -11.5079, -11.2651, -11.4700,   2.2336,\n",
            "          -11.5090,  -3.3018, -11.4647,   0.1220, -11.2284, -11.3552,  -9.4663,\n",
            "           -6.3287,  -7.2296, -11.1219, -11.5366,  -5.9758, -11.3748, -11.3034,\n",
            "          -11.3461, -11.7397, -11.4775, -11.4566, -11.4489, -11.1412, -11.2714,\n",
            "          -11.3103, -11.3512, -11.1802, -10.3148, -11.6613, -11.1955, -11.2601,\n",
            "          -11.3199, -11.4709,  -5.9088, -11.6149,  -1.9408, -11.3601,  -5.0971,\n",
            "           -1.8705,   1.6385,  -2.8700, -11.5010,  -2.9211,  -1.8854, -11.5266,\n",
            "           -2.1595,  -5.7762,  -5.5810,  -0.0134,  -0.1789,  -3.3799, -11.5879,\n",
            "            0.0572,  -1.2473,  -0.7131,  -4.8054, -11.3401, -10.1651, -11.6140,\n",
            "           -3.2828,  -3.7530],\n",
            "         [ -1.5021,   0.2900,  -5.1773,  -5.2369,  -5.2139,  -5.2372,  -0.1312,\n",
            "           -5.2924,  -2.1981,  -5.0924,  -0.8955,  -5.3171,  -5.1707,  -4.0706,\n",
            "           -3.4316,  -0.4406,  -5.1957,  -5.4953,  -2.3883,  -5.2343,  -5.2009,\n",
            "           -5.0916,  -5.2986,  -5.2192,  -5.2804,  -5.3197,  -5.1971,  -5.2023,\n",
            "           -5.2623,  -5.2573,  -5.2367,  -4.3967,  -5.3822,  -5.2113,  -5.3970,\n",
            "           -5.3515,  -5.0832,  -2.0424,  -5.3803,   1.3421,  -5.2748,  -1.5329,\n",
            "           -1.8157,  -1.4388,   0.7657,  -5.4380,   4.0827,  -0.9506,  -5.3232,\n",
            "           -1.6886,  -2.1127,   2.4641,  -1.1269,  -0.5963,  -0.6008,  -5.4141,\n",
            "           -0.9408,   3.3491,  -0.5679,  -2.2031,  -5.2104,   0.8451,  -5.3468,\n",
            "           -2.6254,  -1.3304],\n",
            "         [ -0.2463,   1.2419,  -3.6163,  -3.5029,  -3.3919,  -4.1720,   1.2878,\n",
            "           -3.9961,  -0.5978,  -3.2043,  -3.0371,  -4.0709,  -3.4103,  -1.4561,\n",
            "           -0.9801,   0.1799,  -3.5196,  -3.8586,  -0.6850,  -3.6850,  -3.8417,\n",
            "           -3.1133,  -3.5980,  -3.7130,  -3.7652,  -3.9075,  -3.4802,  -3.4943,\n",
            "           -3.4819,  -3.6434,  -3.8527,  -0.6722,  -3.7307,  -4.0748,  -3.5928,\n",
            "           -3.8363,  -3.4878,  -1.6383,  -3.8534,  -3.5396,  -3.8744,   1.3138,\n",
            "            0.5421,   0.9217,  -3.8006,  -4.0875,  -1.4446,  -2.3359,  -3.4252,\n",
            "           -4.0127,   1.7023,  -1.5259,  -2.7437,   0.3913,  11.9075,  -3.8092,\n",
            "            0.8213,  -3.1116,   3.1937,   1.9157,  -3.3451,  -1.0668,  -3.6974,\n",
            "           -1.7053,   1.1111]],\n",
            "\n",
            "        [[ -2.0033,   0.3974,  -7.7373,  -8.2390,  -8.0232,  -8.3226,   0.2254,\n",
            "           -8.3118,  -1.8590,  -8.1779,  -0.2505,  -8.2625,  -8.0593,  -3.7951,\n",
            "           -4.2922,  -3.2068,  -8.0563,  -8.0880,  -3.1299,  -8.1863,  -8.1899,\n",
            "           -8.2816,  -8.2627,  -8.1567,  -8.2396,  -7.9326,  -7.9499,  -8.0786,\n",
            "           -8.1449,  -8.2967,  -8.4876,  -3.8733,  -8.3007,  -8.2281,  -8.3203,\n",
            "           -8.1180,  -8.0696,  -2.3312,  -8.2276,   0.0159,  -7.9832,  -4.8398,\n",
            "           -1.9982,   9.3421,  -1.8307,  -8.2595,  -3.4132,   1.6423,  -8.0109,\n",
            "           -3.9013,  -1.8000,  -5.0202,   2.1708,  -1.6109,  -2.2864,  -7.8594,\n",
            "           -1.1639,  -0.6338,  -2.9300,  -4.6318,  -7.9642,  -8.1341,  -8.0160,\n",
            "           -5.0496,  -3.3481],\n",
            "         [ -3.0840,   2.0854,  -8.8920,  -8.9786,  -9.0722,  -9.0941,  -1.0895,\n",
            "           -9.1482,  -3.6297,  -9.0700,  -1.5872,  -9.2032,  -9.1375,  -6.6771,\n",
            "           -5.8473,  -4.4281,  -8.9633,  -9.0718,  -6.2737,  -9.0255,  -8.9860,\n",
            "           -9.2011,  -9.2883,  -9.1735,  -9.0439,  -8.7256,  -9.1058,  -9.0571,\n",
            "           -8.9123,  -8.9713,  -9.3118,  -7.2397,  -9.0763,  -8.9617,  -9.1064,\n",
            "           -8.8253,  -9.0715,  -4.2099,  -8.8455,   2.9623,  -9.0146,  -5.2993,\n",
            "            0.9049,   3.5232,   1.9164,  -9.0919,  -3.7155,   0.2254,  -9.0615,\n",
            "           -1.9812,  -4.2088,  -4.5139,   6.3129,  -2.6875,  -1.9433,  -8.8092,\n",
            "            0.7132,  -1.0565,  -2.5761,  -4.4356,  -8.8008,  -6.6214,  -8.8879,\n",
            "           -5.5922,  -3.6736],\n",
            "         [ -2.2681,   0.2613,  -3.8699,  -3.9274,  -4.2027,  -4.0338,   0.4754,\n",
            "           -4.0284,  -2.1686,  -4.2260,   8.6891,  -3.8543,  -3.9172,  -4.0690,\n",
            "           -2.7703,  -4.4299,  -4.1090,  -3.9412,  -2.9623,  -3.9138,  -4.0932,\n",
            "           -4.0600,  -4.0412,  -4.1014,  -4.0564,  -3.8378,  -4.0662,  -3.9009,\n",
            "           -4.0128,  -4.2235,  -3.9670,  -4.5832,  -4.0939,  -3.8481,  -3.9484,\n",
            "           -3.7889,  -3.9597,  -2.5423,  -3.9854,  -0.5140,  -3.9074,  -3.7142,\n",
            "           -1.6204,   0.0174,  -1.0515,  -4.0071,  -2.1404,   0.4050,  -4.0482,\n",
            "           -0.4424,   0.4983,  -0.6314,  -0.1081,  -1.1331,  -5.1553,  -4.3679,\n",
            "           -2.7016,   0.7225,  -1.7360,  -3.8164,  -3.9573,  -4.0381,  -3.9871,\n",
            "            1.4819,  -2.0634]]], grad_fn=<SliceBackward0>)\n",
            "Epoch 2/1, Loss: 0.8101118803024292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "class LSTMCell:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.Wf = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.Wi = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.Wo = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.Wc = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "\n",
        "        self.bf = np.zeros((hidden_size, 1))\n",
        "        self.bi = np.zeros((hidden_size, 1))\n",
        "        self.bo = np.zeros((hidden_size, 1))\n",
        "        self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "        #print(\"Forget gate dimensions and values\")\n",
        "        #print(self.Wf.shape)\n",
        "        #print(self.Wf)\n",
        "\n",
        "        #print(\"Input gate dimensions and values\")\n",
        "        #print(self.Wi.shape)\n",
        "        #print(self.Wi)\n",
        "\n",
        "\n",
        "        #print(\"Output gate dimensions and values\")\n",
        "        #print(self.Wo.shape)\n",
        "        #print(self.Wo)\n",
        "\n",
        "\n",
        "        #print(\"Cell  gate dimensions and values\")\n",
        "        #print(self.Wc.shape)\n",
        "        #print(self.Wc )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        # Concatenate h_prev and x\n",
        "        print(\"h_prev==>\",h_prev.shape,h_prev)\n",
        "        print(\"x==>\",x.shape,x)\n",
        "        concat = np.concatenate((h_prev, x), axis=1)\n",
        "        print(\"concat=======================>\",concat.shape)\n",
        "        print(\"h_prev==>\",h_prev.shape,h_prev)\n",
        "        print(\"x==>\",x.shape,x)\n",
        "        print(concat)\n",
        "        # Compute values for forget gate, input gate, output gate and cell state\n",
        "        ft = sigmoid(np.dot(self.Wf, concat.T) + self.bf)\n",
        "        print(\"forget weight weights==>\",self.Wf.shape,self.Wf)\n",
        "        print(\"forget gate output==> sigmoid(np.dot(self.Wf, concat.T) + self.bf) ====>\",ft.shape)\n",
        "        print(ft)\n",
        "        it = sigmoid(np.dot(self.Wi, concat.T) + self.bi)\n",
        "        print(\"forget weight weights==>\",self.Wf.shape,self.Wf)\n",
        "        print(\"Input gate output==> igmoid(np.dot(self.Wi, concat.T) + self.bi) ====>\",it.shape)\n",
        "        print(it)\n",
        "        ot = sigmoid(np.dot(self.Wo, concat.T) + self.bo)\n",
        "        print(\"forget weight weights==>\",self.Wf.shape,self.Wf)\n",
        "        print(\"Output gate output==> sigmoid(np.dot(self.Wo, concat.T) + self.bo) ====>\",ot.shape)\n",
        "        print(ot)\n",
        "        ct_hat = tanh(np.dot(self.Wc, concat.T) + self.bc)\n",
        "        print(\"forget weight weights==>\",self.Wf.shape,self.Wf)\n",
        "        print(\"Cell gate output==> tanh(np.dot(self.Wc, concat.T) + self.bc) ====>\",ct_hat.shape)\n",
        "        print(ct_hat)\n",
        "        # Compute current cell state\n",
        "        ct = ft * c_prev.T + it * ct_hat\n",
        "        print(\"Current cell state==> ft * c_prev + it * ct_hat ====>\",ct.shape)\n",
        "        print(ct)\n",
        "\n",
        "        # Compute current hidden state\n",
        "        ht = ot * tanh(ct)\n",
        "        print(\"Current hidden state==> ot * tanh(ct) ====>\",ht.shape)\n",
        "        print(ht)\n",
        "        return ht.T, ct\n",
        "\n",
        "# Test the LSTMCell\n",
        "batch_size = 2\n",
        "seq_length = 3\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "\n",
        "# Initialize LSTM cell\n",
        "lstm = LSTMCell(input_size, hidden_size)\n",
        "\n",
        "# Initialize hidden state and cell state\n",
        "h = np.zeros((batch_size, hidden_size))\n",
        "c = np.zeros((batch_size, hidden_size))\n",
        "\n",
        "# Assume input is random tensor\n",
        "x = np.random.randn(batch_size, seq_length, input_size)\n",
        "\n",
        "print(\"Input tensor===>\",x.shape,x)\n",
        "\n",
        "\n",
        "# Process each sequence element\n",
        "for t in range(seq_length):\n",
        "    print(\"ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt====>\",t)\n",
        "    h, c = lstm.forward(x[:, t, :], h, c)\n",
        "    print(\"Hidden state at time t===>\",h.shape,h)\n",
        "    print(\"Cell state at time t===>\",c.shape,c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Zkub_uKZW8N",
        "outputId": "256c0298-fb46-4722-f1b6-df1c0e0355ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor===> (2, 3, 4) [[[-0.35399391 -1.37495129 -0.6436184  -2.22340315]\n",
            "  [ 0.62523145 -1.60205766 -1.10438334  0.05216508]\n",
            "  [-0.739563    1.5430146  -1.29285691  0.26705087]]\n",
            "\n",
            " [[-0.03928282 -1.1680935   0.52327666 -0.17154633]\n",
            "  [ 0.77179055  0.82350415  2.16323595  1.33652795]\n",
            "  [-0.36918184 -0.23937918  1.0996596   0.65526373]]]\n",
            "ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt====> 0\n",
            "h_prev==> (2, 5) [[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "x==> (2, 4) [[-0.35399391 -1.37495129 -0.6436184  -2.22340315]\n",
            " [-0.03928282 -1.1680935   0.52327666 -0.17154633]]\n",
            "concat=======================> (2, 9)\n",
            "h_prev==> (2, 5) [[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "x==> (2, 4) [[-0.35399391 -1.37495129 -0.6436184  -2.22340315]\n",
            " [-0.03928282 -1.1680935   0.52327666 -0.17154633]]\n",
            "[[ 0.          0.          0.          0.          0.         -0.35399391\n",
            "  -1.37495129 -0.6436184  -2.22340315]\n",
            " [ 0.          0.          0.          0.          0.         -0.03928282\n",
            "  -1.1680935   0.52327666 -0.17154633]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "forget gate output==> sigmoid(np.dot(self.Wf, concat.T) + self.bf) ====> (5, 2)\n",
            "[[0.34672607 0.24362691]\n",
            " [0.24575575 0.60104276]\n",
            " [0.11673042 0.0325774 ]\n",
            " [0.94853737 0.89470751]\n",
            " [0.93856218 0.95918717]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "Input gate output==> igmoid(np.dot(self.Wi, concat.T) + self.bi) ====> (5, 2)\n",
            "[[0.93927783 0.381893  ]\n",
            " [0.95657715 0.67941674]\n",
            " [0.37606565 0.58281832]\n",
            " [0.96036982 0.64929199]\n",
            " [0.13881011 0.35954859]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "Output gate output==> sigmoid(np.dot(self.Wo, concat.T) + self.bo) ====> (5, 2)\n",
            "[[0.15496805 0.70530966]\n",
            " [0.41848402 0.09009613]\n",
            " [0.05067789 0.62138462]\n",
            " [0.2405702  0.13023979]\n",
            " [0.89374124 0.7093509 ]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "Cell gate output==> tanh(np.dot(self.Wc, concat.T) + self.bc) ====> (5, 2)\n",
            "[[-0.92098094 -0.44334843]\n",
            " [ 0.66979892  0.80831793]\n",
            " [-0.99465899 -0.03510792]\n",
            " [-0.99748193 -0.86657692]\n",
            " [ 0.90273892  0.53394754]]\n",
            "Current cell state==> ft * c_prev + it * ct_hat ====> (5, 2)\n",
            "[[-0.86505699 -0.16931166]\n",
            " [ 0.64071434  0.54918473]\n",
            " [-0.37405708 -0.02046154]\n",
            " [-0.95795154 -0.56266145]\n",
            " [ 0.12530929  0.19198009]]\n",
            "Current hidden state==> ot * tanh(ct) ====> (5, 2)\n",
            "[[-0.10830004 -0.118289  ]\n",
            " [ 0.2366049   0.04503986]\n",
            " [-0.01811913 -0.01271271]\n",
            " [-0.17883069 -0.06641571]\n",
            " [ 0.11141155  0.1345325 ]]\n",
            "Hidden state at time t===> (2, 5) [[-0.10830004  0.2366049  -0.01811913 -0.17883069  0.11141155]\n",
            " [-0.118289    0.04503986 -0.01271271 -0.06641571  0.1345325 ]]\n",
            "Cell state at time t===> (5, 2) [[-0.86505699 -0.16931166]\n",
            " [ 0.64071434  0.54918473]\n",
            " [-0.37405708 -0.02046154]\n",
            " [-0.95795154 -0.56266145]\n",
            " [ 0.12530929  0.19198009]]\n",
            "ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt====> 1\n",
            "h_prev==> (2, 5) [[-0.10830004  0.2366049  -0.01811913 -0.17883069  0.11141155]\n",
            " [-0.118289    0.04503986 -0.01271271 -0.06641571  0.1345325 ]]\n",
            "x==> (2, 4) [[ 0.62523145 -1.60205766 -1.10438334  0.05216508]\n",
            " [ 0.77179055  0.82350415  2.16323595  1.33652795]]\n",
            "concat=======================> (2, 9)\n",
            "h_prev==> (2, 5) [[-0.10830004  0.2366049  -0.01811913 -0.17883069  0.11141155]\n",
            " [-0.118289    0.04503986 -0.01271271 -0.06641571  0.1345325 ]]\n",
            "x==> (2, 4) [[ 0.62523145 -1.60205766 -1.10438334  0.05216508]\n",
            " [ 0.77179055  0.82350415  2.16323595  1.33652795]]\n",
            "[[-0.10830004  0.2366049  -0.01811913 -0.17883069  0.11141155  0.62523145\n",
            "  -1.60205766 -1.10438334  0.05216508]\n",
            " [-0.118289    0.04503986 -0.01271271 -0.06641571  0.1345325   0.77179055\n",
            "   0.82350415  2.16323595  1.33652795]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "forget gate output==> sigmoid(np.dot(self.Wf, concat.T) + self.bf) ====> (5, 2)\n",
            "[[0.092951   0.36867648]\n",
            " [0.11134616 0.97012644]\n",
            " [0.06285262 0.14694278]\n",
            " [0.96711805 0.06025928]\n",
            " [0.44034057 0.69642014]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "Input gate output==> igmoid(np.dot(self.Wi, concat.T) + self.bi) ====> (5, 2)\n",
            "[[0.33730533 0.04781775]\n",
            " [0.76559809 0.05830624]\n",
            " [0.29532651 0.81456448]\n",
            " [0.40540283 0.08269733]\n",
            " [0.90450549 0.48619762]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "Output gate output==> sigmoid(np.dot(self.Wo, concat.T) + self.bo) ====> (5, 2)\n",
            "[[0.20643694 0.99071508]\n",
            " [0.06489958 0.28305051]\n",
            " [0.65407686 0.98005907]\n",
            " [0.22729555 0.37316127]\n",
            " [0.84064026 0.28403777]]\n",
            "forget weight weights==> (5, 9) [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
            "   0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351  0.76103773  0.12167502  0.44386323\n",
            "   0.33367433  1.49407907 -0.20515826]\n",
            " [ 0.3130677  -0.85409574 -2.55298982  0.6536186   0.8644362  -0.74216502\n",
            "   2.26975462 -1.45436567  0.04575852]\n",
            " [-0.18718385  1.53277921  1.46935877  0.15494743  0.37816252 -0.88778575\n",
            "  -1.98079647 -0.34791215  0.15634897]\n",
            " [ 1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
            "  -1.70627019  1.9507754  -0.50965218]]\n",
            "Cell gate output==> tanh(np.dot(self.Wc, concat.T) + self.bc) ====> (5, 2)\n",
            "[[-0.95732949  0.19299229]\n",
            " [-0.98592607  0.94879445]\n",
            " [ 0.37229876  0.13615557]\n",
            " [-0.99815392  0.88940232]\n",
            " [ 0.91333489 -0.96261792]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (5,2) (2,5) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-cb54ecc82b62>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt====>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hidden state at time t===>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cell state at time t===>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-cb54ecc82b62>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h_prev, c_prev)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Compute current cell state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mct_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current cell state==> ft * c_prev + it * ct_hat ====>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,2) (2,5) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "Wf = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "Wi = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "Wo = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "Wc = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "\n",
        "bf = np.zeros((hidden_size, 1))\n",
        "bi = np.zeros((hidden_size, 1))\n",
        "bo = np.zeros((hidden_size, 1))\n",
        "bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "h_prev = np.zeros((batch_size, hidden_size))\n",
        "c_prev = np.zeros((batch_size, hidden_size))\n",
        "\n",
        "# Assume x is some random tensor of shape (3, 4, 5)\n",
        "x = np.random.randn(batch_size, seq_length, input_size)"
      ],
      "metadata": {
        "id": "5jDaOFSJf1VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(\"***********\")\n",
        "print(x[:, 0, :])\n",
        "print(\"*********************\")\n",
        "print(\"h_prev==>,\",h_prev.shape,h_prev)\n",
        "print(\"x[:, t, :]==>\",x[:, 0, :])\n",
        "print(np.concatenate((h_prev, x[:, 0, :]), axis=1).shape,\n",
        "      np.concatenate((h_prev, x[:, 0, :]), axis=1))\n",
        "print(\"wf==>\",lstm.Wf.shape,lstm.Wf)\n",
        "x_t = x[:, 0, :]\n",
        "concat = np.concatenate((h_prev, x_t), axis=1)\n",
        "print(\"****************************************************************\")\n",
        "print(np.dot(Wf, concat.T) + bf)\n",
        "print(\"****************************************************************\")\n",
        "print(sigmoid(np.dot(Wf, concat.T) + bf))\n",
        "print(sigmoid(np.dot(Wi, concat.T) + bi))\n",
        "print(sigmoid(np.dot(Wo, concat.T) + bo))\n",
        "print(tanh(np.dot(Wc, concat.T) + bc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNAuCkNzgGKD",
        "outputId": "b22e3361-bc8d-4a0b-99cb-99233c61ca04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.05933176 -0.91549661  0.47711997  0.20160923]\n",
            "  [-0.98224828 -0.45371799 -0.60614324 -0.28247412]\n",
            "  [ 0.58099559 -1.58919699 -0.1668525   0.45628182]]\n",
            "\n",
            " [[-0.72127281 -0.3670134   1.60307057 -0.98966282]\n",
            "  [ 1.1893153  -0.44717291  0.35499218 -0.22841008]\n",
            "  [ 0.3343586  -0.59322402 -1.08880928  2.51871154]]]\n",
            "***********\n",
            "[[ 0.05933176 -0.91549661  0.47711997  0.20160923]\n",
            " [-0.72127281 -0.3670134   1.60307057 -0.98966282]]\n",
            "*********************\n",
            "h_prev==>, (2, 5) [[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "x[:, t, :]==> [[ 0.05933176 -0.91549661  0.47711997  0.20160923]\n",
            " [-0.72127281 -0.3670134   1.60307057 -0.98966282]]\n",
            "(2, 9) [[ 0.          0.          0.          0.          0.          0.05933176\n",
            "  -0.91549661  0.47711997  0.20160923]\n",
            " [ 0.          0.          0.          0.          0.         -0.72127281\n",
            "  -0.3670134   1.60307057 -0.98966282]]\n",
            "wf==> (5, 9) [[ 1.99302768 -0.73690195 -0.22045035  0.62419505 -1.15943087 -0.61887807\n",
            "   1.58044444 -0.43098462 -0.2191978 ]\n",
            " [-0.4887132  -1.11886172 -0.010474   -1.9109398  -0.88902677 -0.00856434\n",
            "  -0.65369615  0.32109015  0.73553289]\n",
            " [-0.55852608 -1.53898927 -0.24889722 -1.04524043  0.08125045  0.72035556\n",
            "  -1.24259777 -1.16204493 -0.50489367]\n",
            " [-2.76982286 -0.01136284  0.87247963 -0.89643508  1.45174299  0.52218938\n",
            "  -2.74967814  0.55090496 -0.5906255 ]\n",
            " [-0.40623809 -1.22363919  0.29033915 -0.01186776  0.00407472  0.08355408\n",
            "   1.21616998 -0.41636859 -0.00728278]]\n",
            "****************************************************************\n",
            "[[-1.65820618  3.14425782]\n",
            " [ 0.63514166 -1.09714157]\n",
            " [ 0.22401217 -0.15644712]\n",
            " [-0.01586742  0.88339851]\n",
            " [ 0.06812865  3.11569678]]\n",
            "****************************************************************\n",
            "[[0.16000294 0.95868187]\n",
            " [0.6536544  0.25027586]\n",
            " [0.55577002 0.4609678 ]\n",
            " [0.49603323 0.70752598]\n",
            " [0.51702558 0.9575356 ]]\n",
            "[[0.57937781 0.56416151]\n",
            " [0.51998328 0.86322938]\n",
            " [0.48910236 0.42536124]\n",
            " [0.10993774 0.06553796]\n",
            " [0.64702699 0.83806802]]\n",
            "[[0.2406568  0.60231305]\n",
            " [0.57639848 0.00382037]\n",
            " [0.46589327 0.82375742]\n",
            " [0.0925333  0.16694921]\n",
            " [0.11480241 0.93437949]]\n",
            "[[-0.39398318  0.61436915]\n",
            " [-0.97175478 -0.94689327]\n",
            " [-0.71377019  0.99087108]\n",
            " [-0.02255508  0.90780395]\n",
            " [-0.06542896 -0.67920564]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(seq_length):\n",
        "    # Concatenate h_prev and x_t\n",
        "    x_t = x[:, t, :]\n",
        "    concat = np.concatenate((h_prev, x_t), axis=1)\n",
        "\n",
        "    # Compute values for forget gate (f_t), input gate (i_t), output gate (o_t) and cell state (c_t_hat)\n",
        "    f_t = sigmoid(np.dot(Wf, concat.T) + bf)\n",
        "    i_t = sigmoid(np.dot(Wi, concat.T) + bi)\n",
        "    o_t = sigmoid(np.dot(Wo, concat.T) + bo)\n",
        "    c_t_hat = tanh(np.dot(Wc, concat.T) + bc)\n",
        "\n",
        "    # Compute current cell state (c_t)\n",
        "    c_t = f_t * c_prev.T + i_t * c_t_hat\n",
        "\n",
        "    # Compute current hidden state (h_t)\n",
        "    h_t = o_t * tanh(c_t)\n",
        "\n",
        "    # Update h_prev and c_prev for next time step\n",
        "    h_prev = h_t.T\n",
        "    c_prev = c_t.T\n",
        "    print(h_prev.shape)\n",
        "    print(h_prev)\n",
        "    print(c_prev.shape)\n",
        "    print(c_prev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qhr7yTSgB-t",
        "outputId": "d41a741a-db20-4396-f696-992f8ba9442e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5)\n",
            "[[-5.39989226e-02 -2.68758567e-01 -1.56345891e-01 -2.29450168e-04\n",
            "  -4.85717868e-03]\n",
            " [ 2.00786991e-01 -2.57356858e-03  3.27999581e-01  9.92104266e-03\n",
            "  -4.81006041e-01]]\n",
            "(2, 5)\n",
            "[[-0.22826512 -0.50529624 -0.34910668 -0.00247965 -0.0423343 ]\n",
            " [ 0.34660343 -0.81738609  0.42147815  0.05949562 -0.56922053]]\n",
            "(2, 5)\n",
            "[[ 0.02886448  0.18255995  0.25987844  0.04154207  0.09117601]\n",
            " [-0.36247351 -0.06304925 -0.05219616  0.00346973  0.01868167]]\n",
            "(2, 5)\n",
            "[[ 0.21315365  0.27201144  0.46188708  0.11514466  0.24940958]\n",
            " [-0.48988214 -0.31728891 -0.32602394  0.03966995  0.13523314]]\n",
            "(2, 5)\n",
            "[[-5.96826378e-02 -1.43427155e-01 -9.01711376e-02  1.28831175e-03\n",
            "   3.01196551e-03]\n",
            " [ 6.84737285e-03 -3.80971705e-01 -9.46467157e-02 -6.98297090e-02\n",
            "  -2.36242054e-04]]\n",
            "(2, 5)\n",
            "[[-0.58760932 -0.15960145 -0.38283673  0.02046985  0.38879184]\n",
            " [ 0.15962094 -0.40120972 -0.60678152 -0.33813716 -0.15837119]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input tensor======>\",x.shape,x)\n",
        "print(\"wf==>\",lstm.Wf.shape,lstm.Wf)\n",
        "print(\"wi==>\",lstm.Wi.shape,lstm.Wi)\n",
        "print(\"Wo===>\",lstm.Wo.shape,lstm.Wo)\n",
        "print(\"Wc==>\",lstm.Wc.shape,lstm.Wc)\n",
        "\n",
        "print(\"bf==>\",lstm.bf.shape,lstm.bf)\n",
        "print(\"bi==>\",lstm.bi.shape,lstm.bi)\n",
        "print(\"bo==>\",lstm.bo.shape,lstm.bo)\n",
        "print(\"bc==>\",lstm.bc.shape,lstm.bc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVnkAFWKZfvc",
        "outputId": "0c70068a-f9bd-4f84-ac74-980a3238f91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor======> (3, 4, 5) [[[-0.79319164  0.23474036  1.0725356   1.69250211  1.14727001]\n",
            "  [-1.24623813  0.89556468  0.44412291  0.22822564  0.7851103 ]\n",
            "  [-1.4572875   0.92766447  0.32136721  2.14382004  0.03943669]\n",
            "  [ 0.71886723  1.96402027  2.05521025 -0.63537273  0.78291232]]\n",
            "\n",
            " [[ 0.20504615 -0.34989747  0.9590309  -0.56740641  1.27787505]\n",
            "  [ 0.11965632 -0.76962936 -1.20200952 -0.3575169   0.40778224]\n",
            "  [-0.35282129 -0.70546969 -0.77397956  0.17971712  1.09737092]\n",
            "  [ 1.218053   -2.50422718  1.58778394 -1.21752511 -0.48265069]]\n",
            "\n",
            " [[ 0.15920292  0.10071423  0.71519149  2.11642408  1.24268321]\n",
            "  [ 2.3231164  -1.07607465 -0.44757528 -1.67021724 -0.49493213]\n",
            "  [-0.37676983  1.53761601 -1.52618961 -0.28168626  0.87250112]\n",
            "  [-1.24851316  0.7067048  -0.12786652  0.41356804 -0.13664172]]]\n",
            "wf==> (6, 11) [[ 1.09379446  0.20016775  0.95837103  1.28973779  1.8233545   0.63755593\n",
            "  -0.24516821 -1.77272191 -0.03246214 -0.0595546   0.18514247]\n",
            " [ 0.84900646  1.07240717  0.9328037  -0.18401139 -0.47059205  0.3762915\n",
            "   1.44146105 -0.0721481   0.27937908 -0.12418218  0.6535455 ]\n",
            " [ 1.96715969  2.05521747  0.0381559  -1.02555762  0.12223711 -0.98791069\n",
            "  -0.31723698 -0.65982433 -0.43778977 -1.52172998 -0.12918068]\n",
            " [ 0.35310737 -0.96533969  0.46994164  0.73028484 -0.92938818 -1.31799762\n",
            "   0.44990293  0.54230946  0.87118544  0.13540039  0.164163  ]\n",
            " [ 1.31344122 -0.81549352 -1.20219048 -1.12464195 -1.23004462 -1.35168566\n",
            "   2.00741983 -0.71008353 -0.99092008  1.46116649  2.11982526]\n",
            " [-0.09255077 -0.16775087 -0.63461152 -0.55482931  0.70963713 -0.8939256\n",
            "  -1.3281518   1.71381156 -0.31355465  0.24140999  0.82362084]]\n",
            "wi==> (6, 11) [[-3.18751646e-01  2.46929618e-01  1.00818985e+00 -1.11454240e-03\n",
            "   8.15297805e-01  5.75921493e-01 -2.08166653e+00 -1.22199002e-01\n",
            "   2.88189602e-01  8.60387686e-01  1.46253213e-01]\n",
            " [ 1.44861859e-01  6.06563928e-01 -3.71933726e-01  3.68125957e-01\n",
            "  -1.60898514e+00  6.43773823e-01  1.19777261e+00  9.78943044e-01\n",
            "   7.72989032e-01  4.03610507e-01 -8.77452056e-01]\n",
            " [-2.06082730e-01  1.91188659e-01  9.90895530e-01 -1.84620629e-01\n",
            "  -2.61189923e-01  3.45521021e-01  5.85726009e-01  1.17797589e+00\n",
            "   1.21179707e+00 -1.49457599e+00  4.75724589e-01]\n",
            " [ 1.99727804e+00 -8.58027729e-01 -9.54706028e-01 -1.84866072e+00\n",
            "   6.22570000e-03 -4.17323559e-01 -1.54203556e-01  3.19967330e-01\n",
            "  -1.34285782e-02 -6.02757054e-01 -1.17241550e+00]\n",
            " [ 1.32914645e+00  8.06115799e-01  2.03605124e-01  1.92179023e-03\n",
            "   2.06928526e+00 -3.85117734e-01 -7.53823900e-02  4.46648301e-01\n",
            "   9.44500918e-01  1.24175394e-01 -2.63583676e-01]\n",
            " [ 1.06300372e+00 -7.78908437e-01  1.07469613e+00 -6.10897549e-02\n",
            "   8.20847905e-02 -1.14979366e+00 -1.26931778e-01  6.29276689e-01\n",
            "   1.07151451e-01  1.47440510e-01 -3.74994162e-01]]\n",
            "Wo===> (6, 11) [[-1.13859766 -0.68146569 -1.10498984 -1.47886923  0.13350474 -0.4175443\n",
            "  -0.82879467 -0.68033347  1.93401684  1.11327322 -0.88395379]\n",
            " [ 0.94121587  1.69103141 -0.30853513 -1.52630091 -0.50368301  0.87788084\n",
            "   0.0733952  -0.33177412 -1.30045935  0.00313621 -1.30100104]\n",
            " [ 0.68907166 -0.48419631 -1.89736952 -0.3692843  -0.50699804 -2.24618953\n",
            "   1.58791147  0.89764768 -0.75915669  0.63377463 -0.1700297 ]\n",
            " [ 1.02653901 -1.76938367 -1.45831386  0.39883264  0.93706701 -1.26806542\n",
            "  -0.134738   -0.91119696  1.97646875  0.62893964 -0.40838687]\n",
            " [ 0.68945543  0.01603262  1.074386   -0.04876243 -0.9660694   1.33930578\n",
            "  -0.79275613 -1.41965571  0.16265009 -1.70554216 -1.29076938]\n",
            " [-1.06348389 -2.02068356  0.70772412  1.07230086 -0.50624982  0.57543643\n",
            "   0.12088803 -1.45093292 -1.6273699   0.03987763 -0.521628  ]]\n",
            "Wc==> (6, 11) [[-0.9478041   0.09975095 -0.29226298 -1.66485721  0.4026271   1.68331918\n",
            "   1.82768365 -1.37770647 -0.46112929  0.24270363  2.18097646]\n",
            " [ 2.59475844  0.62248132  1.45745453 -0.69649036  0.91251514  0.61320977\n",
            "  -0.40886666  0.45380281  1.18401834  1.07619117  1.73674942]\n",
            " [ 0.64287649 -2.00335351 -0.05467055 -0.12843583  0.37521817 -1.60236453\n",
            "   0.67919286 -0.54184418 -0.33572083  0.86439619 -0.59112635]\n",
            " [-0.69151174 -0.75980103  0.43359123 -2.75036433  0.79636632  2.45262453\n",
            "   1.5295618  -0.47223197 -0.42328491  0.75113694 -0.19607352]\n",
            " [-2.62310941 -0.36476657  0.52097751  0.62407395 -0.76820988  0.30579844\n",
            "  -0.51690436 -1.71525628 -0.78613239 -1.73156928 -0.59551989]\n",
            " [ 0.21733676 -0.54200558  0.42130823 -1.01299054  0.28256063  0.03692379\n",
            "   0.00393425 -0.79582098  0.47853903  1.58060121 -0.04044343]]\n",
            "bf==> (6, 1) [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "bi==> (6, 1) [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "bo==> (6, 1) [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "bc==> (6, 1) [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Specify the shape\n",
        "shape = (1,3, 5)\n",
        "\n",
        "# Create a NumPy array with random values\n",
        "random_array = np.random.rand(*shape)\n",
        "\n",
        "print(random_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4DgUw0buqhr",
        "outputId": "42f6b47a-fb6a-46e4-ec34-1c2483b23e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.04534995 0.30259884 0.42103576 0.78677728 0.05813113]\n",
            "  [0.72135263 0.15994732 0.08239097 0.78180559 0.73328152]\n",
            "  [0.53943352 0.97780512 0.00187622 0.35076162 0.43244048]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "filename = 'shakespeare.txt'\n",
        "if not os.path.isfile(filename):\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "text_as_int, vocab, char2idx, idx2char = process_text(filename)\n",
        "dataset = DataLoader(TextDataset(text_as_int), batch_size=16, shuffle=True, drop_last=True)\n",
        "print(text_as_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCf4GREqdP6v",
        "outputId": "b25feb1a-3e81-4455-bb5a-617e6cc6c40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(inputs, targets) for inputs, targets in dataset]\n",
        "# Iterate over the DataLoader\n",
        "for inputs, targets in dataset:\n",
        "    # Convert tensors to lists\n",
        "    inputs_list = inputs.tolist()\n",
        "    targets_list = targets.tolist()\n",
        "\n",
        "    # Iterate over the sequences in the batch\n",
        "    print(text_as_int)\n",
        "    for inp, tgt in zip(inputs_list, targets_list):\n",
        "        print(inp, \",\", tgt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2taDtA4_KVzt",
        "outputId": "fd900b2e-64c1-418b-a3b3-3f91e647b282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n",
            "[1, 15, 47] , [15, 47, 58]\n",
            "[58, 47, 64] , [47, 64, 43]\n",
            "[56, 57, 58] , [57, 58, 1]\n",
            "[47, 56, 57] , [56, 57, 58]\n",
            "[14, 43, 44] , [43, 44, 53]\n",
            "[47, 64, 43] , [64, 43, 52]\n",
            "[57, 58, 1] , [58, 1, 15]\n",
            "[43, 52, 10] , [52, 10, 0]\n",
            "[10, 0, 14] , [0, 14, 43]\n",
            "[52, 10, 0] , [10, 0, 14]\n",
            "[0, 14, 43] , [14, 43, 44]\n",
            "[47, 58, 47] , [58, 47, 64]\n",
            "[58, 1, 15] , [1, 15, 47]\n",
            "[15, 47, 58] , [47, 58, 47]\n",
            "[18, 47, 56] , [47, 56, 57]\n",
            "[43, 44, 53] , [44, 53, 56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (inputs, targets) in enumerate(dataset):\n",
        "    # Print the shape of the inputs and targets\n",
        "\n",
        "    print(f\"  input: {inputs}\")\n",
        "    print(f\" target: {targets}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlernvYYRfUo",
        "outputId": "436cd42e-2063-4cef-ba0e-5498d909f80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  input: tensor([[ 1, 15, 47],\n",
            "        [58, 47, 64],\n",
            "        [43,  1, 54],\n",
            "        [58,  1, 15],\n",
            "        [10,  0, 14],\n",
            "        [43, 44, 53],\n",
            "        [52, 10,  0],\n",
            "        [44, 53, 56],\n",
            "        [ 1, 54, 56],\n",
            "        [43, 52, 10],\n",
            "        [47, 64, 43],\n",
            "        [18, 47, 56],\n",
            "        [57, 58,  1],\n",
            "        [61, 43,  1],\n",
            "        [56, 43,  1],\n",
            "        [ 1, 61, 43]])\n",
            " target: tensor([[15, 47, 58],\n",
            "        [47, 64, 43],\n",
            "        [ 1, 54, 56],\n",
            "        [ 1, 15, 47],\n",
            "        [ 0, 14, 43],\n",
            "        [44, 53, 56],\n",
            "        [10,  0, 14],\n",
            "        [53, 56, 43],\n",
            "        [54, 56, 53],\n",
            "        [52, 10,  0],\n",
            "        [64, 43, 52],\n",
            "        [47, 56, 57],\n",
            "        [58,  1, 15],\n",
            "        [43,  1, 54],\n",
            "        [43,  1, 61],\n",
            "        [61, 43,  1]])\n",
            "Shape of input: torch.Size([16, 3])\n",
            "Shape of target: torch.Size([16, 3])\n",
            "Contents of input: tensor([ 1, 15, 47])\n",
            "Contents of target: tensor([15, 47, 58])\n",
            "Contents of input: tensor([58, 47, 64])\n",
            "Contents of target: tensor([47, 64, 43])\n",
            "Contents of input: tensor([43,  1, 54])\n",
            "Contents of target: tensor([ 1, 54, 56])\n",
            "Contents of input: tensor([58,  1, 15])\n",
            "Contents of target: tensor([ 1, 15, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "# create a tensor of weights\n",
        "weights = torch.tensor([0, 10, 3, 0], dtype=torch.float)\n",
        "\n",
        "# draw two samples\n",
        "samples = torch.multinomial(weights, 2)\n",
        "print(samples)  # tensor([1, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt0sWpm-feF-",
        "outputId": "c95e7f50-ce2c-43ee-f4b9-d2141fc9974f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.W = np.random.randn(output_size, input_size)\n",
        "        self.b = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return np.dot(self.W, x) + self.b\n",
        "\n",
        "class LSTMCell:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.Wf = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.Wi = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.Wo = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.Wc = np.random.randn(hidden_size, hidden_size + input_size)\n",
        "        self.bf = np.zeros((hidden_size, 1))\n",
        "        self.bi = np.zeros((hidden_size, 1))\n",
        "        self.bo = np.zeros((hidden_size, 1))\n",
        "        self.bc = np.zeros((hidden_size, 1))\n",
        "        self.fc = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        print(h_prev.shape,h_prev)\n",
        "        print(x.shape,x)\n",
        "        concat = np.concatenate((h_prev, x), axis=1)\n",
        "        ft = sigmoid(np.dot(self.Wf, concat.T) + self.bf)\n",
        "        it = sigmoid(np.dot(self.Wi, concat.T) + self.bi)\n",
        "        ot = sigmoid(np.dot(self.Wo, concat.T) + self.bo)\n",
        "        ct_hat = tanh(np.dot(self.Wc, concat.T) + self.bc)\n",
        "        print(ft.shape,c_prev.shape,it.shape,ct_hat.shape)\n",
        "        print(ft)\n",
        "        print(c_prev)\n",
        "        ct = ft * c_prev.T + it * ct_hat\n",
        "        ht = ot * tanh(ct)\n",
        "        output = self.fc.forward(ht)\n",
        "        return output, ht.T, ct.T\n",
        "\n",
        "# Test the LSTMCell\n",
        "batch_size = 2\n",
        "seq_length = 3\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 10  # For example\n",
        "\n",
        "# Initialize LSTM cell\n",
        "lstm = LSTMCell(input_size, hidden_size, output_size)\n",
        "\n",
        "# Initialize hidden state and cell state\n",
        "h = np.zeros((batch_size, hidden_size))\n",
        "c = np.zeros((batch_size, hidden_size))\n",
        "\n",
        "# Assume input is random tensor\n",
        "x = np.random.randn(batch_size, seq_length, input_size)\n",
        "\n",
        "# Process each sequence element\n",
        "for t in range(seq_length):\n",
        "    output, h, c = lstm.forward(x[:, t, :], h, c)\n",
        "    probabilities = softmax(output)\n",
        "    predicted_id = np.argmax(probabilities)\n",
        "    print(predicted_id)  # Prints the index of the predicted class\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XNZpH-bYKnN",
        "outputId": "1abd0249-609c-454b-a8fc-01d7f04de9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5) [[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "(2, 4) [[-0.79937715  0.70399459 -1.55023118 -0.26228547]\n",
            " [ 0.56182756  2.28516652 -1.67499205 -0.8679992 ]]\n",
            "(5, 2) (2, 5) (5, 2) (5, 2)\n",
            "[[0.79415863 0.96409792]\n",
            " [0.0461629  0.45031164]\n",
            " [0.58459015 0.71646367]\n",
            " [0.4429187  0.28155426]\n",
            " [0.5260012  0.53446473]]\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "0\n",
            "(2, 5) [[ 7.69344712e-02  2.11563880e-01  4.14734788e-02 -5.77555046e-01\n",
            "  -3.37529164e-01]\n",
            " [ 3.43297933e-04 -5.46571964e-01  5.81490189e-03  6.75509857e-01\n",
            "  -5.09173200e-02]]\n",
            "(2, 4) [[-0.19327239 -0.43915334  1.99484741 -1.67202834]\n",
            " [ 0.17184498 -0.23366289 -0.40351162 -0.62760795]]\n",
            "(5, 2) (2, 5) (5, 2) (5, 2)\n",
            "[[0.37444208 0.24567084]\n",
            " [0.98300705 0.24494998]\n",
            " [0.2753869  0.45115689]\n",
            " [0.95118621 0.18563856]\n",
            " [0.81165944 0.71611133]]\n",
            "[[ 0.14393286  0.3920013   0.67627243 -0.92394627 -0.66468028]\n",
            " [ 0.01104992 -0.94248217  0.61343835  0.91535515 -0.2707623 ]]\n",
            "0\n",
            "(2, 5) [[-0.1878676   0.32297264 -0.14554883 -0.65723358 -0.00613174]\n",
            " [-0.01581233 -0.19051329  0.21326432  0.10930762 -0.17846266]]\n",
            "(2, 4) [[-1.00071289  0.34961249 -0.20584685 -0.62138156]\n",
            " [-1.84534266 -0.13585029 -1.31652117  0.42787543]]\n",
            "(5, 2) (2, 5) (5, 2) (5, 2)\n",
            "[[0.78595367 0.44268855]\n",
            " [0.38296423 0.01149644]\n",
            " [0.52453529 0.47501208]\n",
            " [0.91000633 0.53892882]\n",
            " [0.52010473 0.55790502]]\n",
            "[[-0.89731962  0.38358231 -0.29485723 -0.80132703 -0.03236443]\n",
            " [-0.06378343 -0.43675584  0.29993052  0.14290689 -0.59118913]]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "ft = np.array([[0.87864782, 0.19956512],\n",
        "               [0.99427636, 0.4190827 ],\n",
        "               [0.98402008, 0.00847185],\n",
        "               [0.45647319, 0.39778012],\n",
        "               [0.67051595, 0.68103651]])\n",
        "\n",
        "c_prev = np.zeros([2, 5])\n",
        "print(c_prev.shape,ft.shape)\n",
        "\n",
        "result = ft * c_prev\n",
        "\n",
        "#print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "UhZ9BahfWhBT",
        "outputId": "d622e0fe-e67e-447f-dd22-f4ccba12237d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5) (5, 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (5,2) (2,5) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-acfa30fd77e6>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#print(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,2) (2,5) "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1f_dp9S-viQ5v9n32ItmNxB7E6m00hAGJ",
      "authorship_tag": "ABX9TyNPVrfO6gcgPs1u6nMY3+sU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}